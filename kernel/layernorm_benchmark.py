import torch
import time
import statistics
from typing import Dict, List, Tuple, Optional
import json

class LayerNormBenchmark:
    """
    LayerNormæ€§èƒ½åŸºå‡†æµ‹è¯•ç±»
    ä¸“æ³¨äºè¯„ä¼°å‰å‘ä¼ æ’­æ€§èƒ½ï¼Œæ”¯æŒä¸åŒå½¢çŠ¶ã€æ•°æ®ç±»å‹å’Œè¿­ä»£æ¬¡æ•°çš„æµ‹è¯•
    """
    
    def __init__(self, 
                 device: str = 'cuda',
                 warmup_iters: int = 10,
                 measurement_iters: int = 100,
                 precision: str = 'float32'):
        """
        åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•é…ç½®
        
        Args:
            device: æµ‹è¯•è®¾å¤‡ ('cuda' æˆ– 'cpu')
            warmup_iters: é¢„çƒ­è¿­ä»£æ¬¡æ•°ï¼Œæ’é™¤å†·å¯åŠ¨å½±å“
            measurement_iters: æµ‹é‡è¿­ä»£æ¬¡æ•°
            precision: æµ‹è¯•ç²¾åº¦ ('float32', 'float16', 'bfloat16')
        """
        self.device = device
        self.warmup_iters = warmup_iters
        self.measurement_iters = measurement_iters
        self.precision = precision
        
        # ç²¾åº¦æ˜ å°„åˆ°torchæ•°æ®ç±»å‹
        self.dtype_map = {
            'float32': torch.float32,
            'float16': torch.float16,
            'bfloat16': torch.bfloat16
        }
        self.dtype = self.dtype_map[precision]
        
        # å­˜å‚¨æµ‹è¯•ç»“æœ
        self.results = []
    
    def _create_test_tensors(self, shape: Tuple[int, int]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        åˆ›å»ºæµ‹è¯•ç”¨çš„è¾“å…¥å¼ é‡å’Œå‚æ•°
        
        Args:
            shape: (M, N) å½¢çŠ¶ï¼ŒM=æ ·æœ¬æ•°ï¼ŒN=ç‰¹å¾ç»´åº¦
            
        Returns:
            (x, weight, bias) å…ƒç»„
        """
        M, N = shape
        x = torch.randn(M, N, device=self.device, dtype=self.dtype)
        weight = torch.randn(N, device=self.device, dtype=self.dtype)
        bias = torch.randn(N, device=self.device, dtype=self.dtype)
        return x, weight, bias
    
    def _ensure_contiguous(self, *tensors):
        """ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„"""
        return [t.contiguous() if t.is_contiguous() else t.contiguous() for t in tensors]
    
    def benchmark_triton(self, 
                        triton_layer_norm_func,
                        shape: Tuple[int, int],
                        eps: float = 1e-5) -> Dict:
        """
        æµ‹è¯•Tritonå®ç°çš„æ€§èƒ½
        
        Args:
            triton_layer_norm_func: ä½ çš„Triton LayerNormå‡½æ•°
            shape: æµ‹è¯•å½¢çŠ¶ (M, N)
            eps: LayerNormçš„epsilonå‚æ•°
            
        Returns:
            åŒ…å«æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        x, weight, bias = self._create_test_tensors(shape)
        x, weight, bias = self._ensure_contiguous(x, weight, bias)
        
        # é¢„çƒ­é˜¶æ®µ
        for _ in range(self.warmup_iters):
            _ = triton_layer_norm_func(x, weight, bias, eps)
        
        # åŒæ­¥GPUç¡®ä¿å‡†ç¡®è®¡æ—¶
        if self.device == 'cuda':
            torch.cuda.synchronize()
        
        # æ€§èƒ½æµ‹é‡é˜¶æ®µ
        start_time = time.perf_counter()
        for _ in range(self.measurement_iters):
            _ = triton_layer_norm_func(x, weight, bias, eps)
        
        if self.device == 'cuda':
            torch.cuda.synchronize()
        end_time = time.perf_counter()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time_ms = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        avg_time_ms = total_time_ms / self.measurement_iters
        M, N = shape
        total_elements = M * N * self.measurement_iters
        
        # è®¡ç®—ååé‡ (å…ƒç´ /ç§’ å’Œ æ ·æœ¬/ç§’)
        elements_per_sec = total_elements / (total_time_ms / 1000)
        samples_per_sec = M * self.measurement_iters / (total_time_ms / 1000)
        
        return {
            'impl': 'triton',
            'shape': shape,
            'dtype': self.precision,
            'avg_time_ms': avg_time_ms,
            'total_time_ms': total_time_ms,
            'elements_per_sec': elements_per_sec,
            'samples_per_sec': samples_per_sec,
            'throughput_gbps': elements_per_sec * 4 / 1e9 if self.precision == 'float32' else elements_per_sec * 2 / 1e9
        }
    
    def benchmark_pytorch(self, 
                         shape: Tuple[int, int],
                         eps: float = 1e-5) -> Dict:
        """
        æµ‹è¯•PyTorchåŸç”Ÿå®ç°çš„æ€§èƒ½
        
        Args:
            shape: æµ‹è¯•å½¢çŠ¶ (M, N)
            eps: LayerNormçš„epsilonå‚æ•°
            
        Returns:
            åŒ…å«æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        x, weight, bias = self._create_test_tensors(shape)
        x, weight, bias = self._ensure_contiguous(x, weight, bias)
        
        # é¢„çƒ­é˜¶æ®µ
        for _ in range(self.warmup_iters):
            _ = torch.nn.functional.layer_norm(x, (shape[1],), weight, bias, eps)
        
        # åŒæ­¥GPUç¡®ä¿å‡†ç¡®è®¡æ—¶
        if self.device == 'cuda':
            torch.cuda.synchronize()
        
        # æ€§èƒ½æµ‹é‡é˜¶æ®µ
        start_time = time.perf_counter()
        for _ in range(self.measurement_iters):
            _ = torch.nn.functional.layer_norm(x, (shape[1],), weight, bias, eps)
        
        if self.device == 'cuda':
            torch.cuda.synchronize()
        end_time = time.perf_counter()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time_ms = (end_time - start_time) * 1000
        avg_time_ms = total_time_ms / self.measurement_iters
        M, N = shape
        total_elements = M * N * self.measurement_iters
        
        elements_per_sec = total_elements / (total_time_ms / 1000)
        samples_per_sec = M * self.measurement_iters / (total_time_ms / 1000)
        
        return {
            'impl': 'pytorch',
            'shape': shape,
            'dtype': self.precision,
            'avg_time_ms': avg_time_ms,
            'total_time_ms': total_time_ms,
            'elements_per_sec': elements_per_sec,
            'samples_per_sec': samples_per_sec,
            'throughput_gbps': elements_per_sec * 4 / 1e9 if self.precision == 'float32' else elements_per_sec * 2 / 1e9
        }
    
    def run_comparison(self, 
                      triton_layer_norm_func,
                      test_shapes: List[Tuple[int, int]],
                      eps: float = 1e-5) -> List[Dict]:
        """
        è¿è¡Œå®Œæ•´çš„æ€§èƒ½å¯¹æ¯”æµ‹è¯•
        
        Args:
            triton_layer_norm_func: ä½ çš„Triton LayerNormå‡½æ•°
            test_shapes: è¦æµ‹è¯•çš„å½¢çŠ¶åˆ—è¡¨ [(M1, N1), (M2, N2), ...]
            eps: LayerNormçš„epsilonå‚æ•°
            
        Returns:
            æ‰€æœ‰æµ‹è¯•ç»“æœçš„åˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹LayerNormæ€§èƒ½æµ‹è¯• (è®¾å¤‡: {self.device}, ç²¾åº¦: {self.precision})")
        print(f"é¢„çƒ­è¿­ä»£: {self.warmup_iters}, æµ‹é‡è¿­ä»£: {self.measurement_iters}")
        print("=" * 80)
        
        self.results = []
        
        for shape in test_shapes:
            M, N = shape
            print(f"\nğŸ“Š æµ‹è¯•å½¢çŠ¶: [{M}, {N}] (æ€»è®¡ {M*N:,} å…ƒç´ )")
            print("-" * 60)
            
            # æµ‹è¯•PyTorchåŸç”Ÿå®ç°
            print("æµ‹è¯• PyTorch åŸç”Ÿå®ç°...")
            torch_result = self.benchmark_pytorch(shape, eps)
            self.results.append(torch_result)
            self._print_result(torch_result)
            
            # æµ‹è¯•Tritonå®ç°
            print("æµ‹è¯• Triton è‡ªå®šä¹‰å®ç°...")
            triton_result = self.benchmark_triton(triton_layer_norm_func, shape, eps)
            self.results.append(triton_result)
            self._print_result(triton_result)
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = torch_result['avg_time_ms'] / triton_result['avg_time_ms']
            throughput_ratio = triton_result['elements_per_sec'] / torch_result['elements_per_sec']
            
            print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
            print(f"  åŠ é€Ÿæ¯” (æ—¶é—´): {speedup:.2f}x")
            print(f"  ååé‡æå‡: {throughput_ratio:.2f}x")
            print(f"  Tritonå¹³å‡è€—æ—¶: {triton_result['avg_time_ms']:.3f} ms")
            print(f"  PyTorchå¹³å‡è€—æ—¶: {torch_result['avg_time_ms']:.3f} ms")
            
            if speedup > 1.0:
                print(f"  âœ… Tritonæ›´å¿«!")
            else:
                print(f"  âš ï¸ PyTorchæ›´å¿«æˆ–æŒå¹³")
        
        print("\n" + "=" * 80)
        print("ğŸ¯ æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        return self.results
    
    def _print_result(self, result: Dict):
        """æ‰“å°å•ä¸ªæµ‹è¯•ç»“æœ"""
        impl = result['impl'].upper()
        print(f"  [{impl}] å¹³å‡è€—æ—¶: {result['avg_time_ms']:.3f} ms")
        print(f"       ååé‡: {result['elements_per_sec']/1e9:.2f} Gå…ƒç´ /ç§’")
        print(f"       å¸¦å®½: {result['throughput_gbps']:.2f} GB/s")
    
    def export_results(self, filename: str = 'layernorm_benchmark_results.json'):
        """å°†æµ‹è¯•ç»“æœå¯¼å‡ºä¸ºJSONæ–‡ä»¶"""
        with open(filename, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"ğŸ“ ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ æ€§èƒ½æµ‹è¯•æ‘˜è¦")
        print("=" * 80)
        
        # æŒ‰å½¢çŠ¶åˆ†ç»„ç»“æœ
        shape_results = {}
        for result in self.results:
            shape_str = str(result['shape'])
            if shape_str not in shape_results:
                shape_results[shape_str] = []
            shape_results[shape_str].append(result)
        
        for shape_str, results in shape_results.items():
            print(f"\nå½¢çŠ¶: {shape_str}")
            print("-" * 40)
            
            # æ‰¾åˆ°Tritonå’ŒPyTorchçš„ç»“æœ
            triton_result = next(r for r in results if r['impl'] == 'triton')
            torch_result = next(r for r in results if r['impl'] == 'pytorch')
            
            speedup = torch_result['avg_time_ms'] / triton_result['avg_time_ms']
            throughput_ratio = triton_result['elements_per_sec'] / torch_result['elements_per_sec']
            
            print(f"  PyTorch: {torch_result['avg_time_ms']:.3f} ms | {torch_result['elements_per_sec']/1e9:.2f} Gå…ƒç´ /ç§’")
            print(f"  Triton:  {triton_result['avg_time_ms']:.3f} ms | {triton_result['elements_per_sec']/1e9:.2f} Gå…ƒç´ /ç§’")
            print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x | ååé‡æå‡: {throughput_ratio:.2f}x")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å¯¼å…¥ä½ å·²ç»å†™å¥½çš„LayerNormå®ç°
    # å‡è®¾ä½ çš„å®ç°åœ¨ä¸€ä¸ªå«layernorm.pyçš„æ–‡ä»¶ä¸­ï¼Œæœ‰ä¸€ä¸ªlayer_normå‡½æ•°
    try:
        from layernorm import layer_norm as triton_layer_norm
        print("âœ… æˆåŠŸå¯¼å…¥Triton LayerNormå®ç°")
    except ImportError:
        print("âš ï¸ æ— æ³•å¯¼å…¥Triton LayerNormå®ç°ï¼Œä½¿ç”¨ä¸€ä¸ªæ¨¡æ‹Ÿå‡½æ•°è¿›è¡Œæ¼”ç¤º")
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿå‡½æ•°ç”¨äºæ¼”ç¤º
        def triton_layer_norm(x, weight, bias, eps=1e-5):
            return torch.nn.functional.layer_norm(x, (x.shape[1],), weight, bias, eps)
    
    # é…ç½®æµ‹è¯•å‚æ•°
    benchmark = LayerNormBenchmark(
        device='cuda',
        warmup_iters=20,
        measurement_iters=100,
        precision='float32'  # å¯ä»¥æ”¹ä¸º 'float16' æµ‹è¯•åŠç²¾åº¦
    )
    
    # å®šä¹‰è¦æµ‹è¯•çš„å„ç§å½¢çŠ¶
    # æ ¼å¼: (M, N) å…¶ä¸­ M = batch_size * seq_len, N = hidden_size
    test_shapes = [
        (1, 768),       # å°æ‰¹é‡ï¼Œå¸¸è§hidden_size
        (32, 768),      # ä¸­ç­‰æ‰¹é‡
        (256, 768),     # å¤§æ‰¹é‡
        (1, 1024),      # å°æ‰¹é‡ï¼Œå¤§hidden_size
        (32, 1024),     # ä¸­ç­‰æ‰¹é‡ï¼Œå¤§hidden_size
        (256, 1024),    # å¤§æ‰¹é‡ï¼Œå¤§hidden_size
        (1024, 4096),    # ç±»ä¼¼å¤§æ¨¡å‹åœºæ™¯
        (2048, 8192),    # ç±»ä¼¼å¤§æ¨¡å‹åœºæ™¯

    ]
    
    # è¿è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    results = benchmark.run_comparison(
        triton_layer_norm_func=triton_layer_norm,
        test_shapes=test_shapes,
        eps=1e-5
    )
    
    # æ‰“å°æ‘˜è¦
    benchmark.print_summary()
    
    # å¯¼å‡ºç»“æœ
    benchmark.export_results()
    
    # é¢å¤–ï¼šæµ‹è¯•ä¸åŒç²¾åº¦
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯•ä¸åŒç²¾åº¦")
    print("=" * 80)
    
    for precision in ['float16', 'float32']:
        print(f"\nç²¾åº¦: {precision}")
        benchmark_fp = LayerNormBenchmark(
            device='cuda',
            warmup_iters=10,
            measurement_iters=50,
            precision=precision
        )
        
        # åªæµ‹è¯•ä¸€ä¸ªä»£è¡¨æ€§å½¢çŠ¶
        test_shape = (32, 1024)
        torch_result = benchmark_fp.benchmark_pytorch(test_shape)
        triton_result = benchmark_fp.benchmark_triton(triton_layer_norm, test_shape)
        
        speedup = torch_result['avg_time_ms'] / triton_result['avg_time_ms']
        print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")