# kernel/swiglu.py
"""
SwiGLU å‰å‘ (ç”Ÿäº§çº§ç¨³å®šç‰ˆ)
- ä¿®å¤ç›¸å¯¹è¯¯å·®è®¡ç®—å¤±çœŸé—®é¢˜
- èšç„¦ç»å¯¹è¯¯å·®ï¼ˆå·¥ä¸šæ ‡å‡†æŒ‡æ ‡ï¼‰
"""
import torch
import triton
import triton.language as tl
import torch.nn.functional as F


@triton.jit
def _fused_silu_kernel(
    gate, up, w_down, output,
    M, I, H,
    stride_gm, stride_gi,
    stride_um, stride_ui,
    stride_wdh, stride_wdi,
    stride_om, stride_oh,
    BLOCK_M: tl.constexpr = 64,
    BLOCK_N: tl.constexpr = 64,
    BLOCK_K: tl.constexpr = 128,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    for k in range(0, I, BLOCK_K):
        offs_k = k + tl.arange(0, BLOCK_K)
        
        g = tl.load(gate + offs_m[:, None] * stride_gm + offs_k[None, :] * stride_gi,
                    mask=(offs_m[:, None] < M) & (offs_k[None, :] < I), other=0.0).to(tl.float32)
        u = tl.load(up + offs_m[:, None] * stride_um + offs_k[None, :] * stride_ui,
                    mask=(offs_m[:, None] < M) & (offs_k[None, :] < I), other=0.0).to(tl.float32)
        
        g = tl.clamp(g, -20.0, 20.0)
        g_silu = g * tl.sigmoid(g)
        hidden = u * g_silu
        
        w = tl.load(w_down + offs_n[:, None] * stride_wdh + offs_k[None, :] * stride_wdi,
                    mask=(offs_n[:, None] < H) & (offs_k[None, :] < I), other=0.0).to(tl.float32)
        acc += tl.dot(hidden, tl.trans(w))
    
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < H)
    tl.store(output + offs_m[:, None] * stride_om + offs_n[None, :] * stride_oh,
             acc.to(tl.float16), mask=mask)


def fused_swiglu(x: torch.Tensor,
                 gate_weight: torch.Tensor,
                 up_weight: torch.Tensor,
                 down_weight: torch.Tensor) -> torch.Tensor:
    shape = x.shape
    x = x.view(-1, shape[-1])
    
    gate = F.linear(x, gate_weight)
    up = F.linear(x, up_weight)
    
    M, I = gate.shape
    H = down_weight.shape[0]
    output = torch.empty((M, H), device=x.device, dtype=x.dtype)
    
    grid = (triton.cdiv(M, 64), triton.cdiv(H, 64))
    _fused_silu_kernel[grid](
        gate, up, down_weight, output,
        M, I, H,
        gate.stride(0), gate.stride(1),
        up.stride(0), up.stride(1),
        down_weight.stride(0), down_weight.stride(1),
        output.stride(0), output.stride(1),
    )
    
    return output.view(shape)


def stable_silu(x: torch.Tensor) -> torch.Tensor:
    return F.silu(x.float()).half() if x.dtype == torch.float16 else F.silu(x)


# === ä¿®æ­£åçš„éªŒè¯æµç¨‹ï¼ˆèšç„¦ç»å¯¹è¯¯å·®ï¼‰===
# === ä¿®æ­£åçš„éªŒè¯æµç¨‹ ===
if __name__ == "__main__":
    torch.manual_seed(42)  # ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿å¯å¤ç°
    torch.cuda.manual_seed_all(42)
    
    M, H, I = 256, 4096, 11008
    
    print("=" * 70)
    print("SwiGLU ç²¾ç¡®éªŒè¯æµç¨‹ (ä¸Tritonè®¡ç®—æµç¨‹å®Œå…¨ä¸€è‡´)")
    print("=" * 70)
    
    # 1. ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆæ›´æ¥è¿‘å®é™…åˆ†å¸ƒï¼‰
    def generate_realistic_data():
        # è¾“å…¥ï¼šæ¨¡æ‹Ÿç»è¿‡LayerNormåçš„æ¿€æ´»å€¼
        x = torch.randn((M, H), device='cuda', dtype=torch.float32)
        x = x / (x.std() + 1e-6)  # æ ‡å‡†åŒ–åˆ°N(0,1)
        
        # æƒé‡ï¼šæ¨¡æ‹Ÿè®­ç»ƒå¥½çš„æƒé‡ï¼ˆæ›´å°èŒƒå›´ï¼‰
        weight_scale = 0.02
        gate_w = torch.randn((I, H), device='cuda', dtype=torch.float32) * weight_scale
        up_w = torch.randn((I, H), device='cuda', dtype=torch.float32) * weight_scale
        down_w = torch.randn((H, I), device='cuda', dtype=torch.float32) * weight_scale
        
        return x.half(), gate_w.half(), up_w.half(), down_w.half()
    
    x, gate_w, up_w, down_w = generate_realistic_data()
    
    print("\nã€1. è¾“å…¥æ•°æ®ç»Ÿè®¡ã€‘")
    print(f"  x: mean={x.float().mean():.3f}, std={x.float().std():.3f}, "
          f"min={x.float().min():.3f}, max={x.float().max():.3f}")
    print(f"  gate_w: mean={gate_w.float().mean():.3f}, std={gate_w.float().std():.3f}")
    print(f"  up_w: mean={up_w.float().mean():.3f}, std={up_w.float().std():.3f}")
    print(f"  down_w: mean={down_w.float().mean():.3f}, std={down_w.float().std():.3f}")
    
    # 2. Tritonå®ç°
    print("\nã€2. Tritonå®ç°ã€‘")
    y_triton = fused_swiglu(x, gate_w, up_w, down_w)
    
    # 3. ä¸Tritonè®¡ç®—æµç¨‹å®Œå…¨ä¸€è‡´çš„å‚è€ƒå®ç°
    print("\nã€3. ç²¾ç¡®å‚è€ƒå®ç° (æ¨¡æ‹ŸTritonè®¡ç®—æµç¨‹)ã€‘")
    with torch.no_grad():
        # æ­¥éª¤1: gateå’Œupçš„çŸ©é˜µä¹˜æ³•ï¼ˆFP16ï¼Œä¸Tritonä¸€è‡´ï¼‰
        gate_fp16 = F.linear(x, gate_w)
        up_fp16 = F.linear(x, up_w)
        
        # æ­¥éª¤2: è½¬æ¢åˆ°FP32è®¡ç®—ï¼ˆä¸Triton kernelä¸€è‡´ï¼‰
        gate_fp32 = gate_fp16.float()
        up_fp32 = up_fp16.float()
        
        # æ­¥éª¤3: è®¡ç®—siluå¹¶ç›¸ä¹˜ï¼ˆFP32ï¼‰
        gate_fp32 = torch.clamp(gate_fp32, -20.0, 20.0)  # ä¸Tritonä¸€è‡´
        silu_gate = gate_fp32 * torch.sigmoid(gate_fp32)
        hidden_fp32 = up_fp32 * silu_gate
        
        # æ­¥éª¤4: ä¸down_wçŸ©é˜µä¹˜æ³•ï¼ˆFP32ç´¯åŠ ï¼‰
        down_w_fp32 = down_w.float()
        y_ref_fp32 = F.linear(hidden_fp32, down_w_fp32)
        
        # æ­¥éª¤5: è½¬å›FP16
        y_ref_exact = y_ref_fp32.half()
    
    # 4. ç®€åŒ–ç‰ˆå‚è€ƒå®ç°ï¼ˆåŸå§‹æ–¹æ³•ï¼‰
    print("\nã€4. ç®€åŒ–ç‰ˆå‚è€ƒå®ç° (åŸå§‹æ–¹æ³•)ã€‘")
    with torch.no_grad():
        gate_simple = F.linear(x, gate_w)
        up_simple = F.linear(x, up_w)
        hidden_simple = up_simple * stable_silu(gate_simple)
        y_ref_simple = F.linear(hidden_simple, down_w)
    
    # 5. è¯¯å·®åˆ†æ
    print("\nã€5. è¯¯å·®åˆ†æã€‘")
    
    # è¯¯å·®1: Triton vs ç²¾ç¡®å‚è€ƒ
    err_exact = torch.max(torch.abs(y_triton - y_ref_exact)).item()
    err_mean_exact = torch.mean(torch.abs(y_triton - y_ref_exact)).item()
    
    # è¯¯å·®2: Triton vs ç®€åŒ–å‚è€ƒ
    err_simple = torch.max(torch.abs(y_triton - y_ref_simple)).item()
    
    # è¯¯å·®3: ä¸¤ç§å‚è€ƒå®ç°ä¹‹é—´çš„å·®å¼‚
    err_refs = torch.max(torch.abs(y_ref_exact - y_ref_simple)).item()
    
    print(f"  Triton vs ç²¾ç¡®å‚è€ƒ:")
    print(f"    Maxç»å¯¹è¯¯å·®: {err_exact:.6f}")
    print(f"    Meanç»å¯¹è¯¯å·®: {err_mean_exact:.6f}")
    
    print(f"  Triton vs ç®€åŒ–å‚è€ƒ:")
    print(f"    Maxç»å¯¹è¯¯å·®: {err_simple:.6f}")
    
    print(f"  ä¸¤ç§å‚è€ƒå®ç°å·®å¼‚:")
    print(f"    Maxç»å¯¹è¯¯å·®: {err_refs:.6f}")
    
    # 6. æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
    print("\nã€6. æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ã€‘")
    triton_nan = torch.isnan(y_triton).any().item()
    triton_inf = torch.isinf(y_triton).any().item()
    exact_nan = torch.isnan(y_ref_exact).any().item()
    exact_inf = torch.isinf(y_ref_exact).any().item()
    
    print(f"  Tritonè¾“å‡º - NaN: {triton_nan}, Inf: {triton_inf}")
    print(f"  ç²¾ç¡®å‚è€ƒè¾“å‡º - NaN: {exact_nan}, Inf: {exact_inf}")
    
    # 7. è¾“å‡ºç»Ÿè®¡
    print("\nã€7. è¾“å‡ºç»Ÿè®¡ã€‘")
    output_std = y_ref_exact.float().std().item()
    print(f"  è¾“å‡ºæ ‡å‡†å·®: {output_std:.3f}")
    print(f"  è¾“å‡ºèŒƒå›´: [{y_ref_exact.float().min():.3f}, {y_ref_exact.float().max():.3f}]")
    
    # 8. åˆ¤æ–­æ ‡å‡†
    print("\nã€8. éªŒè¯ç»“æœã€‘")
    
    # å·¥ä¸šæ ‡å‡†ï¼šç»å¯¹è¯¯å·® < 0.01
    if err_exact < 0.01:
        print(f"  âœ… é€šè¿‡å·¥ä¸šæ ‡å‡†æµ‹è¯•ï¼")
        print(f"     ç»å¯¹è¯¯å·® {err_exact:.6f} < 0.01")
        
        if err_exact < 1e-4:
            print(f"  ğŸ‰ ä¼˜ç§€ï¼è¯¯å·®æå° ({err_exact:.2e})")
        elif err_exact < 1e-3:
            print(f"  ğŸ‘ è‰¯å¥½ï¼è¯¯å·®å¾ˆå° ({err_exact:.2e})")
        else:
            print(f"  âš ï¸  å¯æ¥å—ï¼è¯¯å·® ({err_exact:.2e})")
    else:
        print(f"  âŒ æœªé€šè¿‡å·¥ä¸šæ ‡å‡†ï¼")
        print(f"     ç»å¯¹è¯¯å·® {err_exact:.6f} >= 0.01")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç³»ç»Ÿæ€§åå·®
    bias = torch.mean(y_triton.float() - y_ref_exact.float()).item()
    print(f"\n  ç³»ç»Ÿæ€§åå·®: {bias:.8f} (ç†æƒ³å€¼ä¸º0)")
    
    # 9. é”™è¯¯åˆ†å¸ƒ
    print("\nã€9. é”™è¯¯åˆ†å¸ƒã€‘")
    errors = (y_triton.float() - y_ref_exact.float()).abs()
    print(f"  è¯¯å·®ç™¾åˆ†ä½æ•°:")
    for p in [50, 90, 95, 99, 99.9, 100]:
        val = torch.quantile(errors, p/100.0).item()
        print(f"    {p}%: {val:.6f}")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ æœ€ç»ˆç»“è®º")
    print("=" * 70)
    
    if err_exact < 0.01 and not triton_nan and not triton_inf:
        print("  âœ… fused_swiglu å®ç°æ­£ç¡®ï¼Œå¯å®‰å…¨ç”¨äºç”Ÿäº§ç¯å¢ƒ")
        print(f"     æœ€å¤§è¯¯å·®: {err_exact:.6f}")
        print(f"     æ•°å€¼ç¨³å®š: {not (triton_nan or triton_inf)}")
    else:
        print("  âŒ éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    print("=" * 70)