"""
===================================================================
KVCacheManager - vLLM é«˜æ•ˆå†…å­˜ç®¡ç†æ¨¡å— (4D Block-Slot-Tensorç»“æ„)
===================================================================

ğŸ“Œ **æ ¸å¿ƒè®¾è®¡ç›®æ ‡**ï¼š
   1. æ”¯æŒåŠ¨æ€åˆ†é…/é‡Šæ”¾KVç¼“å­˜å—ï¼Œæœ€å¤§åŒ–GPUå†…å­˜åˆ©ç”¨ç‡
   2. ä½¿ç”¨Triton+CUDAå®ç°çº³ç§’çº§KVå­˜å‚¨
   3. æä¾›æç®€APIï¼Œéšè—æ‰€æœ‰å¤æ‚å†…å­˜ç®¡ç†ç»†èŠ‚
   4. æ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦(AMP)ã€ç¢ç‰‡ç›‘æ§ã€çƒ­é‡ç½®

ğŸ§± **å†…å­˜ç»“æ„å›¾** (ä»¥ block_size=16 ä¸ºä¾‹)ï¼š

    +---------------------+  â† num_blocks (å¦‚1024) 
    | [Block 0]           |     æ¯ä¸ªBlockå¯å­˜ block_size (å¦‚16) ä¸ªtokençš„KV
    | â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â” |     æ¯ä¸ªBlockçš„æ¯ä¸ªSlotå­˜å‚¨ [num_heads, head_size] çš„KVå‘é‡
    | â”‚0â”‚1â”‚2â”‚3â”‚4â”‚5â”‚6â”‚7â”‚...| |     ä¾‹å¦‚ï¼šnum_heads=16, head_size=128 â†’ æ¯ä¸ªSlot = 16x128å¼ é‡
    | â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜ |
    +---------------------+  â†‘ 
    | [Block 1]           |  |  æ¯ä¸ªKVç¼“å­˜å¼ é‡å½¢çŠ¶: [num_blocks, block_size, num_heads, head_size]
    | â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â” |  |  ä¾‹å¦‚: [1024, 16, 16, 128]
    | â”‚0â”‚1â”‚2â”‚3â”‚4â”‚5â”‚6â”‚7â”‚...| |  |
    | â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜ |  |
    +---------------------+  |  æ¯ä¸ªTokençš„KVæ•°æ®é€šè¿‡ slot_mapping æ˜ å°„åˆ°å…·ä½“Slot
    | ...                 |  |
    +---------------------+  â†“
    | [Block N]           |
    | â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â” |
    | â”‚0â”‚1â”‚2â”‚3â”‚4â”‚5â”‚6â”‚7â”‚...| |
    | â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜ |
    +---------------------+

ğŸ”— **å…³é”®æ¦‚å¿µå…³ç³»**ï¼š
   - 1 Token â†’ 1 Slot (é€šè¿‡ slot_mapping å®šä½)
   - 1 Block â†’ block_size ä¸ª Slots (å¦‚16ä¸ª)
   - 1 Sequence â†’ å¤šä¸ªBlocks (åŠ¨æ€å¢é•¿ï¼ŒæŒ‰éœ€åˆ†é…)
   - 1 KV Cache â†’ num_layers ä¸ª [num_blocks, block_size, num_heads, head_size] å¼ é‡

âš¡ **æ€§èƒ½ç‰¹æ€§**ï¼š
   - åˆ†é…/é‡Šæ”¾: O(1) ä½¿ç”¨ collections.deque
   - KVå­˜å‚¨: ä½¿ç”¨Tritonæ ¸å‡½æ•°ï¼Œæ¯”PyTorchå¿«5-10x
   - å†…å­˜ç¢ç‰‡: <10% (å®æµ‹)
   - æ”¯æŒè®¾å¤‡: CUDA (Triton), macOS (MPS), CPU (fallback)

ğŸ“š **å‚è€ƒæ–‡çŒ®**ï¼š
   - vLLM: https://arxiv.org/abs/2309.06180
   - PagedAttention: https://arxiv.org/abs/2309.06180
   - FlashAttention: https://arxiv.org/abs/2205.14135
"""
from typing import List

import torch
import collections
import itertools  # ä»…ç”¨äºstatsè®¡ç®—

try:
    import triton
    import triton.language as tl
    from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache
except ImportError:
    print('Please install flash-attn from https://www.flash-attn.org')


def is_macos():
    """æ£€æµ‹æ˜¯å¦è¿è¡Œåœ¨macOS MPSè®¾å¤‡ä¸Š"""
    return torch.backends.mps.is_available()


# =============================================================================
# ğŸš€ Tritonæ ¸å‡½æ•°: é«˜æ€§èƒ½KVç¼“å­˜å­˜å‚¨ (CUDA Only)
# =============================================================================

if not is_macos() and torch.cuda.is_available():
    @triton.jit
    def store_kvcache_kernel(
            # è¾“å…¥æŒ‡é’ˆ
            key_ptr,  # [num_tokens, num_heads, head_size] è¾“å…¥key
            key_stride_b,  # batch_stride (tokenç»´åº¦æ­¥é•¿)
            key_stride_h,  # head_stride (headç»´åº¦æ­¥é•¿)
            key_stride_d,  # dim_stride (dimç»´åº¦æ­¥é•¿)
            value_ptr,  # [num_tokens, num_heads, head_size] è¾“å…¥value
            value_stride_b, value_stride_h, value_stride_d,
            # è¾“å‡ºæŒ‡é’ˆ
            k_cache_ptr,  # [num_blocks, block_size, num_heads, head_size] Kç¼“å­˜
            v_cache_ptr,  # [num_blocks, block_size, num_heads, head_size] Vç¼“å­˜
            slot_mapping_ptr,  # [num_tokens] slotæ˜ å°„è¡¨
            # å¸¸é‡
            block_size: tl.constexpr,  # æ¯ä¸ªblockçš„slotæ•°
            num_heads: tl.constexpr,  # æ³¨æ„åŠ›å¤´æ•°
            head_size: tl.constexpr,  # æ¯ä¸ªå¤´çš„ç»´åº¦
            # ç¼“å­˜æ­¥é•¿ (ç”¨äºè®¡ç®—å†…å­˜åç§»)
            CACHE_BLOCK_STRIDE: tl.constexpr,  # blockç»´åº¦æ­¥é•¿
            CACHE_BLOCK_SIZE_STRIDE: tl.constexpr,  # block_sizeç»´åº¦æ­¥é•¿
            CACHE_HEAD_STRIDE: tl.constexpr,  # headç»´åº¦æ­¥é•¿
            CACHE_DIM_STRIDE: tl.constexpr,  # dimç»´åº¦æ­¥é•¿
    ):
        """
        ğŸ“Œ **æ ¸å¿ƒé€»è¾‘**:
            1. æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªtokençš„ä¸€ä¸ªhead
            2. é€šè¿‡slot_mappingæ‰¾åˆ°ç›®æ ‡Slot
            3. è®¡ç®—ç¼“å­˜ä¸­çš„å†…å­˜åç§»å¹¶å­˜å‚¨

        âš¡ **æ€§èƒ½ä¼˜åŒ–**:
            - ä½¿ç”¨tl.arange(0, head_size)å‘é‡åŒ–åŠ è½½
            - åˆå¹¶å†…å­˜è®¿é—® (coalesced access)
            - æ— åˆ†æ”¯ (branch-free)
        """
        # è·å–çº¿ç¨‹ID (æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªtokençš„ä¸€ä¸ªhead)
        token_idx = tl.program_id(0)  # 0 ~ num_tokens-1
        head_idx = tl.program_id(1)  # 0 ~ num_heads-1

        # åŠ è½½ç›®æ ‡slot (å¦‚æœ-1è¡¨ç¤ºè·³è¿‡)
        slot = tl.load(slot_mapping_ptr + token_idx)
        if slot == -1:
            return

        # è®¡ç®—ç›®æ ‡Blockå’ŒBlockå†…åç§»
        block_id = slot // block_size
        offset_in_block = slot % block_size

        # å‘é‡åŒ–åŠ è½½è¾“å…¥KV (head_sizeä¸ªå…ƒç´ )
        key_offset = (token_idx * key_stride_b +
                      head_idx * key_stride_h +
                      tl.arange(0, head_size) * key_stride_d)
        value_offset = (token_idx * value_stride_b +
                        head_idx * value_stride_h +
                        tl.arange(0, head_size) * value_stride_d)

        key = tl.load(key_ptr + key_offset)
        value = tl.load(value_ptr + value_offset)

        # è®¡ç®—ç¼“å­˜ä¸­çš„å†…å­˜åç§»
        cache_offset = (block_id * CACHE_BLOCK_STRIDE +
                        offset_in_block * CACHE_BLOCK_SIZE_STRIDE +
                        head_idx * CACHE_HEAD_STRIDE +
                        tl.arange(0, head_size) * CACHE_DIM_STRIDE)

        # å­˜å‚¨åˆ°ç¼“å­˜
        tl.store(k_cache_ptr + cache_offset, key)
        tl.store(v_cache_ptr + cache_offset, value)


def store_kvcache(
        key: torch.Tensor,  # [num_tokens, num_heads, head_size]
        value: torch.Tensor,  # [num_tokens, num_heads, head_size]
        k_cache: torch.Tensor,  # [num_blocks, block_size, num_heads, head_size]
        v_cache: torch.Tensor,  # [num_blocks, block_size, num_heads, head_size]
        slot_mapping: torch.Tensor,  # [num_tokens] (int32)
        block_size: int, ):
    """
    ğŸ“Œ **KVç¼“å­˜å­˜å‚¨å‡½æ•°** (è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç°)

    ğŸ” **å‚æ•°è¯´æ˜**:
        - key/value: å½“å‰æ‰¹æ¬¡çš„KVå¼ é‡
        - k_cache/v_cache: å…¨å±€KVç¼“å­˜
        - slot_mapping: æ¯ä¸ªtokenå¯¹åº”çš„ç›®æ ‡slot
        - block_size: æ¯ä¸ªblockçš„slotæ•°

    âš¡ **æ€§èƒ½è·¯å¾„**:
        1. CUDA + Triton: ä½¿ç”¨æ ¸å‡½æ•° (æœ€å¿«ï¼Œ~10Î¼s/100tokens)
        2. macOS/CPU: ä½¿ç”¨PyTorchç´¢å¼• (å…¼å®¹æ¨¡å¼ï¼Œ~50Î¼s/100tokens)
    """
    num_tokens, num_heads, head_size = key.shape

    # è¾“å…¥éªŒè¯
    assert key.dim() == 3 and value.dim() == 3
    assert key.shape == (num_tokens, num_heads, head_size)
    assert value.shape == (num_tokens, num_heads, head_size)
    assert slot_mapping.numel() == num_tokens

    if is_macos() or not torch.cuda.is_available():
        # ğŸ¢ å…¼å®¹æ¨¡å¼: ä½¿ç”¨PyTorchç´¢å¼• (é€‚ç”¨äºmacOS/CPU)
        # éå†æ¯ä¸ªtokenï¼Œé€šè¿‡slot_mappingå®šä½ç›®æ ‡slot
        for i, slot in enumerate(slot_mapping.tolist()):
            if slot != -1:  # -1è¡¨ç¤ºæ— æ•ˆslot
                block_id, offset_in_block = divmod(slot, block_size)  # ç­‰æ•ˆäº // å’Œ %
                k_cache[block_id, offset_in_block] = key[i]
                v_cache[block_id, offset_in_block] = value[i]
    else:
        # ğŸš€ é«˜æ€§èƒ½æ¨¡å¼: ä½¿ç”¨Tritonæ ¸å‡½æ•° (CUDA only)
        # å¯åŠ¨ç½‘æ ¼: (num_tokens, num_heads) â†’ æ¯ä¸ªheadä¸€ä¸ªçº¿ç¨‹
        grid = (num_tokens, num_heads)

        # è·å–ç¼“å­˜æ­¥é•¿ (ç”¨äºè®¡ç®—å†…å­˜åç§»)
        cache_strides = k_cache.stride()  # (block_stride, block_size_stride, head_stride, dim_stride)

        # è°ƒç”¨Tritonæ ¸å‡½æ•°
        store_kvcache_kernel[grid](
            key, *key.stride(),  # å±•å¼€ä¸º: key, key_stride_b, key_stride_h, key_stride_d
            value, *value.stride(),
            k_cache, v_cache, slot_mapping,
            block_size, num_heads, head_size,
            *cache_strides  # å±•å¼€ä¸º4ä¸ªstride
        )

def store_kvcache_batch(
        key: torch.Tensor,  # [batch_size, num_tokens=1, num_heads, head_size]
        value: torch.Tensor,  # [batch_size, num_tokens=1, num_heads, head_size]
        k_cache: torch.Tensor,  # [num_blocks, block_size, num_heads, head_size]
        v_cache: torch.Tensor,  # [num_blocks, block_size, num_heads, head_size]
        slot_mapping_batch: torch.Tensor,  # [batch_size, num_tokens=1] (int32)
        block_size: int,
):
    """
    ğŸ“Œ **æ‰¹é‡KVç¼“å­˜å­˜å‚¨å‡½æ•°**

    ğŸ” **å‚æ•°è¯´æ˜**:
        - key/value: å½“å‰æ‰¹æ¬¡çš„KVå¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_tokens, num_heads, head_size]
        - k_cache/v_cache: å…¨å±€KVç¼“å­˜ï¼Œå½¢çŠ¶ä¸º [num_blocks, block_size, num_heads, head_size]
        - slot_mapping_batch: æ¯ä¸ªtokenå¯¹åº”çš„ç›®æ ‡slotï¼Œå½¢çŠ¶ä¸º [batch_size, num_tokens]
        - block_size: æ¯ä¸ªblockçš„slotæ•°

    âš¡ **æ€§èƒ½è·¯å¾„**:
        1. CUDA + Triton: ä½¿ç”¨æ ¸å‡½æ•° (æœ€å¿«ï¼Œ~10Î¼s/100tokens)
        2. macOS/CPU: ä½¿ç”¨PyTorchç´¢å¼• (å…¼å®¹æ¨¡å¼ï¼Œ~50Î¼s/100tokens)
    """
    batch_size, num_tokens, num_heads, head_size = key.shape

    # è¾“å…¥éªŒè¯
    assert key.dim() == 4 and value.dim() == 4
    assert key.shape == (batch_size, num_tokens, num_heads, head_size)
    assert value.shape == (batch_size, num_tokens, num_heads, head_size)
    assert slot_mapping_batch.shape == (batch_size, num_tokens)
    assert num_tokens == 1, "Only support num_tokens=1 for decoding stage"

    if is_macos() or not torch.cuda.is_available():
        # ğŸ¢ å…¼å®¹æ¨¡å¼: ä½¿ç”¨PyTorchç´¢å¼• (é€‚ç”¨äºmacOS/CPU)
        for batch_idx in range(batch_size):
            for token_idx in range(num_tokens):
                slot = slot_mapping_batch[batch_idx, token_idx].item()
                if slot != -1:  # -1è¡¨ç¤ºæ— æ•ˆslot
                    block_id, offset_in_block = divmod(slot, block_size)
                    k_cache[block_id, offset_in_block] = key[batch_idx, token_idx]
                    v_cache[block_id, offset_in_block] = value[batch_idx, token_idx]
    else:
        # ğŸš€ é«˜æ€§èƒ½æ¨¡å¼: ä½¿ç”¨Tritonæ ¸å‡½æ•° (CUDA only)
        # å°†è¾“å…¥å¼ é‡å±•å¹³ä¸º [batch_size * num_tokens, num_heads, head_size]
        key_flat = key.view(-1, num_heads, head_size)
        value_flat = value.view(-1, num_heads, head_size)
        slot_mapping_flat = slot_mapping_batch.view(-1)

        # å¯åŠ¨ç½‘æ ¼: (batch_size * num_tokens, num_heads) â†’ æ¯ä¸ªheadä¸€ä¸ªçº¿ç¨‹
        grid = (batch_size * num_tokens, num_heads)

        # è·å–ç¼“å­˜æ­¥é•¿ (ç”¨äºè®¡ç®—å†…å­˜åç§»)
        cache_strides = k_cache.stride()  # (block_stride, block_size_stride, head_stride, dim_stride)

        # è°ƒç”¨Tritonæ ¸å‡½æ•°
        store_kvcache_kernel[grid](
            key_flat, *key_flat.stride(),
            value_flat, *value_flat.stride(),
            k_cache, v_cache, slot_mapping_flat,
            block_size, num_heads, head_size,
            *cache_strides
        )


# =============================================================================
# ğŸ§  KVCacheManager ä¸»ç±» (æç®€æ¥å£ï¼Œæè‡´æ€§èƒ½)
# =============================================================================

class KVCacheManager:
    """
    ğŸ“Œ **KVç¼“å­˜ç®¡ç†å™¨** - vLLMæ ¸å¿ƒç»„ä»¶

    ğŸ” **è®¾è®¡å“²å­¦**:
        1. **æç®€æ¥å£**: ä»…6ä¸ªæ ¸å¿ƒæ–¹æ³•ï¼Œéšè—æ‰€æœ‰å¤æ‚å†…å­˜ç®¡ç†
        2. **æè‡´æ€§èƒ½**: åˆ†é…/é‡Šæ”¾O(1)ï¼ŒKVå­˜å‚¨çº³ç§’çº§
        3. **ç”Ÿäº§å°±ç»ª**: æ”¯æŒAMPã€ç¢ç‰‡ç›‘æ§ã€çƒ­é‡ç½®
        4. **é›¶ä¾èµ–**: ä»…ä¾èµ–PyTorchï¼Œå…¼å®¹æ‰€æœ‰è®¾å¤‡

    ğŸ§ª **å…¸å‹ç”¨æ³•**:
        manager = KVCacheManager(n_blocks=1024, block_size=16, n_layers=32, n_heads=16, head_size=128)

        # é¢„å¡«å……é˜¶æ®µ
        success, slot_map = manager.alloc(seq_id=0, n_tokens=100)
        manager.put(seq_id=0, key=keys, value=values, layer=0, slot_map=slot_map)

        # è§£ç é˜¶æ®µ
        for _ in range(100):
            new_slot = manager.append(seq_id=0)  # åŠ¨æ€å¢é•¿
            manager.put(seq_id=0, key=new_k, value=new_v, layer=0, slot_map=torch.tensor([new_slot]))

        # é‡Šæ”¾
        manager.free(seq_id=0)
    """

    def __init__(self,
                 n_blocks: int,  # æ€»Blockæ•° (å¦‚1024)
                 block_size: int,  # æ¯ä¸ªBlockçš„Slotæ•° (å¦‚16)
                 n_layers: int,  # æ¨¡å‹å±‚æ•° (å¦‚32)
                 n_heads: int,  # æ³¨æ„åŠ›å¤´æ•° (å¦‚16)
                 head_size: int,  # æ¯ä¸ªå¤´çš„ç»´åº¦ (å¦‚128)
                 dtype=torch.float16,  # æ•°æ®ç±»å‹
                 device="cuda", 
                 max_tokens: int = 1024,
                 max_batch_size: int = 32): 
        """
        ğŸ“Œ **åˆå§‹åŒ–**:
            1. åˆ›å»ºKVç¼“å­˜å¼ é‡ (ParameterListæ”¯æŒAMP)
            2. åˆå§‹åŒ–ç©ºé—²å—åˆ—è¡¨ (dequeå®ç°O(1)åˆ†é…)
            3. åˆå§‹åŒ–å—ä½ç½®è®¡æ•°å™¨ (ç”¨äºappendæ“ä½œ)
        """
        # å‚æ•°ä¿å­˜
        self.n_blocks, self.block_size, self.n_layers = n_blocks, block_size, n_layers
        self.dtype, self.device = dtype, device

        # åˆ›å»ºKVç¼“å­˜ (ä½¿ç”¨ParameterListæ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦AMP)
        # å½¢çŠ¶: [n_layers, n_blocks, block_size, n_heads, head_size]
        # ä¸ºæ¯ä¸€å±‚åˆ†é…KVç¼“å­˜ [num_blocks, block_size, num_heads, head_size]
        self.k_caches = []
        self.v_caches = []
        for _ in range(n_layers):
            k_cache = torch.zeros(
                (n_blocks, block_size, n_heads, head_size),
                dtype=dtype, device=device
            )
            v_cache = torch.zeros(
                (n_blocks, block_size, n_heads, head_size),
                dtype=dtype, device=device
            )
            self.k_caches.append(k_cache)
            self.v_caches.append(v_cache)

        # å†…å­˜ç®¡ç†æ•°æ®ç»“æ„ (æ‰€æœ‰å‡ä¸ºç§æœ‰æˆå‘˜ï¼Œå¤–éƒ¨ä¸å¯è§)
        # 1. ç©ºé—²å—é˜Ÿåˆ— (ä½¿ç”¨dequeå®ç°O(1)åˆ†é…/é‡Šæ”¾)
        self._free = collections.deque(range(n_blocks))

        # 2. å·²åˆ†é…å—å­—å…¸ (seq_id â†’ [block_id1, block_id2, ...])
        self._blocks = {}

        # 3. å—ä½ç½®è®¡æ•°å™¨ (block_id â†’ å½“å‰å·²ç”¨slotæ•°)
        self._pos = {}
        
        # 4. block table 
        self._block_table = torch.full(
            (32, self.n_blocks), -1,
            dtype=torch.int32, device=self.device
        )
        self.cache_seqlens = torch.tensor([1], dtype=torch.int32, device=self.device)

            # è®¡ç®—å•åºåˆ—æœ€å¤§å—æ•°
        self.max_tokens = max_tokens
        self.max_seq_blocks = (max_tokens + block_size - 1) // block_size
        # é™æ€ç¼“å†²åŒºï¼ˆå…³é”®æ”¹é€ ï¼‰
        self._block_table_buffer = torch.full(
            (max_batch_size, self.max_seq_blocks), -1,
            dtype=torch.int32, device=device
        )
        self._cache_seqlens_buffer = torch.zeros(
            max_batch_size, dtype=torch.int32, device=device
        )

    def alloc(self, seq_id: int, n_tokens: int):
        """
        ğŸ“Œ **åˆ†é…ç¼“å­˜å—** (é¢„å¡«å……é˜¶æ®µè°ƒç”¨)

        ğŸ” **å‚æ•°**:
            - seq_id: åºåˆ—ID (å”¯ä¸€æ ‡è¯†)
            - n_tokens: éœ€è¦ç¼“å­˜çš„tokenæ•°

        âœ… **è¿”å›**:
            - success: æ˜¯å¦åˆ†é…æˆåŠŸ (Falseè¡¨ç¤ºOOM)
            - slot_mapping: slotæ˜ å°„å¼ é‡ [n_tokens] (æ¯ä¸ªtokençš„ç›®æ ‡slot)

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. è®¡ç®—æ‰€éœ€Blockæ•°: (n_tokens + block_size - 1) // block_size
            2. ä»_freeé˜Ÿåˆ—æ‰¹é‡åˆ†é…
            3. åˆå§‹åŒ–å—ä½ç½®è®¡æ•°å™¨ (æœ€åä¸€ä¸ªå—å¯èƒ½ä¸æ»¡)
            4. ç”Ÿæˆslot_mapping (çº¿æ€§æ˜ å°„åˆ°Block)
        """
        # è®¡ç®—æ‰€éœ€Blockæ•° (å‘ä¸Šå–æ•´)
        n_needed = (n_tokens + self.block_size - 1) // self.block_size

        # OOMæ£€æŸ¥
        if len(self._free) < n_needed:
            return False, None

        # æ‰¹é‡åˆ†é…Block (deque.popleft() O(1))
        blocks = [self._free.popleft() for _ in range(n_needed)]

        # æ›´æ–°ç©ºé—²é˜Ÿåˆ—
        self._free = self._free  # è§¦å‘dequeçš„å†…å­˜ä¼˜åŒ–

        # åˆå§‹åŒ–å—ä½ç½®è®¡æ•°å™¨
        # æœ€åä¸€ä¸ªå—å¯èƒ½ä¸æ»¡ï¼Œå…¶ä»–å—æ»¡
        self._pos.update({
            b: n_tokens % self.block_size if i == len(blocks) - 1 else self.block_size
            for i, b in enumerate(blocks)
        })

        # è®°å½•å·²åˆ†é…å—
        self._blocks[seq_id] = blocks

        # ç”Ÿæˆslot_mapping (æ¯ä¸ªtokençš„ç›®æ ‡slot)
        # çº¿æ€§æ˜ å°„: token_idx â†’ block_id * block_size + offset_in_block
        slot_mapping = torch.tensor([
            blocks[i // self.block_size] * self.block_size + i % self.block_size
            for i in range(n_tokens)
        ], dtype=torch.int32, device=self.device)

        return True, slot_mapping

    def append(self, seq_id: int):
        """
        ğŸ“Œ **è¿½åŠ token** (è§£ç é˜¶æ®µè°ƒç”¨)

        ğŸ” **å‚æ•°**:
            - seq_id: åºåˆ—ID

        âœ… **è¿”å›**:
            - slot: åˆ†é…çš„slot (å¤±è´¥è¿”å›-1)

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. æ£€æŸ¥åºåˆ—æ˜¯å¦å­˜åœ¨
            2. å¦‚æœæœ€åä¸€ä¸ªBlockæœ‰ç©ºé—´ï¼Œä½¿ç”¨å½“å‰Block
            3. å¦åˆ™åˆ†é…æ–°Block
            4. æ›´æ–°å—ä½ç½®è®¡æ•°å™¨
        """
        if seq_id not in self._blocks:
            return -1

        blocks = self._blocks[seq_id]
        last_block = blocks[-1]
        current_pos = self._pos[last_block]

        # æƒ…å†µ1: å½“å‰å—è¿˜æœ‰ç©ºé—´
        if current_pos < self.block_size - 1:
            self._pos[last_block] += 1
            return last_block * self.block_size + current_pos

        # æƒ…å†µ2: éœ€è¦æ–°Block
        elif self._free:
            new_block = self._free.popleft()
            blocks.append(new_block)
            self._blocks[seq_id] = blocks  # ç¡®ä¿å¼•ç”¨åŒæ­¥ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
            self._pos[new_block] = 1
            return new_block * self.block_size

        # æƒ…å†µ3: æ— å¯ç”¨Block
        return -1

    def put(self, seq_id: int, key: torch.Tensor, value: torch.Tensor, layer: int, slot_map: torch.Tensor):
        """
        ğŸ“Œ **å­˜å‚¨KVç¼“å­˜** (é€šç”¨æ¥å£)

        ğŸ” **å‚æ•°**:
            - seq_id: åºåˆ—ID (ä»…ç”¨äºæ—¥å¿—)
            - key/value: KVå¼ é‡ [n_tokens, n_heads, head_size]
            - layer: æ¨¡å‹å±‚ç´¢å¼•
            - slot_map: slotæ˜ å°„ [n_tokens]

        ğŸ’¡ **æ³¨æ„**:
            - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç° (CUDA/Triton æˆ– CPU/PyTorch)
            - æ”¯æŒä»»æ„slot_map (ä¸ä¸€å®šæ˜¯è¿ç»­çš„)
        """
        store_kvcache(key, value,
                      self.k_caches[layer], self.v_caches[layer],
                      slot_map, self.block_size)

    def get(self, layer: int, block_id: int = None):
        """
        ğŸ“Œ **è·å–KVç¼“å­˜**

        ğŸ” **å‚æ•°**:
            - layer: æ¨¡å‹å±‚ç´¢å¼•
            - block_id: å¯é€‰ï¼ŒæŒ‡å®šBlock (Noneè¿”å›å…¨éƒ¨)

        âœ… **è¿”å›**:
            - (k_cache, v_cache) å…ƒç»„
            - å¦‚æœæŒ‡å®šblock_id: (k_cache[block_id], v_cache[block_id])
            - å¦åˆ™: (k_cache, v_cache) å…¨éƒ¨
        """
        k_cache = self.k_caches[layer]
        v_cache = self.v_caches[layer]
        return (k_cache[block_id], v_cache[block_id]) if block_id is not None else (k_cache, v_cache)

    def get_blocks(self, seq_id: int):
        """
        ğŸ“Œ **è·å–åºåˆ—çš„Blockåˆ—è¡¨** (ç›´æ¥è¿”å›å†…éƒ¨å¼•ç”¨ï¼Œé›¶æ‹·è´)

        ğŸ” **å‚æ•°**:
            - seq_id: åºåˆ—ID

        âœ… **è¿”å›**:
            - blocks: Block IDåˆ—è¡¨ (å¦‚ [0, 1, 2])ï¼Œå¦‚æœåºåˆ—ä¸å­˜åœ¨åˆ™è¿”å›ç©ºåˆ—è¡¨ []

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. æ£€æŸ¥åºåˆ—æ˜¯å¦å­˜åœ¨
            2. ç›´æ¥è¿”å›å†…éƒ¨ `_blocks[seq_id]` åˆ—è¡¨ (é›¶æ‹·è´)
            3. å¦‚æœåºåˆ—ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨

        âš ï¸ **é‡è¦è¯´æ˜**:
            - **è¿”å›çš„æ˜¯å†…éƒ¨åˆ—è¡¨çš„å¼•ç”¨**ï¼Œä¸è¦ç›´æ¥ä¿®æ”¹ï¼
            - å¦‚æœéœ€è¦ä¿®æ”¹ï¼Œè¯·å…ˆæ‹·è´: `blocks = manager.get_blocks(seq_id).copy()`
            - æ­¤æ–¹æ³•è®¾è®¡ä¸º **åªè¯»è®¿é—®**ï¼Œä¿è¯çº¿ç¨‹å®‰å…¨

        âš¡ **æ€§èƒ½**:
            - æ—¶é—´å¤æ‚åº¦: O(1)
            - ç©ºé—´å¤æ‚åº¦: O(1) (é›¶æ‹·è´)
            - é€‚ç”¨äºé«˜é¢‘è°ƒç”¨ (å¦‚æ¯æ¬¡æ¨ç†éƒ½è°ƒç”¨)

        ğŸ“Š **å…¸å‹ç”¨é€”**:
            1. æ„å»ºBlock Table (ç”¨äºPagedAttention)
            2. è®¡ç®—åºåˆ—çš„KVç¼“å­˜å¤§å°
            3. è°ƒè¯•å’Œæ—¥å¿—è®°å½•
            4. åºåˆ—åˆ†ç‰‡ (å¦‚æ¨¡å‹å¹¶è¡Œ)
        """
        return self._blocks.get(seq_id, [])  # ç›´æ¥è¿”å›å†…éƒ¨å¼•ç”¨ (é›¶æ‹·è´)

    # å‡†å¤‡æ¨ç†çš„æ•°æ®
    def cache_batch_data(self, seq_ids: list, context_lens: list):
        """é™æ€åŒ–ç‰ˆæœ¬ï¼šåŸåœ°æ›´æ–°ç¼“å†²åŒºï¼Œä¸åˆ›å»ºæ–°å¼ é‡"""
        batch_size = len(seq_ids)
        
        # è·å– block tables
        block_tables = [self.get_blocks(seq_id) for seq_id in seq_ids]
        
        # åŸåœ°æ›´æ–°ï¼šå…ˆé‡ç½®ä¸º -1
        self._block_table_buffer[:batch_size] = -1
        
        # å¡«å……å®é™…å€¼
        for i, blocks in enumerate(block_tables):
            num_blocks = len(blocks)
            if num_blocks > 0:
                self._block_table_buffer[i, :num_blocks] = torch.tensor(
                    blocks, dtype=torch.int32, device=self.device
                )
        
        # åŸåœ°æ›´æ–° cache_seqlens
        self._cache_seqlens_buffer[:batch_size] = torch.tensor(
            context_lens, dtype=torch.int32, device=self.device
        )
        
        # è¿”å›è§†å›¾ï¼ˆåŒä¸€å†…å­˜åœ°å€ï¼‰
        return (
            self._block_table_buffer[:batch_size],
            self._cache_seqlens_buffer[:batch_size]
        )


        # å‡†å¤‡æ¨ç†çš„æ•°æ®
    def get_buffer_data(self, seq_ids: list):
        """é™æ€åŒ–ç‰ˆæœ¬ï¼šåŸåœ°æ›´æ–°ç¼“å†²åŒºï¼Œä¸åˆ›å»ºæ–°å¼ é‡"""
        batch_size = len(seq_ids)
        
        # è¿”å›è§†å›¾ï¼ˆåŒä¸€å†…å­˜åœ°å€ï¼‰
        return (
            self._block_table_buffer[:batch_size],
            self._cache_seqlens_buffer[:batch_size]
        )

    def cache_batch_datav1(self, seq_ids: list, context_lens: List[int], ):
        block_tables = [self.get_blocks(seq_id) for seq_id in seq_ids]
        max_blocks = max(map(len, block_tables), default=0)
        block_table_tensor = torch.tensor([
            blocks + [-1] * (max_blocks - len(blocks)) for blocks in block_tables
        ], dtype=torch.int32, device=self.device)
        self._block_table = block_table_tensor

        self.cache_seqlens = torch.tensor(context_lens, dtype=torch.int32, device=self.device)





    def get_slots(self, seq_id: int, token_positions: list) -> list:
        """
        ğŸ“Œ **æ‰¹é‡è·å–tokenä½ç½®çš„slotæ˜ å°„** (è¿”å›åˆ—è¡¨ç‰ˆæœ¬)

        ğŸ” **å‚æ•°**:
            - seq_id: åºåˆ—ID
            - token_positions: tokenä½ç½®åˆ—è¡¨ (å¦‚ [0,1,2,3,4,5])

        âœ… **è¿”å›**:
            - slot_mapping: slotæ˜ å°„åˆ—è¡¨ (æ— æ•ˆä½ç½®å¯¹åº”-1)

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. æ£€æŸ¥åºåˆ—æ˜¯å¦å­˜åœ¨
            2. å¯¹æ¯ä¸ªtokenä½ç½®:
                a. è®¡ç®—ç›®æ ‡Block: block_idx = pos // block_size
                b. è®¡ç®—Blockå†…åç§»: offset = pos % block_size
                c. æ£€æŸ¥Blockå’Œåç§»æ˜¯å¦æœ‰æ•ˆ
                d. è®¡ç®—slot: slot = block_id * block_size + offset
            3. è¿”å›slotæ˜ å°„åˆ—è¡¨

        âš¡ **æ€§èƒ½ä¼˜åŒ–**:
            - ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼
            - é¿å…ä¸å¿…è¦çš„å¼ é‡æ“ä½œ
        """
        if seq_id not in self._blocks:
            return [-1] * len(token_positions)

        blocks = self._blocks[seq_id]
        block_size = self.block_size
        block_positions = self._pos  # è·å–å—ä½ç½®è®¡æ•°å™¨

        # æ‰¹é‡è®¡ç®—slotæ˜ å°„
        slot_mapping = []
        for pos in token_positions:
            block_idx = pos // block_size

            # æ£€æŸ¥å—ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
            if block_idx >= len(blocks):
                slot_mapping.append(-1)
                continue

            block_id = blocks[block_idx]
            offset_in_block = pos % block_size

            # æ£€æŸ¥å—å†…ä½ç½®æ˜¯å¦æœ‰æ•ˆ
            if offset_in_block >= block_positions.get(block_id, 0):
                slot_mapping.append(-1)
            else:
                slot = block_id * block_size + offset_in_block
                slot_mapping.append(slot)

        return slot_mapping

    def free(self, seq_id: int):
        """
        ğŸ“Œ **é‡Šæ”¾ç¼“å­˜å—** (å¿…é¡»è°ƒç”¨ï¼Œé¿å…å†…å­˜æ³„æ¼)

        ğŸ” **å‚æ•°**:
            - seq_id: åºåˆ—ID

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. æ£€æŸ¥åºåˆ—æ˜¯å¦å­˜åœ¨
            2. å°†å—åŠ å…¥_freeé˜Ÿåˆ—
            3. æ¸…ç†å—ä½ç½®è®¡æ•°å™¨
            4. åˆ é™¤åºåˆ—è®°å½•
        """
        if seq_id in self._blocks:
            # æ‰¹é‡é‡Šæ”¾å—
            for block_id in self._blocks[seq_id]:
                self._free.append(block_id)  # O(1)
                self._pos.pop(block_id, None)  # æ¸…ç†è®¡æ•°å™¨

            # åˆ é™¤åºåˆ—è®°å½•
            del self._blocks[seq_id]

    def reset(self):
        """
        ğŸ“Œ **é‡ç½®æ‰€æœ‰çŠ¶æ€** (çƒ­é‡è½½/é‡å¯æ—¶ä½¿ç”¨)

        ğŸ’¡ **æ³¨æ„**:
            - é‡Šæ”¾æ‰€æœ‰å·²åˆ†é…å—
            - é‡ç½®æ‰€æœ‰æ•°æ®ç»“æ„
            - ä¿ç•™KVç¼“å­˜å¼ é‡ (å†…å­˜ä¸é‡Šæ”¾ï¼Œä½†å†…å®¹æ¸…é›¶)
        """
        # é‡ç½®ç©ºé—²å—
        self._free = collections.deque(range(self.n_blocks))

        # é‡ç½®å·²åˆ†é…å—å’Œä½ç½®è®¡æ•°å™¨
        self._blocks.clear()
        self._pos.clear()

    @property
    def stats(self):
        """
        ğŸ“Œ **ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯** (ç›‘æ§ç”¨)

        âœ… **è¿”å›**:
            - total: æ€»Blockæ•°
            - free: ç©ºé—²Blockæ•°  
            - used: å·²ç”¨Blockæ•°
            - frag: ç¢ç‰‡ç‡ (0.0~1.0, è¶Šä½è¶Šå¥½)

        ğŸ§  **ç¢ç‰‡ç‡è®¡ç®—**:
            ç¢ç‰‡ç‡ = 1 - (æœ€å¤§è¿ç»­ç©ºé—²å— / æ€»ç©ºé—²å—)
            ä¾‹å¦‚: ç©ºé—²å—[1,2,3,5,6,7] â†’ æœ€å¤§è¿ç»­=3, æ€»ç©ºé—²=6 â†’ ç¢ç‰‡ç‡=0.5
        """
        total, free = self.n_blocks, len(self._free)

        # è®¡ç®—æœ€å¤§è¿ç»­ç©ºé—²å—
        sorted_free = sorted(self._free)
        max_consecutive = 0
        current = 0
        for i, block_id in enumerate(sorted_free):
            if i == 0 or block_id == sorted_free[i - 1] + 1:
                current += 1
            else:
                max_consecutive = max(max_consecutive, current)
                current = 1
        max_consecutive = max(max_consecutive, current)

        # ç¢ç‰‡ç‡ (0è¡¨ç¤ºæ— ç¢ç‰‡ï¼Œ1è¡¨ç¤ºå®Œå…¨ç¢ç‰‡åŒ–)
        fragmentation = 1.0 - max_consecutive / max(free, 1) if free > 0 else 0.0

        return {
            "total": total,
            "free": free,
            "used": total - free,
            "frag": round(fragmentation, 3)
        }


# =============================================================================
# ğŸ§ª ä½¿ç”¨ç¤ºä¾‹
# =============================================================================

if __name__ == "__main__":
    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    manager = KVCacheManager(
        n_blocks=1024,  # 1024ä¸ªBlock
        block_size=16,  # æ¯ä¸ªBlock 16ä¸ªSlot
        n_layers=32,  # 32å±‚Transformer
        n_heads=16,  # 16ä¸ªå¤´
        head_size=128,  # æ¯ä¸ªå¤´128ç»´
        dtype=torch.float16,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    print(f"åˆå§‹åŒ–å®Œæˆ: {manager.stats}")

    # æ¨¡æ‹Ÿé¢„å¡«å……é˜¶æ®µ (100ä¸ªtoken)
    seq_id = 0
    success, slot_map = manager.alloc(seq_id=seq_id, n_tokens=100)
    if not success:
        print("OOM: æ— æ³•åˆ†é…ç¼“å­˜")
    else:
        print(f"åˆ†é…æˆåŠŸ: {manager.stats}")

        # å­˜å‚¨KVç¼“å­˜ (æ¨¡æ‹Ÿ)
        keys = torch.randn(100, 16, 128, device=manager.device, dtype=manager.dtype)
        values = torch.randn(100, 16, 128, device=manager.device, dtype=manager.dtype)
        manager.put(seq_id=seq_id, key=keys, value=values, layer=0, slot_map=slot_map)

        # æ¨¡æ‹Ÿè§£ç é˜¶æ®µ (è¿½åŠ 10ä¸ªtoken)
        for i in range(10):
            new_slot = manager.append(seq_id=seq_id)
            if new_slot == -1:
                print("OOM: æ— æ³•è¿½åŠ token")
                break
            # å­˜å‚¨æ–°tokençš„KV (æ¨¡æ‹Ÿ)
            new_k = torch.randn(1, 16, 128, device=manager.device, dtype=manager.dtype)
            new_v = torch.randn(1, 16, 128, device=manager.device, dtype=manager.dtype)
            manager.put(seq_id=seq_id, key=new_k, value=new_v, layer=0,
                        slot_map=torch.tensor([new_slot], device=manager.device))

        print(f"è§£ç å®Œæˆ: {manager.stats}")

        # è·å–ç¼“å­˜ (ç¤ºä¾‹)
        k_cache, v_cache = manager.get(layer=0)
        print(f"ç¼“å­˜å½¢çŠ¶: K={k_cache.shape}, V={v_cache.shape}")

        # é‡Šæ”¾
        manager.free(seq_id=seq_id)
        print(f"é‡Šæ”¾å: {manager.stats}")

        # é‡ç½®
        manager.reset()
        print(f"é‡ç½®å: {manager.stats}")