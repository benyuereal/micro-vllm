# core/cache_manager.py
from typing import Dict, List, Tuple
import torch

class KVCache:
    def __init__(self):
        self.seq_kv_cache: Dict[int, Tuple] = {}  # seq_id -> past_key_values
        self.max_batch_size = 8

    def allocate(self, seq_id: int, kv: Tuple):
        self.seq_kv_cache[seq_id] = kv

    def get(self, seq_id: int):
        return self.seq_kv_cache.get(seq_id)

    def delete(self, seq_id: int):
        if seq_id in self.seq_kv_cache:
            del self.seq_kv_cache[seq_id]

    # core/cache_manager.py
    def batch_kv(self, seq_ids: List[int]) -> Tuple[torch.Tensor, ...]:
        """
        å°†å¤šä¸ªåºåˆ—çš„ past_key_values æŒ‰ layer æ‹¼æŽ¥æˆ batch
        è¾“å…¥: seq_ids = [id1, id2, ...]
        è¾“å‡º: tuple of (batch_k, batch_v) for each layer
        """
        # èŽ·å–æ¯ä¸ªåºåˆ—çš„ past_key_values
        all_seq_kv = [self.get(seq_id) for seq_id in seq_ids]  # [((k0,v0), (k1,v1), ...), ...]

        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not all_seq_kv or all_seq_kv[0] is None:
            return None

            # ðŸš¨ å…ˆæ£€æŸ¥ None å’Œç©º
        for i, seq_kv in enumerate(all_seq_kv):
            if seq_kv is None:
                print(f"[ERROR] seq {seq_ids[i]} has None past_key_values")
                raise RuntimeError(f"Sequence {seq_ids[i]} has no past_key_values")
            if len(seq_kv) == 0:
                print(f"[ERROR] seq {seq_ids[i]} has empty past_key_values")
                raise RuntimeError(f"Sequence {seq_ids[i]} has empty past_key_values")

            # ðŸš¨ å†æ£€æŸ¥å±‚æ•°ä¸€è‡´æ€§
        num_layers = len(all_seq_kv[0])
        for seq_kv in all_seq_kv:
            if len(seq_kv) != num_layers:
                raise RuntimeError(f"Inconsistent number of layers: {len(seq_kv)} vs {num_layers}")

        # âœ… çŽ°åœ¨å¯ä»¥å®‰å…¨æ‰“å°
        print(f"Batching {len(seq_ids)} sequences, num_layers={num_layers}")
        for i, seq_kv in enumerate(all_seq_kv):
            print(f"  seq {seq_ids[i]}: {len(seq_kv)} layers, first key shape: {seq_kv[0].shape}")

        for layer_idx in range(num_layers):
            keys = []
            values = []
            for seq_kv in all_seq_kv:
                k, v = seq_kv[layer_idx]  # âœ… æ­£ç¡®è§£åŒ…æ¯ä¸ªåºåˆ—çš„ç¬¬ layer_idx å±‚
                keys.append(k)
                values.append(v)
            # æ‹¼æŽ¥ batch
            batch_k = torch.cat(keys, dim=0)  # [batch_size, ...]
            batch_v = torch.cat(values, dim=0)
            batch_kv.append((batch_k, batch_v))

        return tuple(batch_kv)  # ((k0_batch, v0_batch), (k1_batch, v1_batch), ...)

    def unbatch_kv(self, seq_ids: List[int], batch_kv: Tuple) -> Dict[int, Tuple]:
        """
                å°† batch çš„ past_key_values æ‹†åˆ†å›žæ¯ä¸ªåºåˆ—
                """
        if batch_kv is None:
            print("[ERROR] batch_kv is None in unbatch_kv")
            raise RuntimeError("batch_kv is None in unbatch_kv")

        kv_dict = {}
        batch_size = len(seq_ids)

        for i, seq_id in enumerate(seq_ids):
            kv_per_seq = []
            for layer_idx, (batch_k, batch_v) in enumerate(batch_kv):
                if i >= batch_k.shape[0]:
                    raise RuntimeError(f"Batch size mismatch: {batch_k.shape[0]} < {i + 1}")
                k_i = batch_k[i:i + 1]
                v_i = batch_v[i:i + 1]
                kv_per_seq.append((k_i, v_i))
            kv_dict[seq_id] = tuple(kv_per_seq)

        return kv_dict


