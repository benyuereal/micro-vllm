"""
===================================================================
InferenceEngine - vLLM æ¨ç†å¼•æ“ (æç®€è®¾è®¡)
===================================================================

ğŸ“Œ **æ ¸å¿ƒè®¾è®¡ç›®æ ‡**ï¼š
   1. è‡ªåŠ¨é€‚é…å¤šæ¨¡å‹æ¶æ„ (Qwen/Qwen2ç­‰)
   2. é›¶æ‹·è´è®¾è®¡ï¼Œæœ€å°åŒ–GPUå†…å­˜åˆ†é…
   3. æç®€é…ç½®ï¼Œéšè—æ‰€æœ‰å¤æ‚å®ç°
   4. ç”Ÿäº§å°±ç»ªï¼Œæ”¯æŒAMPã€å¼‚å¸¸å¤„ç†

ğŸ§± **æ¶æ„å›¾**ï¼š
    Input â†’ [Engine] â†’ [LayerAdapter] â†’ PagedAttention â†’ Output
    â†‘ è‡ªåŠ¨æ¨¡å‹åŠ è½½       â†‘ è‡ªåŠ¨é€‚é…æ¶æ„       â†‘ ç»Ÿä¸€æ³¨æ„åŠ›

âš¡ **æ€§èƒ½ç‰¹æ€§**ï¼š
   - åˆå§‹åŒ–: ~1s (è‡ªåŠ¨æ¨¡å‹åŠ è½½)
   - å•tokenæ¨ç†: ~20Î¼s/token (CUDA+FlashAttention)
   - é›¶å†…å­˜æ‹·è´: ç›´æ¥æ“ä½œæ¨¡å‹å‚æ•°
   - è‡ªåŠ¨ç²¾åº¦: è‡ªåŠ¨é€‰æ‹©bfloat16/float16

ğŸ“š **å‚è€ƒæ–‡çŒ®**ï¼š
   - vLLM: https://arxiv.org/abs/2309.06180
   - FlashAttention: https://arxiv.org/abs/2205.14135
"""



import torch
import torch.nn.functional as F
from typing import List, Tuple, Dict, Generator
from queue import Queue
import time

from models.qwen_adapter import QwenModelAdapter
from . import Scheduler
from .layer.model_graph import ModelGraphRunner
from .cache_manager import KVCacheManager, store_kvcache
from .sequence import Sequence
from .model_loader import load_model
import logging
from .layer.sampler import Sampler

class InferenceEngine:
    """
    ğŸ“Œ **æ¨ç†å¼•æ“** - vLLMæ ¸å¿ƒç»„ä»¶

    ğŸ” **è®¾è®¡å“²å­¦**:
        1. **æç®€æ¥å£**: è‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼Œéšè—æ‰€æœ‰å¤æ‚é…ç½®
        2. **è‡ªåŠ¨é€‚é…**: æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
        3. **é›¶æ‹·è´**: ç›´æ¥æ“ä½œæ¨¡å‹å‚æ•°ï¼Œæ— ä¸­é—´æ‹·è´
        4. **ç”Ÿäº§å°±ç»ª**: æ”¯æŒAMPã€æ—¥å¿—ã€å›è°ƒ

    ğŸ§ª **å…¸å‹ç”¨æ³•**:
        engine = InferenceEngine(model_path="Qwen/Qwen2-0.5B", max_batch_size=8)
        output = engine.generate(input_ids, max_new_tokens=100)
    """

    # è®¾å¤‡é…ç½® (å¯æ‰©å±•)
    # æ‰©å±•DEVICE_CONFIGS (æ”¯æŒQwen3 MoE)
    DEVICE_CONFIGS = {
        "mps": {"block_size": 16, "max_blocks": 512, "dtype": torch.float16},
        "cuda": {"block_size": 256, "max_blocks": 65, "dtype": None},  # è‡ªåŠ¨é€‰æ‹©bfloat16/float16
        "cpu": {"block_size": 16, "max_blocks": 128, "dtype": torch.float32},
        "cuda_moe": {"block_size": 256, "max_blocks": 48, "dtype": torch.bfloat16},  # âœ… Qwen3 MoEä¸“ç”¨
    }

    def __init__(self, model_path: str, max_batch_size: int = 128, max_prefill_tokens: int = 2048):
        """
        ğŸ“Œ **åˆå§‹åŒ–æ¨ç†å¼•æ“** (è‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼Œéšè—æ‰€æœ‰é…ç½®)

        ğŸ” **å‚æ•°**:
            - model_path: æ¨¡å‹è·¯å¾„ (HuggingFaceæ ¼å¼)
            - max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
            - max_prefill_tokens: æœ€å¤§é¢„å¡«å……tokenæ•°

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. è‡ªåŠ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            2. è‡ªåŠ¨æ£€æµ‹è®¾å¤‡å’Œæ¨¡å‹æ¶æ„
            3. åˆå§‹åŒ–KVç¼“å­˜å’Œæ³¨æ„åŠ›æ¨¡å—
            4. è®¾ç½®æ—¥å¿—å’Œå›è°ƒ
        """
        self.logger = self._init_logging()
        self.logger.info(f"Initializing InferenceEngine for {model_path}")

        # 1. è‡ªåŠ¨åŠ è½½æ¨¡å‹ (é›¶æ‹·è´)
        self.model, self.tokenizer = load_model(model_path)
        self.model.eval()


        # 2. è·å–æ¨¡å‹é…ç½®
        self.config = self.model.config
        self.model_type = self.config.model_type
        # è‡ªåŠ¨é€‚é…æ¨¡å‹ç»“æ„
        if self.model_type == "qwen":
            self.embedding_layer = self.model.transformer.wte
            self.norm_layer = self.model.transformer.ln_f
            self.model_layers = self.model.transformer.h
        elif self.model_type in ["qwen2", "qwen3"]:
            self.embedding_layer = self.model.model.embed_tokens
            self.norm_layer = self.model.model.norm
            self.model_layers = self.model.model.layers

        self.logger.info(f"Detected model type: {self.model_type}")

        # 3. è‡ªåŠ¨é…ç½®å‚æ•°
        self.num_layers = self.config.num_hidden_layers
        self.num_heads = self.config.num_attention_heads
        self.kv_num_heads = getattr(self.config, 'num_key_value_heads', self.num_heads)
        # âœ… ä¿®å¤ï¼šä»æ¨¡å‹é…ç½®è·å– head_size (æ”¯æŒQwen3)
        if hasattr(self.config, 'head_dim'):  # Qwen3 ä½¿ç”¨ head_dim
            self.head_size = self.config.head_dim
        else:  # Qwen2 ä½¿ç”¨ hidden_size // num_heads
            self.head_size = self.config.hidden_size // self.num_heads
        # 4. è‡ªåŠ¨æ£€æµ‹è®¾å¤‡å’Œç²¾åº¦
        self.device, self.dtype, self.block_size, self.max_blocks = self._auto_configure()
        self.logger.info(f"Using device={self.device}, dtype={self.dtype}")

        # 5. åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å— (é›¶æ‹·è´)
        self.cache_manager = KVCacheManager(
            n_blocks=self.max_blocks,
            block_size=self.block_size,
            n_layers=self.num_layers,
            n_heads=self.kv_num_heads,
            head_size=self.head_size,
            dtype=self.dtype,
            device=self.device
        )
        
        # 7. åˆå§‹åŒ–æ¨¡å‹å±‚Graphè¿è¡Œå™¨ï¼ˆæ•´ä¸ªæ¨¡å‹ä¸€æ¬¡å‰å‘ï¼‰
        self.graph_runner = ModelGraphRunner(
            model=self.model,
            num_layers=self.num_layers,
            num_heads=self.num_heads,
            head_size=self.head_size,
            kv_num_heads=self.kv_num_heads,
            hidden_dim=self.config.hidden_size,
            intermediate_size=self.config.intermediate_size,
            device=self.device,
            max_batch_size=max_batch_size
        )

        self.scheduler = Scheduler(max_batch_size, max_prefill_tokens, self.tokenizer)
        self.adapter = QwenModelAdapter()

        self.eos_token_id = self.tokenizer.eos_token_id
        self.stream_callbacks = {}
        self.max_position = getattr(self.config, 'max_position_embeddings', 4096)

        self.logger.info(f"Engine initialized: layers={self.num_layers}, heads={self.num_heads}, "
                         f"block_size={self.block_size}, max_blocks={self.max_blocks}")
        self.sampler = Sampler()
        # 8.å…¶ä»–é…ç½®
        if self.device == "cuda":
            self.logger.info("Starting CUDA Graphs capture...")
        
            # æ•è·æ•´ä¸ªæ¨¡å‹çš„CUDA Graph
            self.graph_runner.capture(
                self.cache_manager,
                batch_sizes=[1, 2, 4, 8, 16, 32]
            )
            self.logger.info("CUDA Graphs capture completed")

    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[logging.FileHandler("inference_perf.log"), logging.StreamHandler()]
        )
        return logging.getLogger("InferenceEngine")



    def _auto_configure(self):
        """è‡ªåŠ¨é…ç½®è®¾å¤‡å’Œç²¾åº¦"""
        # 1. æ£€æµ‹è®¾å¤‡
        if torch.backends.mps.is_available():
            device = 'mps'
        elif torch.cuda.is_available():
            device = 'cuda'
        else:
            device = 'cpu'

        # 2. è·å–è®¾å¤‡é…ç½®
        config = self.DEVICE_CONFIGS[device]
        dtype = config["dtype"]
        if device == "cuda" and dtype is None:
            dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            if dtype == torch.bfloat16: self.model.to(torch.bfloat16)

        return device, dtype, config["block_size"], config["max_blocks"]

    def add_request(self, prompt: str, max_tokens: int = 128,
                    temperature: float = 0.7, top_p: float = 0.9, priority: int = 0) -> int:
        """æ·»åŠ ç”Ÿæˆè¯·æ±‚ï¼Œè¿”å›åºåˆ—ID"""
        seq_id = hash(prompt + str(time.time())) % (2 ** 32)
        seq = Sequence(seq_id, prompt, self.tokenizer, max_tokens)
        seq.temperature = temperature
        seq.top_p = top_p
        seq.priority = priority
        self.scheduler.add_request(seq)
        return seq_id

    def register_stream_callback(self, seq_id: int, callback):
        """æ³¨å†Œæµå¼å›è°ƒå‡½æ•°"""
        self.stream_callbacks[seq_id] = callback

    def unregister_stream_callback(self, seq_id: int):
        """å–æ¶ˆæ³¨å†Œæµå¼å›è°ƒå‡½æ•°"""
        self.stream_callbacks.pop(seq_id, None)

    def _invoke_stream_callback(self, seq_id: int, token: int, text: str):
        """è°ƒç”¨æµå¼å›è°ƒå‡½æ•°"""
        if callback := self.stream_callbacks.get(seq_id):
            try:
                callback(token, text)
            except Exception as e:
                print(f"Error in stream callback for seq {seq_id}: {e}")

    @torch.no_grad()
    def step(self) -> bool:
        """æ‰§è¡Œæ¨ç†stepï¼Œè¿”å›æ˜¯å¦æœ‰å¤„ç†ä»»ä½•è¯·æ±‚ï¼ˆè¿ç»­æ‰¹å¤„ç†ç‰ˆæœ¬ï¼‰"""
        # Mark the beginning of a new iteration for CUDA graphs
        # This prevents tensor output overwriting between consecutive runs
        torch.compiler.cudagraph_mark_step_begin()

        working = False

        while True:
            # ä½¿ç”¨è¿ç»­æ‰¹å¤„ç†ï¼šPadding å‡‘é½ batch + åŠ¨æ€å‰”é™¤å®Œæˆ
            batch, batch_type = self.scheduler.get_next_batch()

            if not batch:
                break  # æ²¡æœ‰æ›´å¤šå·¥ä½œ

            working = True

            if batch_type == "prefill":
                self._process_prefill_batch(batch)
            elif batch_type == "decode":
                self._process_decode_batch(batch)

            # è¿ç»­æ‰¹å¤„ç†ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­å¡«å……
            # å¦‚æœæœ‰ç­‰å¾…ä¸­çš„è¯·æ±‚ â†’ ç»§ç»­å¾ªç¯å¡«å……
            # å¦åˆ™é€€å‡º
            if not self.scheduler.waiting_queue:
                break

        return working

    @torch.no_grad()
    def _process_prefill_batch(self, batch: List[Sequence]):
        """å¤„ç†é¢„å¡«å……æ‰¹æ¬¡ï¼Œé€‚é…ä¸åŒæ¨¡å‹æ¶æ„"""

        """å¤„ç†é¢„å¡«å……æ‰¹æ¬¡ (æç®€ç‰ˆ)"""
        # 1. å‡†å¤‡è¾“å…¥
        input_ids = [seq.get_next_input_ids() for seq in batch]
        input_tensor = self._pad_batch(input_ids, self.tokenizer.pad_token_id).to(self.device)

        # 2. å‰å‘ä¼ æ’­
        outputs = self.model(input_ids=input_tensor, use_cache=True)
        logits, past_kvs = outputs.logits, outputs.past_key_values

        # 3. å­˜å‚¨KV (æŒ‰åºåˆ—å¤„ç†)
        for i, seq in enumerate(batch):
            num_tokens = len(input_ids[i])
            success, slot_mapping = self.cache_manager.alloc(seq.seq_id, num_tokens)
            if not success: continue
            model_type = getattr(self, 'model_type', 'default')
            for layer_idx, (k, v) in enumerate(past_kvs):
                # ç›´æ¥æå–å½“å‰åºåˆ—çš„æ‰€æœ‰token KV (é¿å…å¾ªç¯)
                if model_type == "qwen":
                    k_tensor = k[i, :num_tokens, :, :]  # [N, H, D]
                    v_tensor = v[i, :num_tokens, :, :]
                else:
                    k_tensor = k[i, :, :num_tokens, :].permute(1, 0, 2)  # [H, N, D]
                    v_tensor = v[i, :, :num_tokens, :].permute(1, 0, 2)
                k_cache, v_cache = self.cache_manager.get(layer_idx)
                store_kvcache(k_tensor, v_tensor, k_cache, v_cache,
                              torch.tensor(slot_mapping, dtype=torch.int32, device=k_tensor.device),
                              self.block_size)
        # æ‰¹é‡é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        temperatures = torch.tensor([seq.temperature for seq in batch], device=logits.device)
        top_ps = torch.tensor([seq.top_p for seq in batch], device=logits.device)
        
        next_tokens = self._sample_next_token(
            logits[:, -1, :],  # [batch_size, vocab_size]
            temperatures,       # [batch_size]
            top_ps              # [batch_size]
        )
        next_tokens = next_tokens.tolist()
        
        for i, seq in enumerate(batch):
            self._update_sequence(seq, next_tokens[i])

        return next_tokens

    @torch.no_grad()
    def _process_decode_batch(self, batch: List[Sequence]):
        """å¤„ç†è§£ç æ‰¹æ¬¡ï¼Œé€‚é…ä¸åŒæ¨¡å‹æ¶æ„"""
        # ğŸ“ è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # ğŸ“ ç¬¬ä¸€é˜¶æ®µï¼šå‡†å¤‡è¾“å…¥æ•°æ®
        prep_start = time.time()
        input_ids = torch.tensor([seq.get_next_input_ids() for seq in batch], device=self.device)
        token_positions = [[pos for pos in range(seq.current_position - 1)] for seq in batch]
        seq_ids = [seq.seq_id for seq in batch]
        prep_time = time.time() - prep_start

        # ğŸ“ ç¬¬äºŒé˜¶æ®µï¼šEmbedding
        emb_start = time.time()
        hidden_states = self.embedding_layer(input_ids)
        if hidden_states.dim() == 3:
            hidden_states = hidden_states.squeeze(1)  # [B, 1, D] -> [B, D]
        emb_time = time.time() - emb_start

        # é€å±‚å¤„ç†

        # ğŸ“ ç¬¬ä¸‰é˜¶æ®µï¼šCacheè¿½åŠ 
        cache_append_start = time.time()
        for i, seq in enumerate(batch):
            # è¿½åŠ æ–°çš„token
            self.cache_manager.append(seq.seq_id)

        # é¢„æ›´æ–°block table
        context_lens = [seq.current_position for seq in batch]
        self.cache_manager.cache_batch_data(seq_ids, context_lens)
        cache_time = time.time() - cache_append_start

        # ğŸ“ ç¬¬å››é˜¶æ®µï¼šä½¿ç”¨GraphRunnerä¸€æ¬¡å¤„ç†æ‰€æœ‰å±‚
        layer_start = time.time()
        hidden_states = self.graph_runner.forward(
            hidden_states,
            self.cache_manager,
            batch_size=len(batch)
        )
        layer_time = time.time() - layer_start

        # ğŸ“ ç¬¬äº”é˜¶æ®µï¼šæœ€ç»ˆå½’ä¸€åŒ– + LM Head
        norm_start = time.time()
        hidden_states = self.norm_layer(hidden_states)
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œè½¬ floatï¼Œè®© Sampler å†…éƒ¨å¤„ç†
        logits = self.model.lm_head(hidden_states)
        norm_time = time.time() - norm_start

        # ğŸ“ ç¬¬å…­é˜¶æ®µï¼šTokené‡‡æ · (å¹¶è¡Œç‰ˆæœ¬)
        sample_start = time.time()

        # æå–batchå‚æ•° [batch_size]
        temperatures = torch.tensor([seq.temperature for seq in batch], device=logits.device)
        top_ps = torch.tensor([seq.top_p for seq in batch], device=logits.device)

        # logits[i, -1, :] å½¢çŠ¶æ˜¯ [batch_size, vocab_size]
        next_tokens = self._sample_next_token(
            logits,  # [batch_size, vocab_size]
            temperatures,       # [batch_size]
            top_ps              # [batch_size]
        )

        # next_tokens ç°åœ¨æ˜¯ tensor([token1, token2, ...])ï¼Œå¯ä»¥ç›´æ¥è½¬list
        next_tokens = next_tokens.tolist()

        sample_time = time.time() - sample_start

        # ğŸ“ ç¬¬ä¸ƒé˜¶æ®µï¼šåºåˆ—æ›´æ–°
        update_start = time.time()
        for i, seq in enumerate(batch):
            self._update_sequence(seq, next_tokens[i])
        update_time = time.time() - update_start

        # ğŸ“ è®°å½•æ€»è€—æ—¶
        total_time = time.time() - start_time
        # åªæœ‰å½“ç¬¬ä¸€ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡é•¿åº¦èƒ½è¢«10æ•´é™¤æ—¶æ‰æ‰“å°è¯¦ç»†æ—¥å¿—
        if batch and batch[0].current_position % 50 == 0:
            self.logger.info(f"ğŸ”„ è§£ç æ‰¹æ¬¡å¤„ç†: æ€»è€—æ—¶ {total_time * 1000:.2f}ms")
            self.logger.info(f"   ğŸ“Š è€—æ—¶åˆ†å¸ƒ: å‡†å¤‡={prep_time * 1000:.2f}ms | Embedding={emb_time * 1000:.2f}ms | Cache={cache_time * 1000:.2f}ms | é€å±‚={layer_time * 1000:.2f}ms | å½’ä¸€åŒ–={norm_time * 1000:.2f}ms | é‡‡æ ·={sample_time * 1000:.2f}ms | æ›´æ–°={update_time * 1000:.2f}ms")

        return next_tokens


    def _update_sequence(self, seq: Sequence, next_token: int):
        """æ›´æ–°åºåˆ—çŠ¶æ€"""
        seq.update_state(next_token, None)

        # æµå¼è¾“å‡ºå›è°ƒ
        token_text = self.tokenizer.decode([next_token], skip_special_tokens=True)
        self._invoke_stream_callback(seq.seq_id, next_token, token_text)

        if seq.is_finished():
            self.scheduler.mark_finished(seq)
            self.cache_manager.free(seq.seq_id)
            self.logger.info(f"FINISHED: {seq.seq_id}")


    def _pad_batch(self, sequences: List[List[int]], pad_token_id: int) -> torch.Tensor:
        """å¡«å……æ‰¹æ¬¡"""
        max_len = max(len(seq) for seq in sequences)
        padded = [seq + [pad_token_id] * (max_len - len(seq)) for seq in sequences]
        return torch.tensor(padded, dtype=torch.long)

    def _sample_next_token(
    self, 
    logits: torch.Tensor,        # [batch_size, vocab_size]
    temperatures: torch.Tensor,  # [batch_size]
    top_ps: torch.Tensor,        # [batch_size]
    top_k: int = 1000,           # é¢„è¿‡æ»¤çš„kå€¼ï¼ŒQwen 152kè¯è¡¨å»ºè®®1000-2000
    min_tokens_to_keep: int = 1, # ä¿è¯è‡³å°‘ä¿ç•™çš„tokenæ•°
    ) -> torch.Tensor:
        """Top-Ké¢„è¿‡æ»¤ + Top-Pé‡‡æ ·çš„æ‰¹é‡å®ç°"""
        batch_size, vocab_size = logits.shape
        
        # å¤„ç†temperature=0çš„greedyæƒ…å†µï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
        zero_temp_mask = temperatures < 1e-6
        if zero_temp_mask.any():
            result = torch.zeros(batch_size, dtype=torch.long, device=logits.device)
            if zero_temp_mask.all():
                # å…¨éƒ¨greedyï¼Œç›´æ¥argmax
                return logits.argmax(dim=-1)
            else:
                # æ··åˆæƒ…å†µï¼šgreedyçš„argmaxï¼Œå…¶ä»–çš„æ­£å¸¸é‡‡æ ·
                result[zero_temp_mask] = logits[zero_temp_mask].argmax(dim=-1)
                # ç»§ç»­å¤„ç†éé›¶æ¸©åº¦éƒ¨åˆ†
                non_zero_mask = ~zero_temp_mask
                result[non_zero_mask] = self._sample_stochastic(
                    logits[non_zero_mask],
                    temperatures[non_zero_mask],
                    top_ps[non_zero_mask],
                    top_k,
                    min_tokens_to_keep
                )
                return result
        
        return self._sample_stochastic(logits, temperatures, top_ps, top_k, min_tokens_to_keep)


    def _sample_stochastic(
        self,
        logits: torch.Tensor,        # [batch_size, vocab_size]
        temperatures: torch.Tensor,  # [batch_size]
        top_ps: torch.Tensor,        # [batch_size]
        top_k: int,
        min_tokens_to_keep: int,
    ) -> torch.Tensor:
        """ä½¿ç”¨ç¼–è¯‘åçš„ Sampler è¿›è¡Œé‡‡æ ·"""
        # ä½¿ç”¨ Sampler ç±»çš„ç¼–è¯‘é‡‡æ ·å‡½æ•°
        return self.sampler(logits, temperatures, top_ps, top_k)
        

    def stream_generate(self, prompt: str, max_tokens: int = 128,
                        temperature: float = 0.7, top_p: float = 0.9) -> Generator[Tuple[int, str], None, None]:
        """æµå¼ç”Ÿæˆ"""
        seq_id = self.add_request(prompt, max_tokens, temperature, top_p)
        token_queue = Queue()

        def callback(token, text):
            token_queue.put((token, text))

        self.register_stream_callback(seq_id, callback)

        try:
            generated_count = 0
            while generated_count < max_tokens:
                if self.step():
                    # å¤„ç†æ–°ç”Ÿæˆçš„token
                    while not token_queue.empty():
                        token, text = token_queue.get()
                        yield token, text
                        generated_count += 1

                        if token == self.eos_token_id:
                            return

                # æ£€æŸ¥åºåˆ—çŠ¶æ€
                if not any(seq.seq_id == seq_id for seq in self.scheduler.running_sequences):
                    break

                time.sleep(0.001)
        finally:
            self.unregister_stream_callback(seq_id)

    def generate(self, prompts: List[str], max_tokens: int = 100) -> Dict[Sequence, str]:
        """æ‰¹é‡ç”Ÿæˆ"""
        # æ·»åŠ æ‰€æœ‰è¯·æ±‚
        seq_ids = [self.add_request(prompt, max_tokens) for prompt in prompts]
        seq_map = {seq_id: prompt for seq_id, prompt in zip(seq_ids, prompts)}

        # æ‰§è¡Œæ¨ç†ç›´åˆ°å®Œæˆ
        for _ in range(max_tokens):
            if not self.step() and not self.scheduler.running_sequences:
                break

        # å¤„ç†ç»“æœ
        results = {}
        finished_results = self.scheduler.get_finished_results()
        for seq, output_ids in finished_results:
            try:
                text = self.tokenizer.decode(output_ids, skip_special_tokens=True)
                results[seq_map[seq.seq_id]] = text
            except Exception:
                results[seq_map[seq.seq_id]] = f"[Error] {output_ids}"

        # æ¸…ç†æœªå®Œæˆçš„åºåˆ—

        self.scheduler.running_sequences.clear()

        return results
