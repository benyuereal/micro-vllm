"""
===================================================================
InferenceEngine - vLLM æ¨ç†å¼•æ“ (æç®€è®¾è®¡)
===================================================================

ğŸ“Œ **æ ¸å¿ƒè®¾è®¡ç›®æ ‡**ï¼š
   1. è‡ªåŠ¨é€‚é…å¤šæ¨¡å‹æ¶æ„ (Qwen/Qwen2ç­‰)
   2. é›¶æ‹·è´è®¾è®¡ï¼Œæœ€å°åŒ–GPUå†…å­˜åˆ†é…
   3. æç®€é…ç½®ï¼Œéšè—æ‰€æœ‰å¤æ‚å®ç°
   4. ç”Ÿäº§å°±ç»ªï¼Œæ”¯æŒAMPã€å¼‚å¸¸å¤„ç†

ğŸ§± **æ¶æ„å›¾**ï¼š
    Input â†’ [Engine] â†’ [LayerAdapter] â†’ PagedAttention â†’ Output
    â†‘ è‡ªåŠ¨æ¨¡å‹åŠ è½½       â†‘ è‡ªåŠ¨é€‚é…æ¶æ„       â†‘ ç»Ÿä¸€æ³¨æ„åŠ›

âš¡ **æ€§èƒ½ç‰¹æ€§**ï¼š
   - åˆå§‹åŒ–: ~1s (è‡ªåŠ¨æ¨¡å‹åŠ è½½)
   - å•tokenæ¨ç†: ~20Î¼s/token (CUDA+FlashAttention)
   - é›¶å†…å­˜æ‹·è´: ç›´æ¥æ“ä½œæ¨¡å‹å‚æ•°
   - è‡ªåŠ¨ç²¾åº¦: è‡ªåŠ¨é€‰æ‹©bfloat16/float16

ğŸ“š **å‚è€ƒæ–‡çŒ®**ï¼š
   - vLLM: https://arxiv.org/abs/2309.06180
   - FlashAttention: https://arxiv.org/abs/2205.14135
"""



import torch
import torch.nn.functional as F
from typing import List, Tuple, Dict, Generator
from queue import Queue
import time

from models.qwen_adapter import QwenModelAdapter
from . import Scheduler
from .layer.layer import ModelLayerAdapter
from .cache_manager import KVCacheManager, store_kvcache
from .sequence import Sequence
from .model_loader import load_model
import logging

class InferenceEngine:
    """
    ğŸ“Œ **æ¨ç†å¼•æ“** - vLLMæ ¸å¿ƒç»„ä»¶

    ğŸ” **è®¾è®¡å“²å­¦**:
        1. **æç®€æ¥å£**: è‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼Œéšè—æ‰€æœ‰å¤æ‚é…ç½®
        2. **è‡ªåŠ¨é€‚é…**: æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
        3. **é›¶æ‹·è´**: ç›´æ¥æ“ä½œæ¨¡å‹å‚æ•°ï¼Œæ— ä¸­é—´æ‹·è´
        4. **ç”Ÿäº§å°±ç»ª**: æ”¯æŒAMPã€æ—¥å¿—ã€å›è°ƒ

    ğŸ§ª **å…¸å‹ç”¨æ³•**:
        engine = InferenceEngine(model_path="Qwen/Qwen2-0.5B", max_batch_size=8)
        output = engine.generate(input_ids, max_new_tokens=100)
    """

    # è®¾å¤‡é…ç½® (å¯æ‰©å±•)
    # æ‰©å±•DEVICE_CONFIGS (æ”¯æŒQwen3 MoE)
    DEVICE_CONFIGS = {
        "mps": {"block_size": 16, "max_blocks": 512, "dtype": torch.float16},
        "cuda": {"block_size": 256, "max_blocks": 96, "dtype": None},  # è‡ªåŠ¨é€‰æ‹©bfloat16/float16
        "cpu": {"block_size": 16, "max_blocks": 128, "dtype": torch.float32},
        "cuda_moe": {"block_size": 256, "max_blocks": 48, "dtype": torch.bfloat16},  # âœ… Qwen3 MoEä¸“ç”¨
    }

    def __init__(self, model_path: str, max_batch_size: int = 128, max_prefill_tokens: int = 2048):
        """
        ğŸ“Œ **åˆå§‹åŒ–æ¨ç†å¼•æ“** (è‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼Œéšè—æ‰€æœ‰é…ç½®)

        ğŸ” **å‚æ•°**:
            - model_path: æ¨¡å‹è·¯å¾„ (HuggingFaceæ ¼å¼)
            - max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
            - max_prefill_tokens: æœ€å¤§é¢„å¡«å……tokenæ•°

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. è‡ªåŠ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            2. è‡ªåŠ¨æ£€æµ‹è®¾å¤‡å’Œæ¨¡å‹æ¶æ„
            3. åˆå§‹åŒ–KVç¼“å­˜å’Œæ³¨æ„åŠ›æ¨¡å—
            4. è®¾ç½®æ—¥å¿—å’Œå›è°ƒ
        """
        self.logger = self._init_logging()
        self.logger.info(f"Initializing InferenceEngine for {model_path}")

        # 1. è‡ªåŠ¨åŠ è½½æ¨¡å‹ (é›¶æ‹·è´)
        self.model, self.tokenizer = load_model(model_path)
        self.model.eval()


        # 2. è·å–æ¨¡å‹é…ç½®
        self.config = self.model.config
        self.model_type = self.config.model_type
        # è‡ªåŠ¨é€‚é…æ¨¡å‹ç»“æ„
        if self.model_type == "qwen":
            self.embedding_layer = self.model.transformer.wte
            self.norm_layer = self.model.transformer.ln_f
            self.model_layers = self.model.transformer.h
        elif self.model_type in ["qwen2", "qwen3"]:
            self.embedding_layer = self.model.model.embed_tokens
            self.norm_layer = self.model.model.norm
            self.model_layers = self.model.model.layers

        self.logger.info(f"Detected model type: {self.model_type}")

        # 3. è‡ªåŠ¨é…ç½®å‚æ•°
        self.num_layers = self.config.num_hidden_layers
        self.num_heads = self.config.num_attention_heads
        self.kv_num_heads = getattr(self.config, 'num_key_value_heads', self.num_heads)
        # âœ… ä¿®å¤ï¼šä»æ¨¡å‹é…ç½®è·å– head_size (æ”¯æŒQwen3)
        if hasattr(self.config, 'head_dim'):  # Qwen3 ä½¿ç”¨ head_dim
            self.head_size = self.config.head_dim
        else:  # Qwen2 ä½¿ç”¨ hidden_size // num_heads
            self.head_size = self.config.hidden_size // self.num_heads
        # 4. è‡ªåŠ¨æ£€æµ‹è®¾å¤‡å’Œç²¾åº¦
        self.device, self.dtype, self.block_size, self.max_blocks = self._auto_configure()
        self.logger.info(f"Using device={self.device}, dtype={self.dtype}")

        # 5. åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å— (é›¶æ‹·è´)
        self.cache_manager = KVCacheManager(
            n_blocks=self.max_blocks,
            block_size=self.block_size,
            n_layers=self.num_layers,
            n_heads=self.kv_num_heads,
            head_size=self.head_size,
            dtype=self.dtype,
            device=self.device
        )

        # 6. åˆå§‹åŒ–å±‚é€‚é…å™¨
        self.layer_adapter = ModelLayerAdapter(
            model=self.model,
            model_config=self.config,
            device=self.device,
            num_heads=self.num_heads,
            head_size=self.head_size,
            kv_num_heads=self.kv_num_heads
        )

        self.scheduler = Scheduler(max_batch_size, max_prefill_tokens)
        self.adapter = QwenModelAdapter()

        self.eos_token_id = self.tokenizer.eos_token_id
        self.stream_callbacks = {}
        self.max_position = getattr(self.config, 'max_position_embeddings', 4096)

        self.logger.info(f"Engine initialized: layers={self.num_layers}, heads={self.num_heads}, "
                         f"block_size={self.block_size}, max_blocks={self.max_blocks}")

        # 8.å…¶ä»–é…ç½®


    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[logging.FileHandler("inference_perf.log"), logging.StreamHandler()]
        )
        return logging.getLogger("InferenceEngine")



    def _auto_configure(self):
        """è‡ªåŠ¨é…ç½®è®¾å¤‡å’Œç²¾åº¦"""
        # 1. æ£€æµ‹è®¾å¤‡
        if torch.backends.mps.is_available():
            device = 'mps'
        elif torch.cuda.is_available():
            device = 'cuda'
        else:
            device = 'cpu'

        # 2. è·å–è®¾å¤‡é…ç½®
        config = self.DEVICE_CONFIGS[device]
        dtype = config["dtype"]
        if device == "cuda" and dtype is None:
            dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            if dtype == torch.bfloat16: self.model.to(torch.bfloat16)

        return device, dtype, config["block_size"], config["max_blocks"]

    def add_request(self, prompt: str, max_tokens: int = 128,
                    temperature: float = 0.7, top_p: float = 0.9, priority: int = 0) -> int:
        """æ·»åŠ ç”Ÿæˆè¯·æ±‚ï¼Œè¿”å›åºåˆ—ID"""
        seq_id = hash(prompt + str(time.time())) % (2 ** 32)
        seq = Sequence(seq_id, prompt, self.tokenizer, max_tokens)
        seq.temperature = temperature
        seq.top_p = top_p
        seq.priority = priority
        self.scheduler.add_request(seq)
        return seq_id

    def register_stream_callback(self, seq_id: int, callback):
        """æ³¨å†Œæµå¼å›è°ƒå‡½æ•°"""
        self.stream_callbacks[seq_id] = callback

    def unregister_stream_callback(self, seq_id: int):
        """å–æ¶ˆæ³¨å†Œæµå¼å›è°ƒå‡½æ•°"""
        self.stream_callbacks.pop(seq_id, None)

    def _invoke_stream_callback(self, seq_id: int, token: int, text: str):
        """è°ƒç”¨æµå¼å›è°ƒå‡½æ•°"""
        if callback := self.stream_callbacks.get(seq_id):
            try:
                callback(token, text)
            except Exception as e:
                print(f"Error in stream callback for seq {seq_id}: {e}")

    @torch.no_grad()
    def step(self) -> bool:
        """æ‰§è¡Œä¸€ä¸ªæ¨ç†stepï¼Œè¿”å›æ˜¯å¦æœ‰å¤„ç†ä»»ä½•è¯·æ±‚"""
        batch, batch_type = self.scheduler.get_next_batch()
        if not batch:
            return False

        if batch_type == "prefill":
            self._process_prefill_batch(batch)
        elif batch_type == "decode":
            self._process_decode_batch(batch)

        return True

    @torch.no_grad()
    def _process_prefill_batch(self, batch: List[Sequence]):
        """å¤„ç†é¢„å¡«å……æ‰¹æ¬¡ï¼Œé€‚é…ä¸åŒæ¨¡å‹æ¶æ„"""

        """å¤„ç†é¢„å¡«å……æ‰¹æ¬¡ (æç®€ç‰ˆ)"""
        # 1. å‡†å¤‡è¾“å…¥
        input_ids = [seq.get_next_input_ids() for seq in batch]
        input_tensor = self._pad_batch(input_ids, self.tokenizer.pad_token_id).to(self.device)

        # 2. å‰å‘ä¼ æ’­
        outputs = self.model(input_ids=input_tensor, use_cache=True)
        logits, past_kvs = outputs.logits, outputs.past_key_values

        # 3. å­˜å‚¨KV (æŒ‰åºåˆ—å¤„ç†)
        for i, seq in enumerate(batch):
            num_tokens = len(input_ids[i])
            success, slot_mapping = self.cache_manager.alloc(seq.seq_id, num_tokens)
            if not success: continue
            model_type = getattr(self, 'model_type', 'default')
            for layer_idx, (k, v) in enumerate(past_kvs):
                # ç›´æ¥æå–å½“å‰åºåˆ—çš„æ‰€æœ‰token KV (é¿å…å¾ªç¯)
                if model_type == "qwen":
                    k_tensor = k[i, :num_tokens, :, :]  # [N, H, D]
                    v_tensor = v[i, :num_tokens, :, :]
                else:
                    k_tensor = k[i, :, :num_tokens, :].permute(1, 0, 2)  # [H, N, D]
                    v_tensor = v[i, :, :num_tokens, :].permute(1, 0, 2)
                k_cache, v_cache = self.cache_manager.get(layer_idx)
                store_kvcache(k_tensor, v_tensor, k_cache, v_cache,
                              torch.tensor(slot_mapping, dtype=torch.int32, device=k_tensor.device),
                              self.block_size)
        # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        next_tokens = []
        for i, seq in enumerate(batch):
            next_token = self._sample_next_token(logits[i, -1, :], seq.temperature, seq.top_p)
            next_tokens.append(next_token)
            self._update_sequence(seq, next_token)

        return next_tokens

    @torch.no_grad()
    def _process_decode_batch(self, batch: List[Sequence]):
        """å¤„ç†è§£ç æ‰¹æ¬¡ï¼Œé€‚é…ä¸åŒæ¨¡å‹æ¶æ„"""
        # ğŸ“ è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # ğŸ“ ç¬¬ä¸€é˜¶æ®µï¼šå‡†å¤‡è¾“å…¥æ•°æ®
        prep_start = time.time()
        input_ids = torch.tensor([seq.get_next_input_ids() for seq in batch], device=self.device)
        token_positions = [[pos for pos in range(seq.current_position - 1)] for seq in batch]
        seq_ids = [seq.seq_id for seq in batch]
        prep_time = time.time() - prep_start

        # ğŸ“ ç¬¬äºŒé˜¶æ®µï¼šEmbedding
        emb_start = time.time()
        hidden_states = self.embedding_layer(input_ids)
        emb_time = time.time() - emb_start

        # é€å±‚å¤„ç†
        all_layer_kvs = []

        # ğŸ“ ç¬¬ä¸‰é˜¶æ®µï¼šCacheè¿½åŠ 
        cache_append_start = time.time()
        for i, seq in enumerate(batch):
            # è¿½åŠ æ–°çš„token
            self.cache_manager.append(seq.seq_id)

        # é¢„æ›´æ–°block table
        context_lens = [seq.current_position for seq in batch]
        self.cache_manager.cache_batch_data(seq_ids, context_lens)
        cache_time = time.time() - cache_append_start

        # ğŸ“ ç¬¬å››é˜¶æ®µï¼šé€å±‚å¤„ç†
        layer_start = time.time()

        ## è¿½åŠ æ–°çš„token
        for layer_idx, layer in enumerate(self.model_layers):

            # ä½¿ç”¨æ¨¡å‹å±‚é€‚é…å™¨å¤„ç†ä¸åŒæ¶æ„çš„å±‚
            hidden_states, layer_kv = self.layer_adapter.process_layer(
                layer, hidden_states, self.cache_manager, seq_ids,
                context_lens, token_positions, layer_idx,
                [seq.current_position - 1 for seq in batch]
            )

            all_layer_kvs.append(layer_kv)

        layer_time = time.time() - layer_start


        # ğŸ“ ç¬¬äº”é˜¶æ®µï¼šæœ€ç»ˆå½’ä¸€åŒ– + LM Head
        norm_start = time.time()
        hidden_states = self.norm_layer(hidden_states)
        logits = self.model.lm_head(hidden_states).float()
        norm_time = time.time() - norm_start

        # ğŸ“ ç¬¬å…­é˜¶æ®µï¼šTokené‡‡æ · (å¹¶è¡Œç‰ˆæœ¬)
        sample_start = time.time()

        # æå–batchå‚æ•° [batch_size]
        temperatures = torch.tensor([seq.temperature for seq in batch], device=logits.device)
        top_ps = torch.tensor([seq.top_p for seq in batch], device=logits.device)

        # logits[i, -1, :] å½¢çŠ¶æ˜¯ [batch_size, vocab_size]
        next_tokens = self._sample_next_token_batch(
            logits[:, -1, :],  # [batch_size, vocab_size]
            temperatures,       # [batch_size]
            top_ps              # [batch_size]
        )

        # next_tokens ç°åœ¨æ˜¯ tensor([token1, token2, ...])ï¼Œå¯ä»¥ç›´æ¥è½¬list
        next_tokens = next_tokens.tolist()

        sample_time = time.time() - sample_start

        # ğŸ“ ç¬¬ä¸ƒé˜¶æ®µï¼šåºåˆ—æ›´æ–°
        update_start = time.time()
        for i, seq in enumerate(batch):
            self._update_sequence(seq, next_tokens[i])
        update_time = time.time() - update_start

        # ğŸ“ è®°å½•æ€»è€—æ—¶
        total_time = time.time() - start_time
        if True:
            self.logger.info(f"ğŸ”„ è§£ç æ‰¹æ¬¡å¤„ç†: æ€»è€—æ—¶ {total_time * 1000:.2f}ms")
            self.logger.info(f"   ğŸ“Š è€—æ—¶åˆ†å¸ƒ: å‡†å¤‡={prep_time * 1000:.2f}ms | Embedding={emb_time * 1000:.2f}ms | Cache={cache_time * 1000:.2f}ms | é€å±‚={layer_time * 1000:.2f}ms | å½’ä¸€åŒ–={norm_time * 1000:.2f}ms | é‡‡æ ·={sample_time * 1000:.2f}ms | æ›´æ–°={update_time * 1000:.2f}ms")

        return next_tokens


    def _update_sequence(self, seq: Sequence, next_token: int):
        """æ›´æ–°åºåˆ—çŠ¶æ€"""
        seq.update_state(next_token, None)

        # æµå¼è¾“å‡ºå›è°ƒ
        token_text = self.tokenizer.decode([next_token], skip_special_tokens=True)
        self._invoke_stream_callback(seq.seq_id, next_token, token_text)

        if seq.is_finished():
            self.scheduler.mark_finished(seq)
            self.cache_manager.free(seq.seq_id)
            self.logger.info(f"FINISHED: {seq.seq_id}")


    def _pad_batch(self, sequences: List[List[int]], pad_token_id: int) -> torch.Tensor:
        """å¡«å……æ‰¹æ¬¡"""
        max_len = max(len(seq) for seq in sequences)
        padded = [seq + [pad_token_id] * (max_len - len(seq)) for seq in sequences]
        return torch.tensor(padded, dtype=torch.long)

    def _sample_next_token_batch(
    self, 
    logits: torch.Tensor,  # [batch_size, vocab_size]
    temperatures: torch.Tensor,  # [batch_size]
    top_ps: torch.Tensor,  # [batch_size]
    ) -> torch.Tensor:
        """å¹¶è¡Œé‡‡æ ·æ•´ä¸ªbatchçš„ä¸‹ä¸€ä¸ªtoken"""
        batch_size, vocab_size = logits.shape
        
        # åº”ç”¨æ¸©åº¦ç¼©æ”¾ [batch_size, vocab_size]
        # æ³¨æ„ï¼štemperatures éœ€è¦ unsqueeze ä»¥ä¾¿å¹¿æ’­
        logits = logits / temperatures.unsqueeze(-1)
        
        # ===== Top-p è¿‡æ»¤ (batchç‰ˆæœ¬) =====
        # æ’åº [batch_size, vocab_size]
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
        
        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡ [batch_size, vocab_size]
        sorted_probs = F.softmax(sorted_logits, dim=-1)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # åˆ›å»ºmaskï¼šå“ªäº›ä½ç½®éœ€è¦ç§»é™¤ [batch_size, vocab_size]
        sorted_indices_to_remove = cumulative_probs > top_ps.unsqueeze(-1)
        
        # ä¿æŒç¬¬ä¸€ä¸ªtokenï¼ˆæœ€é«˜æ¦‚ç‡çš„ï¼‰
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = False
        
        # å°†maskæ˜ å°„å›åŸå§‹ç´¢å¼•é¡ºåº
        # ä½¿ç”¨ scatter æ¥ "è¿˜åŸ" æ’åºåçš„maskåˆ°åŸå§‹ä½ç½®
        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool)
        indices_to_remove.scatter_(
            dim=-1, 
            index=sorted_indices, 
            src=sorted_indices_to_remove
        )
        
        # åº”ç”¨mask
        logits = logits.masked_fill(indices_to_remove, float('-inf'))
        
        # é‡æ–°è®¡ç®—æ¦‚ç‡å¹¶é‡‡æ · [batch_size]
        probs = F.softmax(logits, dim=-1)
        return torch.multinomial(probs, num_samples=1).squeeze(-1)  # [batch_size]

    def stream_generate(self, prompt: str, max_tokens: int = 128,
                        temperature: float = 0.7, top_p: float = 0.9) -> Generator[Tuple[int, str], None, None]:
        """æµå¼ç”Ÿæˆ"""
        seq_id = self.add_request(prompt, max_tokens, temperature, top_p)
        token_queue = Queue()

        def callback(token, text):
            token_queue.put((token, text))

        self.register_stream_callback(seq_id, callback)

        try:
            generated_count = 0
            while generated_count < max_tokens:
                if self.step():
                    # å¤„ç†æ–°ç”Ÿæˆçš„token
                    while not token_queue.empty():
                        token, text = token_queue.get()
                        yield token, text
                        generated_count += 1

                        if token == self.eos_token_id:
                            return

                # æ£€æŸ¥åºåˆ—çŠ¶æ€
                if not any(seq.seq_id == seq_id for seq in self.scheduler.running_sequences):
                    break

                time.sleep(0.001)
        finally:
            self.unregister_stream_callback(seq_id)

    def generate(self, prompts: List[str], max_tokens: int = 100) -> Dict[Sequence, str]:
        """æ‰¹é‡ç”Ÿæˆ"""
        # æ·»åŠ æ‰€æœ‰è¯·æ±‚
        seq_ids = [self.add_request(prompt, max_tokens) for prompt in prompts]
        seq_map = {seq_id: prompt for seq_id, prompt in zip(seq_ids, prompts)}

        # æ‰§è¡Œæ¨ç†ç›´åˆ°å®Œæˆ
        for _ in range(max_tokens):
            if not self.step() and not self.scheduler.running_sequences:
                break

        # å¤„ç†ç»“æœ
        results = {}
        finished_results = self.scheduler.get_finished_results()
        for seq, output_ids in finished_results:
            try:
                text = self.tokenizer.decode(output_ids, skip_special_tokens=True)
                results[seq_map[seq.seq_id]] = text
            except Exception:
                results[seq_map[seq.seq_id]] = f"[Error] {output_ids}"

        # æ¸…ç†æœªå®Œæˆçš„åºåˆ—

        self.scheduler.running_sequences.clear()

        return results
