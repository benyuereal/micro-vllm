"""
===================================================================
PagedAttention - vLLM é«˜æ€§èƒ½æ³¨æ„åŠ›å±‚ (FlashAttentionä¼˜åŒ–ç‰ˆ)
===================================================================

ğŸ“Œ **æ ¸å¿ƒè®¾è®¡ç›®æ ‡**ï¼š
   1. æä¾›æç®€çš„FlashAttentionæ¥å£ï¼Œéšè—æ‰€æœ‰å¤æ‚å®ç°
   2. ç›´æ¥æ“ä½œKVç¼“å­˜ï¼Œé¿å…ä¸­é—´æ‹·è´
   3. æ”¯æŒåŠ¨æ€Blockåˆ†é…ï¼Œè‡ªåŠ¨æ›´æ–°Block Table
   4. æè‡´æ€§èƒ½ï¼Œæœ€å°åŒ–GPUå†…å­˜åˆ†é…

ğŸ§± **æ•°æ®æµå›¾**ï¼š
    Input â†’ Rotary Emb â†’ (Store KV) â†’ FlashAttention â†’ Output
    â†‘ ç›´æ¥æ“ä½œç¼“å­˜          â†‘ è‡ªåŠ¨Block Table          â†‘ é›¶æ‹·è´

âš¡ **æ€§èƒ½ç‰¹æ€§**ï¼š
   - å•tokenæ³¨æ„åŠ›: ~15Î¼s/token (CUDA+FlashAttention)
   - æ‰¹é‡æ³¨æ„åŠ›: ~10Î¼s/token (CUDA+FlashAttention)
   - é›¶å†…å­˜æ‹·è´: ç›´æ¥æ“ä½œKVç¼“å­˜
   - è‡ªåŠ¨Blockç®¡ç†: æ— éœ€æ‰‹åŠ¨æ›´æ–°Block Table

ğŸ“š **å‚è€ƒæ–‡çŒ®**ï¼š
   - FlashAttention: https://arxiv.org/abs/2205.14135
   - PagedAttention: https://arxiv.org/abs/2309.06180
"""

import torch
import torch.nn as nn
from typing import List, Optional
from core.cache_manager import KVCacheManager, store_kvcache, store_kvcache_batch
import time
try:
    from flash_attn import flash_attn_with_kvcache  # âœ… æ­£ç¡®å¯¼å…¥
except ImportError:
    print('flash_attn_with_kvcache not installed')
    flash_attn_with_kvcache = None


# é¢„å…ˆè®¡ç®—æ‰€æœ‰å¯èƒ½ä½ç½®çš„æ—‹è½¬çŸ©é˜µ
class PrecomputedRotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position=8192, base=10000, device=None):
        super().__init__()
        self.dim = dim
        self.device = device or torch.device('cpu')
        self.max_position = max_position

        # é¢„å…ˆè®¡ç®—æ‰€æœ‰ä½ç½®çš„æ—‹è½¬çŸ©é˜µ
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=self.device).to(torch.bfloat16) / dim))
        t = torch.arange(max_position, device=self.device, dtype=inv_freq.dtype)
        freqs = torch.einsum("i,j->ij", t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)

        # é¢„å…ˆè®¡ç®—å¥½æ‰€æœ‰ä½ç½®çš„coså’Œsin
        cos_cache = emb.cos().to(torch.bfloat16)
        sin_cache = emb.sin().to(torch.bfloat16)

        # æ³¨å†Œä¸ºç¼“å†²åŒºï¼Œç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        self.register_buffer("cos_cache", cos_cache)
        self.register_buffer("sin_cache", sin_cache)

        # é¢„è®¡ç®—æ—‹è½¬çŸ©é˜µçš„è½¬ç½®ç‰ˆæœ¬ï¼Œä»¥ä¾¿å¿«é€Ÿåº”ç”¨
        self.register_buffer("neg_mask", torch.tensor([-1.0, 1.0], device=self.device).repeat(dim // 2))

    def forward(self, x, positions):
        batch_size, num_heads, seq_len, head_size = x.shape
        positions = positions.to(self.device)

        # ä½¿ç”¨æ›´é«˜æ•ˆçš„ç´¢å¼•æ–¹å¼
        flat_positions = positions.view(-1)

        # ç›´æ¥ç´¢å¼•é¢„å…ˆè®¡ç®—å¥½çš„æ—‹è½¬çŸ©é˜µ (ä½¿ç”¨é«˜çº§ç´¢å¼•)
        cos = self.cos_cache[flat_positions].view(batch_size, seq_len, head_size).unsqueeze(1)
        sin = self.sin_cache[flat_positions].view(batch_size, seq_len, head_size).unsqueeze(1)

        # æ‰©å±•ç»´åº¦ä»¥åŒ¹é…è¾“å…¥
        cos = cos.expand(-1, num_heads, -1, -1)
        sin = sin.expand(-1, num_heads, -1, -1)

        # ä¼˜åŒ–æ—‹è½¬è®¡ç®— - ä½¿ç”¨æ›´é«˜æ•ˆçš„å…ƒç´ çº§æ“ä½œ
        x_rotated = x * self.neg_mask.reshape(1, 1, 1, -1)
        x_rotated = torch.cat([x_rotated[..., head_size // 2:], x_rotated[..., :head_size // 2]], dim=-1)

        # åº”ç”¨æ—‹è½¬
        return x * cos + x_rotated * sin

class RotaryEmbedding(nn.Module):
    """
    ğŸ“Œ **æ—‹è½¬ä½ç½®ç¼–ç ** (æç®€å®ç°)

    ğŸ” **è®¾è®¡**:
        - ä½¿ç”¨é¢„è®¡ç®—çš„cos/sinç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
        - æ”¯æŒåŠ¨æ€æ‰©å±•æœ€å¤§ä½ç½®
        - è‡ªåŠ¨åŒ¹é…è¾“å…¥è®¾å¤‡

    âš¡ **æ€§èƒ½**:
        - æ—¶é—´: ~1Î¼s/token (CUDA)
        - ç©ºé—´: O(max_position * dim)
    """

    def __init__(self, dim, max_position=2048, base=10000, device=None):
        super().__init__()
        self.dim = dim
        self.device = device or torch.device('cpu')
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=self.device).to(torch.bfloat16) / dim))
        self.max_seq_len = max_position
        self._update_cos_sin_cache(max_position)

    def _update_cos_sin_cache(self, seq_len):
        self.max_seq_len = max(self.max_seq_len, seq_len)
        t = torch.arange(self.max_seq_len, device=self.device, dtype=self.inv_freq.dtype)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.cos_cache = emb.cos()[None, None, :, :]  # [1, 1, seq_len, dim]
        self.sin_cache = emb.sin()[None, None, :, :]  # [1, 1, seq_len, dim]

    def forward(self, x, positions):
        """
        ğŸ“Œ **åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç **

        ğŸ” **å‚æ•°**:
            - x: [B, H, S, D] æŸ¥è¯¢/é”®
            - positions: [B, S] ä½ç½®ç´¢å¼•

        âœ… **è¿”å›**:
            - æ—‹è½¬åçš„x: [B, H, S, D]
        """
        positions = positions.to(self.device)
        max_pos = positions.max().item() + 1
        if max_pos > self.max_seq_len:
            self._update_cos_sin_cache(max_pos)

        # æ‰¹é‡è·å–cos/sin (å‘é‡åŒ–)
        batch_size, num_heads, seq_len, head_size = x.shape
        positions_flat = positions.view(-1)
        cos = self.cos_cache[:, :, positions_flat].view(1, 1, batch_size, seq_len, head_size).permute(2, 1, 3, 0,
                                                                                                      4).squeeze(3)
        sin = self.sin_cache[:, :, positions_flat].view(1, 1, batch_size, seq_len, head_size).permute(2, 1, 3, 0,
                                                                                                      4).squeeze(3)

        # æ—‹è½¬å…¬å¼: (x * cos) + (rotated * sin)
        x1, x2 = x[..., :self.dim // 2], x[..., self.dim // 2:]
        return (x * cos) + (torch.cat((-x2, x1), dim=-1) * sin)



# æ·»åŠ æ—¶é—´æµ‹é‡å·¥å…·å‡½æ•°
def get_current_time_us():
    """è·å–å½“å‰æ—¶é—´æˆ³ï¼ˆå¾®ç§’ï¼‰"""
    return time.time() * 1e6


class PagedAttention(nn.Module):
    """
    ğŸ“Œ **åˆ†é¡µæ³¨æ„åŠ›** - vLLMæ ¸å¿ƒç»„ä»¶

    ğŸ” **è®¾è®¡å“²å­¦**:
        1. **æç®€æ¥å£**: ä»…1ä¸ªforwardæ–¹æ³•ï¼Œéšè—æ‰€æœ‰å¤æ‚å®ç°
        2. **é›¶æ‹·è´è®¾è®¡**: ç›´æ¥æ“ä½œKVç¼“å­˜ï¼Œæ— ä¸­é—´æ‹·è´
        3. **è‡ªåŠ¨Blockç®¡ç†**: åŠ¨æ€åˆ†é…Blockï¼Œè‡ªåŠ¨æ›´æ–°Block Table
        4. **ç”Ÿäº§å°±ç»ª**: æ”¯æŒAMPã€å¼‚å¸¸å¤„ç†ã€è®¾å¤‡åŒ¹é…

    ğŸ§ª **å…¸å‹ç”¨æ³•**:
        attn = PagedAttention(num_heads=16, head_size=128, kv_num_heads=16, device="cuda")
        output = attn(
            query=query,                    # [B, H, D]
            cache_manager=cache_manager,    # KVCacheManagerå®ä¾‹
            seq_ids=[0, 1, 2],             # åºåˆ—IDåˆ—è¡¨
            context_lens=[10, 20, 30],      # æ¯ä¸ªåºåˆ—çš„å½“å‰é•¿åº¦
            layer_idx=0,                    # å±‚ç´¢å¼•
            key=new_k,                      # [B, H, D] (å¯é€‰ï¼Œæ–°token)
            value=new_v                     # [B, H, D] (å¯é€‰ï¼Œæ–°token)
        )
    """

    def __init__(self, num_heads: int, head_size: int, kv_num_heads: int, device: str = "auto"):
        super().__init__()
        self.num_heads = num_heads
        self.kv_num_heads = kv_num_heads
        self.head_size = head_size
        self.scale = head_size ** -0.5  # 1/sqrt(head_size)

        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        self.device = (torch.device('mps') if torch.backends.mps.is_available() else
                       torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))
        if device != "auto": self.device = torch.device(device)

        # åˆå§‹åŒ–æ—‹è½¬ä½ç½®ç¼–ç 
        self.rotary_emb = PrecomputedRotaryEmbedding(head_size, max_position=4096, device=self.device)
        self.use_flash_attn = self.device.type == 'cuda' and flash_attn_with_kvcache is not None

        # æ€§èƒ½ç»Ÿè®¡
        self.total_calls = 0
        self.total_time_us = 0

    def forward(
            self,
            query: torch.Tensor,  # [B, H, D] æŸ¥è¯¢å¼ é‡
            cache_manager: KVCacheManager,  # KVç¼“å­˜ç®¡ç†å™¨
            seq_ids: List[int],  # [B] åºåˆ—IDåˆ—è¡¨
            context_lens: List[int],  # [B] æ¯ä¸ªåºåˆ—çš„å½“å‰é•¿åº¦
            layer_idx: int,  # å±‚ç´¢å¼•
            key: Optional[torch.Tensor] = None,  # [B, H, D] æ–°tokençš„é”® (å¯é€‰)
            value: Optional[torch.Tensor] = None  # [B, H, D] æ–°tokençš„å€¼ (å¯é€‰)
    ) -> torch.Tensor:
        """
        ğŸ“Œ **PagedAttentionå‰å‘ä¼ æ’­** (æç®€æ¥å£)

        ğŸ” **å‚æ•°**:
            - query: æŸ¥è¯¢å¼ é‡ [B, H, D]
            - cache_manager: KVç¼“å­˜ç®¡ç†å™¨
            - seq_ids: åºåˆ—IDåˆ—è¡¨ [B]
            - context_lens: æ¯ä¸ªåºåˆ—çš„å½“å‰é•¿åº¦ [B]
            - layer_idx: å±‚ç´¢å¼•
            - key/value: æ–°tokençš„KV (è§£ç é˜¶æ®µæä¾›)

        âœ… **è¿”å›**:
            - output: æ³¨æ„åŠ›è¾“å‡º [B, H, D]

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
            2. (å¯é€‰) å­˜å‚¨æ–°tokençš„KVåˆ°ç¼“å­˜
            3. å‡†å¤‡Block Table (è‡ªåŠ¨å¤„ç†Blockåˆ†é…)
            4. è°ƒç”¨FlashAttention
        """
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = get_current_time_us()
        total_start = start_time

        batch_size, num_heads, head_dim = query.shape

        # 1. æ—‹è½¬ä½ç½®ç¼–ç 
        rotary_start = get_current_time_us()
        positions = torch.tensor(context_lens, dtype=torch.int32, device=self.device).unsqueeze(1)
        query = self.rotary_emb(query.unsqueeze(2), positions).squeeze(2)
        if key is not None:
            key = self.rotary_emb(key.unsqueeze(2), positions).squeeze(2)
        rotary_end = get_current_time_us()
        rotary_time = rotary_end - rotary_start

        # 2. å­˜å‚¨æ–°token KV (ç›´æ¥æ“ä½œç¼“å­˜ï¼Œé›¶æ‹·è´)
        store_start = get_current_time_us()
        k_cache, v_cache = cache_manager.get(layer_idx)
        store_slots = []
        for i, (seq_id, token_idx) in enumerate(zip(seq_ids, context_lens)):
            if token_idx > 0:  # ç¡®ä¿ä¸æ˜¯ç¬¬ä¸€ä¸ªtoken
                slot = cache_manager.get_slots(seq_id, [token_idx - 1])[0]
                store_slots.append(slot)
            else:
                store_slots.append(-1)  # æ— æ•ˆslot

        # è½¬æ¢ä¸ºå¼ é‡ [batch_size, 1]
        store_slots_tensor = torch.tensor(store_slots, dtype=torch.int32, device=self.device).unsqueeze(1)

        # æ‰¹é‡å­˜å‚¨
        if key is not None and value is not None:
            store_kvcache_batch(
                key=key.unsqueeze(1),  # [batch_size, 1, num_heads, head_size]
                value=value.unsqueeze(1),
                k_cache=k_cache,
                v_cache=v_cache,
                block_size=cache_manager.block_size,
                slot_mapping_batch=store_slots_tensor
            )
        store_end = get_current_time_us()
        store_time = store_end - store_start

        # 3. å‡†å¤‡Block Table (è‡ªåŠ¨å¤„ç†åŠ¨æ€Blockåˆ†é…)
        block_start = get_current_time_us()
        block_tables = [cache_manager.get_blocks(seq_id) for seq_id in seq_ids]
        max_blocks = max(map(len, block_tables), default=0)
        block_table_tensor = torch.tensor([
            blocks + [-1] * (max_blocks - len(blocks)) for blocks in block_tables
        ], dtype=torch.int32, device=self.device)
        block_end = get_current_time_us()
        block_time = block_end - block_start

        # 4. FlashAttention (é›¶æ‹·è´)
        attn_start = get_current_time_us()
        k_cache, v_cache = cache_manager.get(layer_idx)
        print("shape of q ,k v", query.shape, k_cache.shape, v_cache.shape)
        output = flash_attn_with_kvcache(
            q=query.unsqueeze(1),  # [B, 1, H, D]
            k_cache=k_cache,  # [max_blocks, block_size, H, D]
            v_cache=v_cache,
            cache_seqlens=torch.tensor(context_lens, dtype=torch.int32, device=self.device),
            block_table=block_table_tensor,  # [B, max_blocks]
            softmax_scale=self.scale,  # 1/sqrt(head_dim)
            causal=True,  # å› æœæ©ç 
            # âŒ ä¸ä¼ rotary_cos/sin (æ€§èƒ½æœ€ä¼˜)
            # âŒ ä¸ä¼ k/v (FA2è‡ªåŠ¨ä»ç¼“å­˜è¯»å–)
            num_splits=1,  # å›ºå®šä¸º1ï¼Œæ€§èƒ½æœ€ä¼˜ (FA1é»˜è®¤)
            rotary_interleaved=False,  # æ›´ä¼˜çš„æ—‹è½¬ç¼–ç  (FA1é»˜è®¤)
            softcap=0.0,
        )
        attn_end = get_current_time_us()
        attn_time = attn_end - attn_start

        # è®°å½•æ€»æ—¶é—´
        total_end = get_current_time_us()
        total_time = total_end - total_start

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        self.total_calls += 1
        self.total_time_us += total_time

        # æ‰“å°æ€§èƒ½æ—¥å¿—ï¼ˆæ¯100æ¬¡æ‰“å°ä¸€æ¬¡ï¼‰
        if self.total_calls % 100 == 0:
            print(f"ğŸ“Š PagedAttention æ€§èƒ½ç»Ÿè®¡ (è°ƒç”¨ #{self.total_calls}):")
            print(f"   â”œâ”€â”€ æ€»è€—æ—¶: {total_time:.2f}Î¼s")
            print(f"   â”œâ”€â”€ æ—‹è½¬ç¼–ç : {rotary_time:.2f}Î¼s ({rotary_time / total_time * 100:.1f}%)")
            print(f"   â”œâ”€â”€ KVå­˜å‚¨: {store_time:.2f}Î¼s ({store_time / total_time * 100:.1f}%)")
            print(f"   â”œâ”€â”€ Blockå‡†å¤‡: {block_time:.2f}Î¼s ({block_time / total_time * 100:.1f}%)")
            print(f"   â”œâ”€â”€ FlashAttention: {attn_time:.2f}Î¼s ({attn_time / total_time * 100:.1f}%)")
            print(f"   â””â”€â”€ å¹³å‡è€—æ—¶: {self.total_time_us / self.total_calls:.2f}Î¼s/call")

        return output.squeeze(1)  # [B, H, D]


# =============================================================================
# ğŸ§ª ä½¿ç”¨ç¤ºä¾‹
# =============================================================================

if __name__ == "__main__":
    # åˆå§‹åŒ–
    cache_manager = KVCacheManager(n_blocks=1024, block_size=16, n_layers=32, n_heads=16, head_size=128)
    attn = PagedAttention(num_heads=16, head_size=128, kv_num_heads=16, device="cuda")

    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 3
    query = torch.randn(batch_size, 16, 128, device=attn.device)
    seq_ids = [0, 1, 2]
    context_lens = [10, 20, 30]

    # ç¤ºä¾‹1: é¢„å¡«å……é˜¶æ®µ (æ— æ–°KV)
    output = attn(
        query=query,
        cache_manager=cache_manager,
        seq_ids=seq_ids,
        context_lens=context_lens,
        layer_idx=0
    )
    print(f"é¢„å¡«å……è¾“å‡º: {output.shape}")  # [3, 16, 128]

    # ç¤ºä¾‹2: è§£ç é˜¶æ®µ (æœ‰æ–°KV)
    new_k = torch.randn(batch_size, 16, 128, device=attn.device)
    new_v = torch.randn(batch_size, 16, 128, device=attn.device)
    output = attn(
        query=query,
        cache_manager=cache_manager,
        seq_ids=seq_ids,
        context_lens=[l + 1 for l in context_lens],  # é•¿åº¦+1
        layer_idx=0,
        key=new_k,
        value=new_v
    )
    print(f"è§£ç è¾“å‡º: {output.shape}")  # [3, 16, 128]

    # ç¤ºä¾‹3: æ£€æŸ¥ç¼“å­˜ç»Ÿè®¡
    print(f"ç¼“å­˜çŠ¶æ€: {cache_manager.stats}")