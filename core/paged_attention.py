import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, Dict, Optional
import numpy as np


class PagedAttention(nn.Module):
    def __init__(self, num_heads: int, head_size: int, kv_head_size:int, device: str = "auto"):
        super().__init__()
        self.num_heads = num_heads
        self.head_size = head_size
        self.kv_head_size = kv_head_size
        self.scale = 1.0 / (head_size ** 0.5)

        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if device == "auto":
            self.device = 'mps' if torch.backends.mps.is_available() else 'cuda'
        else:
            self.device = device

        # é’ˆå¯¹CUDAä½¿ç”¨ä¼˜åŒ–å†…æ ¸
        self.use_cuda_kernel = (self.device == 'cuda') and torch.cuda.is_available()
        if self.use_cuda_kernel:
            try:
                from vllm import paged_attention
                self.cuda_paged_attention = paged_attention
            except ImportError:
                print("vllm not available, using fallback implementation")
                self.use_cuda_kernel = False

    def forward(
            self,
            query: torch.Tensor,  # [batch, num_heads, head_size]
            cache_manager: 'KVCache',
            seq_ids: List[int],
            context_lens: List[int],
            token_positions: List[List[int]]
    ) -> torch.Tensor:
        batch_size = query.size(0)
        output = torch.zeros_like(query)

        if self.use_cuda_kernel:
            return self._cuda_forward(query, cache_manager, seq_ids, context_lens, token_positions)
        else:
            return self._mps_forward(query, cache_manager, seq_ids, context_lens, token_positions)

    def _cuda_forward(
            self,
            query: torch.Tensor,
            cache_manager: 'KVCache',
            seq_ids: List[int],
            context_lens: List[int],
            token_positions: List[List[int]]
    ) -> torch.Tensor:
        """CUDAä¼˜åŒ–å®ç°"""
        # å®ç°vLLMçš„å—è¡¨æ ¼å¼
        max_blocks_per_seq = 0
        block_tables = []

        for seq_id in seq_ids:
            tokens = cache_manager.get_sequence_tokens(seq_id)
            block_table = []

            for token_id, position in tokens:
                allocation = cache_manager.memory_pool.find_token(token_id, position)
                if allocation:
                    block_id, slot_id = allocation
                    block_table.append(block_id * cache_manager.memory_pool.block_size + slot_id)

            block_tables.append(block_table)
            max_blocks_per_seq = max(max_blocks_per_seq, len(block_table))

        # åˆ›å»ºå—è¡¨å¼ é‡
        block_tables_tensor = torch.full(
            (len(seq_ids), max_blocks_per_seq),
            -1,
            dtype=torch.int, device=self.device
        )

        for i, table in enumerate(block_tables):
            if table:
                block_tables_tensor[i, :len(table)] = torch.tensor(table, device=self.device)

        # è·å–æ‰€æœ‰KVå—
        all_k = []
        all_v = []
        blocks = cache_manager.memory_pool.get_all_blocks()
        for block in blocks:
            all_k.append(block.key_cache)
            all_v.append(block.value_cache)

        # æ‹¼æ¥æ‰€æœ‰å— (vLLMè¦æ±‚è¿ç»­å†…å­˜)
        all_k = torch.cat(all_k, dim=2)  # [layers, heads, total_slots, head_size]
        all_v = torch.cat(all_v, dim=2)

        # è°ƒç”¨vLLMå†…æ ¸
        return self.cuda_paged_attention(
            query,
            all_k, all_v,
            block_tables_tensor,
            torch.tensor(context_lens, device=self.device),
            self.scale
        )

    def _mps_forward(
            self,
            query: torch.Tensor,  # [2, 14, 64] â† Q å¤´
            cache_manager: 'KVCache',
            seq_ids: List[int],
            context_lens: List[int],
            token_positions: List[List[int]]
    ) -> torch.Tensor:
        batch_size = query.size(0)
        output = torch.zeros_like(query)  # [2, 14, 64]

        for i in range(batch_size):
            seq_id = seq_ids[i]
            tokens = cache_manager.get_sequence_tokens(seq_id)
            seq_len = context_lens[i]

            # æ”¶é›† KVï¼ˆæ³¨æ„ï¼šæ¯ä¸ª token çš„ k/v æ˜¯ [layers, kv_heads, head_size]ï¼‰
            keys = []
            values = []
            for token_id, position in tokens[:seq_len]:
                k, v = cache_manager.get_token_kv(token_id, position)
                keys.append(k)  # [layers, kv_heads, head_size]
                values.append(v)

            # æ‹¼æ¥ï¼šK = [layers, kv_heads, seq_len, head_size]
            K = torch.stack(keys, dim=2)  # [24, 2, 5, 64]
            V = torch.stack(values, dim=2)

            # å½“å‰ queryï¼š[1, 14, 64]
            q = query[i].unsqueeze(0)  # [1, 14, 64]

            # ğŸ”¥ ä¿®å¤1: æ¯å±‚è¾“å‡ºæ˜¯ç‹¬ç«‹çš„ï¼Œä¸è¦ç´¯åŠ ï¼
            layer_output = None
            for layer_idx in range(K.size(0)):
                layer_k = K[layer_idx]  # [2, 5, 64]
                layer_v = V[layer_idx]

                # GQA ä¿®å¤ï¼šå°† K/V å¤´ä» 2 æ‰©å±•åˆ° 14
                repeat_times = self.num_heads // layer_k.size(0)
                layer_k = layer_k.repeat_interleave(repeat_times, dim=0)  # [14, 5, 64]
                layer_v = layer_v.repeat_interleave(repeat_times, dim=0)

                # æ³¨æ„åŠ›è®¡ç®—
                scores = torch.einsum("hd,hsd->hs", q[0], layer_k) * self.scale  # [14, 5]
                attn = F.softmax(scores, dim=-1)
                layer_output = torch.einsum("hs,hsd->hd", attn, layer_v)  # [14, 64]

                # ğŸ”¥ ä¿®å¤2: q = layer_output ä»…ç”¨äºæœ¬å±‚å¾ªç¯ï¼ˆæ¨¡æ‹Ÿä¸‹ä¸€å±‚ï¼‰
                # ä½†æ³¨æ„ï¼šä¸‹ä¸€å±‚ q åº”è¯¥ç”± o_proj åé‡æ–°æŠ•å½±ï¼Œè¿™é‡Œç®€åŒ–
                q = layer_output.unsqueeze(0)  # [1,14,64]

            # ğŸ”¥ ä¿®å¤3: åªå–æœ€åä¸€å±‚çš„è¾“å‡ºï¼ˆæˆ–æ‰€æœ‰å±‚å¹³å‡ï¼Œä½†ä¸è¦ç´¯åŠ ï¼ï¼‰
            output[i] = layer_output  # â† èµ‹å€¼ï¼Œä¸æ˜¯ +=ï¼

        return output  # [2, 14, 64]



