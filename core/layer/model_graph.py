"""
===================================================================
ModelGraphRunner - æ•´ä¸ªæ¨¡å‹å±‚çš„CUDA Graphå°è£… (Fixed KV Cache Shape)
===================================================================
"""

import logging
import torch
import torch.nn.functional as F
from typing import Dict, List
from core.paged_attention import PagedAttention
from kernel.swiglu import swiglu_fused as swiglu

try:
    from flash_attn import flash_attn_with_kvcache
except ImportError:
    flash_attn_with_kvcache = None

logger = logging.getLogger(__name__)


class ModelGraphRunner:
    """
    ğŸ“Œ ä¿®å¤ KV Cache å½¢çŠ¶åŒ¹é…é—®é¢˜
    """
    
    def __init__(self, model, num_layers: int, num_heads: int, head_size: int,
                 kv_num_heads: int, hidden_dim: int, intermediate_size: int,
                 device: str, max_batch_size: int = 16, dtype: torch.dtype = torch.bfloat16):
        self.model = model
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.head_size = head_size
        self.kv_num_heads = kv_num_heads
        self.hidden_dim = hidden_dim
        self.intermediate_size = intermediate_size
        self.device = device
        self.max_batch_size = max_batch_size
        self.dtype = dtype
        
        # åˆå§‹åŒ– PagedAttention
        self.attention = PagedAttention(
            num_heads=num_heads,
            head_size=head_size,
            kv_num_heads=kv_num_heads,
            device=device,
            max_batch_size=max_batch_size
        )
        
        # åˆå§‹åŒ–ç¼“å†²åŒº
        self._allocate_buffers()
        
        # é¢„ç¼“å­˜æƒé‡
        self.prepare()
        
        # Graphå­˜å‚¨
        self._graphs: Dict[int, torch.cuda.CUDAGraph] = {}
        self._ready = False
    
    def prepare(self):
        """é¢„ç¼“å­˜è½¬ç½®æƒé‡"""
        for idx, block in enumerate(self.model.transformer.h):
            mlp = block.mlp
            
            # Gate + Up
            gate_up = torch.cat([mlp.w1.weight, mlp.w2.weight], dim=0)
            mlp._gu = gate_up.t().contiguous()
            
            # Down
            mlp._d = mlp.c_proj.weight.t().contiguous()
            
            # Attention output
            block.attn._o = block.attn.c_proj.weight.t().contiguous()
            
            # QKV
            block.attn._qkv_w = block.attn.c_attn.weight.t().contiguous()
            block.attn._qkv_b = block.attn.c_attn.bias
        
        logger.info("âœ… ModelGraphRunner æƒé‡é¢„ç¼“å­˜å®Œæˆ")
    
    def _allocate_buffers(self):
        """åˆå§‹åŒ–é™æ€ç¼“å†²åŒº"""
        max_b = self.max_batch_size
        self._hidden = torch.empty((max_b, self.hidden_dim), dtype=self.dtype, device=self.device)
        self._output = torch.empty_like(self._hidden)
    
    def _forward_pass(self, h, batch_size: int, cache_manager, use_graph_cache: bool = True):
        """
        ğŸ”§ æ ¸å¿ƒé€»è¾‘ï¼šå•æ¬¡å‰å‘ä¼ æ’­é€šè¿‡æ‰€æœ‰å±‚
        """
        for layer_idx in range(self.num_layers):
            block = self.model.transformer.h[layer_idx]
            
            # æå–æƒé‡
            w_qkv = block.attn._qkv_w
            b_qkv = block.attn._qkv_b
            w_o = block.attn._o
            w_gu = block.mlp._gu
            w_d = block.mlp._d
            
            # è·å– KV Cache
            if use_graph_cache:
                # çœŸå®æ¨ç†æ¨¡å¼
                k_cache, v_cache = cache_manager.get(layer_idx)
                cache_seqlens = cache_manager._cache_seqlens_buffer[:batch_size]
                block_table = cache_manager._block_table_buffer[:batch_size]
            else:
                # Warmup æ¨¡å¼ï¼š
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šè·å–çœŸå®çš„ KV Cache æŒ‡é’ˆä»¥ä¿è¯å½¢çŠ¶æ­£ç¡®
                k_cache, v_cache = cache_manager.get(layer_idx)
                
                # åªå°† metadata è®¾ä¸º dummy
                cache_seqlens = torch.ones(batch_size, dtype=torch.int32, device=self.device)
                block_table = torch.zeros(batch_size, self.attention.max_blocks, dtype=torch.int32, device=self.device)
            
            # === Attention Block ===
            normed = torch.nn.functional.rms_norm(h, h.shape[-1:], block.ln_1.weight, block.ln_1.eps)
            
            qkv = torch.matmul(normed, w_qkv)
            if b_qkv is not None:
                qkv = qkv + b_qkv
            
            # Split QKV
            qkv_reshaped = qkv.reshape(batch_size, 3, self.num_heads, self.head_size)
            q, k, v = qkv_reshaped[:, 0], qkv_reshaped[:, 1], qkv_reshaped[:, 2]
            
            # Flash Attention
            attn = flash_attn_with_kvcache(
                q=q.unsqueeze(1),
                k_cache=k_cache,
                v_cache=v_cache,
                k=k.unsqueeze(1),
                v=v.unsqueeze(1),
                rotary_cos=self.attention._cos_pool,
                rotary_sin=self.attention._sin_pool,
                cache_seqlens=cache_seqlens,
                block_table=block_table,
                causal=True,
                window_size=(-1, -1),
                rotary_interleaved=False,
                alibi_slopes=None,
            ).squeeze(1)
            
            # === MLP Block ===
            out = torch.matmul(attn.reshape(batch_size, -1), w_o)
            residual = out + h
            
            normed = torch.nn.functional.rms_norm(residual, residual.shape[-1:], block.ln_2.weight, block.ln_2.eps)
            gate_up = torch.matmul(normed, w_gu)
            
            up, gate = gate_up.chunk(2, dim=-1)
            activated = swiglu(gate, up)
            
            mlp_out = torch.matmul(activated, w_d)
            h = mlp_out + residual
            
        return h
    
    def capture(self, cache_manager, batch_sizes: List[int] = [1, 2, 4, 8, 16]):
        """æ•è·CUDA Graph"""
        if self._ready:
            return
        
        logger.info(f"Capturing ModelGraph for {self.num_layers} layers x {len(batch_sizes)} batch sizes")
        
        for bs in batch_sizes:
            self._capture_single(bs, cache_manager)
        
        self._ready = True
        logger.info("ModelGraph capture completed")
    
    def _capture_single(self, batch_size: int, cache_manager):
        """æ•è·å•ä¸ªbatch_sizeçš„graph"""
        g = torch.cuda.CUDAGraph()
        
        # é¢„çƒ­
        self._warmup(batch_size, cache_manager)
        
        with torch.cuda.graph(g):
            # è¾“å…¥è§†å›¾
            h = self._hidden[:batch_size]
            
            # è°ƒç”¨æ ¸å¿ƒé€»è¾‘
            output_h = self._forward_pass(h, batch_size, cache_manager, use_graph_cache=True)
            
            # å†™å…¥è¾“å‡ºç¼“å†²åŒº
            self._output[:batch_size] = output_h
        
        self._graphs[batch_size] = g
    
    def _warmup(self, batch_size: int, cache_manager, num_warmup: int = 3):
        """é¢„çƒ­"""
        dummy_hidden = torch.randn(
            batch_size, self.hidden_dim,
            dtype=self.dtype, device=self.device
        )
        
        for _ in range(num_warmup):
            with torch.no_grad():
                self._eager(dummy_hidden, batch_size, cache_manager)
        
        torch.cuda.synchronize()

    def _eager(self, hidden_states, batch_size: int, cache_manager):
        """Eageræ¨¡å¼çš„å‰å‘ï¼ˆç”¨äºé¢„çƒ­ï¼‰"""
        h = hidden_states.squeeze(1) if hidden_states.dim() == 3 else hidden_states
        
        # è°ƒç”¨æ ¸å¿ƒé€»è¾‘
        return self._forward_pass(h, batch_size, cache_manager, use_graph_cache=False)
    
    def forward(self, hidden_states, cache_manager, batch_size: int):
        """æ‰§è¡Œå‰å‘ä¼ æ’­"""
        # è¾“å…¥æ‹·è´
        self._hidden[:batch_size] = hidden_states.squeeze(1) if hidden_states.dim() == 3 else hidden_states
        
        # Replay Graph
        if batch_size not in self._graphs:
            print(f"Graph not found for batch_size={batch_size}")
            return self._eager(hidden_states, batch_size, cache_manager)
        
        self._graphs[batch_size].replay()
        
        # è¿”å›è¾“å‡ºè§†å›¾
        return self._output[:batch_size]
    
    @property
    def is_ready(self) -> bool:
        return self._ready