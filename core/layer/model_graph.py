"""
===================================================================
ModelGraphRunner - æ•´ä¸ªæ¨¡å‹å±‚çš„CUDA Graphå°è£…
===================================================================

ğŸ“Œ **åŠŸèƒ½**ï¼š
   å°†æ‰€æœ‰transformer layerå°è£…åˆ°ä¸€ä¸ªCUDA Graphä¸­ï¼Œä¸€æ¬¡replayå®Œæˆæ‰€æœ‰å±‚çš„è®¡ç®—

âš¡ **æ€§èƒ½æå‡**ï¼š
   - å‡å°‘ Næ¬¡ graph replay â†’ 1æ¬¡ graph replay
   - æ¶ˆé™¤å±‚é—´è°ƒåº¦overhead
"""

import logging
import torch
import torch.nn.functional as F
from typing import Dict, List
from core.paged_attention import PagedAttention
from kernel.swiglu import swiglu_fused as swiglu

try:
    from flash_attn import flash_attn_with_kvcache
except ImportError:
    flash_attn_with_kvcache = None

logger = logging.getLogger(__name__)


class ModelGraphRunner:
    """
    ğŸ“Œ **æ¨¡å‹Graphè¿è¡Œå™¨** - ä¸€æ¬¡å‰å‘å¤„ç†æ‰€æœ‰å±‚
    
    ğŸ” **è®¾è®¡**:
        - å°†æ‰€æœ‰transformer layerçš„è®¡ç®—å°è£…åˆ°ä¸€ä¸ªCUDA Graphä¸­
        - ä½¿ç”¨é™æ€bufferé¿å…é‡å¤å†…å­˜åˆ†é…
        - æ”¯æŒå¤šä¸ªbatch_sizeçš„graph
    
    âš¡ **æ€§èƒ½**:
        - åŸæ¥ï¼šnum_layersæ¬¡ graph.replay()
        - ç°åœ¨ï¼š1æ¬¡ graph.replay()
    """
    
    def __init__(self, model, num_layers: int, num_heads: int, head_size: int,
                 kv_num_heads: int, hidden_dim: int, intermediate_size: int,
                 device: str, max_batch_size: int = 16):
        self.model = model
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.head_size = head_size
        self.kv_num_heads = kv_num_heads
        self.hidden_dim = hidden_dim
        self.intermediate_size = intermediate_size
        self.device = device
        self.max_batch_size = max_batch_size
        
        # åˆå§‹åŒ– PagedAttention
        self.attention = PagedAttention(
            num_heads=num_heads,
            head_size=head_size,
            kv_num_heads=kv_num_heads,
            device=device,
            max_batch_size=max_batch_size
        )
        
        # åˆå§‹åŒ–ç¼“å†²åŒº
        self._init_buffers()
        
        # é¢„ç¼“å­˜æƒé‡
        self.prepare()
        
        # Graphå­˜å‚¨
        self._graphs: Dict[int, torch.cuda.CUDAGraph] = {}  # batch_size -> graph
        self._ready = False
        
        # é¢„åˆ†é…è¾“å‡ºç¼“å†²åŒºï¼ˆé¿å…æ¯æ¬¡ cloneï¼‰
        self._output_buffer = torch.empty(
            (self.max_batch_size, self.hidden_dim),
            dtype=torch.bfloat16, device=self.device
        )
    
    def prepare(self):
        """é¢„ç¼“å­˜è½¬ç½®æƒé‡ï¼ˆä¸layer.pyä¸€è‡´ï¼‰"""
        for idx, block in enumerate(self.model.transformer.h):
            mlp = block.mlp
            
            # Gate + Up: [2*intermediate, hidden] -> [hidden, 2*intermediate]
            gate_up = torch.cat([mlp.w1.weight, mlp.w2.weight], dim=0)
            mlp._gu = gate_up.t().contiguous()
            
            # Down: [intermediate, hidden] -> [hidden, intermediate]
            mlp._d = mlp.c_proj.weight.t().contiguous()
            
            # Attention output
            block.attn._o = block.attn.c_proj.weight.t().contiguous()
            
            # QKV
            block.attn._qkv_w = block.attn.c_attn.weight.t().contiguous()
            if block.attn.c_attn.bias is not None:
                block.attn._qkv_b = block.attn.c_attn.bias
            else:
                block.attn._qkv_b = None
        
        logger.info("âœ… ModelGraphRunner æƒé‡é¢„ç¼“å­˜å®Œæˆ")
    
    def _init_buffers(self):
        """åˆå§‹åŒ–é™æ€ç¼“å†²åŒº"""
        max_b = self.max_batch_size
        
        # è¾“å…¥ç¼“å†²åŒº
        self._hidden = torch.empty(
            (max_b, self.hidden_dim),
            dtype=torch.bfloat16, device=self.device
        )
        
        # å±‚é—´ä¸´æ—¶ç¼“å†²åŒºï¼ˆå¤ç”¨åŒä¸€ä¸ªbufferï¼‰
        self._intermediate = torch.empty_like(self._hidden)
        
        # æ®‹å·®ç¼“å†²åŒº
        self._residual = torch.empty_like(self._hidden)
        
        # å½’ä¸€åŒ–è¾“å‡ºç¼“å†²åŒº
        self._normed_1 = torch.empty_like(self._hidden)
        self._normed_2 = torch.empty_like(self._hidden)
        
        # QKVç¼“å†²åŒº
        self._qkv = torch.empty(
            (max_b, 3 * self.hidden_dim),
            dtype=torch.bfloat16, device=self.device
        )
        
        # MLPä¸­é—´ç¼“å†²åŒº
        self._gate_up = torch.empty(
            (max_b, 2 * self.intermediate_size),
            dtype=torch.bfloat16, device=self.device
        )
        
        # æœ€ç»ˆè¾“å‡ºç¼“å†²åŒº
        self._output = torch.empty_like(self._hidden)
    
    def capture(self, cache_manager, batch_sizes: List[int] = [1, 2, 4, 8, 16]):
        """
        æ•è·CUDA Graph
        
        Args:
            cache_manager: KVCacheManagerå®ä¾‹
            batch_sizes: éœ€è¦æ•è·çš„batch_sizeåˆ—è¡¨
        """
        if self._ready:
            return
        
        logger.info(f"Capturing ModelGraph for {self.num_layers} layers x {len(batch_sizes)} batch sizes")
        
        for bs in batch_sizes:
            self._capture_single(bs, cache_manager)
        
        self._ready = True
        logger.info("ModelGraph capture completed")
    
    def _capture_single(self, batch_size: int, cache_manager):
        """æ•è·å•ä¸ªbatch_sizeçš„graph"""
        g = torch.cuda.CUDAGraph()
        
        # é¢„çƒ­
        self._warmup(batch_size, cache_manager)
        
        with torch.cuda.graph(g):
            # ============ Layer 0 ============
            h = self._hidden[:batch_size]
            
            for layer_idx in range(self.num_layers):
                block = self.model.transformer.h[layer_idx]
                
                # è·å–æƒé‡
                w_qkv = block.attn._qkv_w
                b_qkv = block.attn._qkv_b
                w_o = block.attn._o
                w_gu = block.mlp._gu
                w_d = block.mlp._d
                
                # è·å–KV cache
                k_cache, v_cache = cache_manager.get(layer_idx)
                
                # === Attention ===
                # RMSNorm
                normed = torch.nn.functional.rms_norm(
                    h, h.shape[-1:], block.ln_1.weight, block.ln_1.eps
                )
                self._normed_1[:batch_size] = normed
                
                # QKV Linear
                self._qkv[:batch_size] = torch.matmul(self._normed_1[:batch_size], w_qkv)
                if b_qkv is not None:
                    self._qkv[:batch_size] = self._qkv[:batch_size] + b_qkv
                
                # Split QKV
                q = self._qkv[:batch_size][:, :self.hidden_dim].reshape(
                    batch_size, self.num_heads, self.head_size
                )
                k = self._qkv[:batch_size][:, self.hidden_dim:2*self.hidden_dim].reshape(
                    batch_size, self.kv_num_heads, self.head_size
                )
                v = self._qkv[:batch_size][:, 2*self.hidden_dim:].reshape(
                    batch_size, self.kv_num_heads, self.head_size
                )
                
                # FlashAttention with KV cache
                attn = flash_attn_with_kvcache(
                    q=q.unsqueeze(1),
                    k_cache=k_cache,
                    v_cache=v_cache,
                    k=k.unsqueeze(1),
                    v=v.unsqueeze(1),
                    rotary_cos=self.attention._cos_pool,
                    rotary_sin=self.attention._sin_pool,
                    cache_seqlens=cache_manager._cache_seqlens_buffer[:batch_size],
                    block_table=cache_manager._block_table_buffer[:batch_size],
                    causal=True,
                    window_size=(-1, -1),
                    rotary_interleaved=False,
                    alibi_slopes=None,
                ).squeeze(1)
                
                # === MLP ===
                # O Proj
                out = torch.matmul(attn.reshape(batch_size, -1), w_o)
                
                # First Residual
                self._residual[:batch_size] = out + h
                
                # MLP RMSNorm
                normed = torch.nn.functional.rms_norm(
                    self._residual[:batch_size],
                    self._residual[:batch_size].shape[-1:],
                    block.ln_2.weight,
                    block.ln_2.eps
                )
                self._normed_2[:batch_size] = normed
                
                # Gate + Up
                gate_up = torch.matmul(self._normed_2[:batch_size], w_gu)
                
                # SwiGLU
                activated = swiglu(gate_up)

                # Down Proj
                mlp_out = torch.matmul(activated, w_d)
                
                # Second Residual -> å­˜åˆ°intermediateæˆ–output
                if layer_idx < self.num_layers - 1:
                    # ä¸æ˜¯æœ€åä¸€å±‚ï¼Œè¾“å‡ºç»™ä¸‹ä¸€å±‚
                    self._intermediate[:batch_size] = mlp_out + self._residual[:batch_size]
                    h = self._intermediate[:batch_size]
                else:
                    # æœ€åä¸€å±‚
                    self._output[:batch_size] = mlp_out + self._residual[:batch_size]
        
        self._graphs[batch_size] = g
    
    def _warmup(self, batch_size: int, cache_manager, num_warmup: int = 3):
        """é¢„çƒ­"""
        dummy_hidden = torch.randn(
            batch_size, self.hidden_dim,
            dtype=torch.bfloat16, device=self.device
        )
        
        for _ in range(num_warmup):
            with torch.no_grad():
                self._eager(dummy_hidden, batch_size, cache_manager)
        
        torch.cuda.synchronize()

    def _eager(self, hidden_states, batch_size: int, cache_manager):
        """Eageræ¨¡å¼çš„å‰å‘ï¼ˆç”¨äºé¢„çƒ­ï¼‰"""
        h = hidden_states.squeeze(1) if hidden_states.dim() == 3 else hidden_states
        
        for layer_idx in range(self.num_layers):
            block = self.model.transformer.h[layer_idx]
            
            w_qkv = block.attn._qkv_w
            b_qkv = block.attn._qkv_b
            w_o = block.attn._o
            w_gu = block.mlp._gu
            w_d = block.mlp._d
            k_cache, v_cache = cache_manager.get(layer_idx)
            
            # Attention
            normed = torch.nn.functional.rms_norm(
                h, h.shape[-1:], block.ln_1.weight, block.ln_1.eps
            )
            qkv = torch.matmul(normed, w_qkv)
            if b_qkv is not None:
                qkv = qkv + b_qkv
            
            q = qkv[:, :self.hidden_dim].reshape(batch_size, self.num_heads, self.head_size)
            k = qkv[:, self.hidden_dim:2*self.hidden_dim].reshape(batch_size, self.kv_num_heads, self.head_size)
            v = qkv[:, 2*self.hidden_dim:].reshape(batch_size, self.kv_num_heads, self.head_size)
            
            attn = flash_attn_with_kvcache(
                q=q.unsqueeze(1),
                k_cache=k_cache,
                v_cache=v_cache,
                k=k.unsqueeze(1),
                v=v.unsqueeze(1),
                rotary_cos=self.attention._cos_pool,
                rotary_sin=self.attention._sin_pool,
                cache_seqlens=torch.ones(batch_size, dtype=torch.int32, device=self.device),
                block_table=torch.zeros(batch_size, self.attention.max_blocks, 
                                    dtype=torch.int32, device=self.device),
                causal=True,
                window_size=(-1, -1),
                rotary_interleaved=False,
                alibi_slopes=None,
            ).squeeze(1)
            
            # MLP
            out = torch.matmul(attn.reshape(batch_size, -1), w_o)
            h = out + h
            
            normed = torch.nn.functional.rms_norm(
                h, h.shape[-1:], block.ln_2.weight, block.ln_2.eps
            )
            gate_up = torch.matmul(normed, w_gu)
            activated = swiglu(gate_up)
            mlp_out = torch.matmul(activated, w_d)
            h = mlp_out + h
        
        return h
    
    def forward(self, hidden_states, cache_manager, batch_size: int):
        """
        æ‰§è¡Œå‰å‘ä¼ æ’­
        
        Args:
            hidden_states: è¾“å…¥hidden states [batch_size, hidden_dim]
            cache_manager: KVCacheManagerå®ä¾‹
            batch_size: batchå¤§å°
            
        Returns:
            output: è¾“å‡ºhidden states [batch_size, hidden_dim]
        """
        # å°†è¾“å…¥å¤åˆ¶åˆ°ç¼“å†²åŒº
        self._hidden[:batch_size] = hidden_states.squeeze(1) if hidden_states.dim() == 3 else hidden_states
        
        # Replay Graph
        if batch_size not in self._graphs:
            print(f"Graph not found for batch_size={batch_size}")
            return self._eager(hidden_states, batch_size, cache_manager)
        
        self._graphs[batch_size].replay()
        
        # è¿”å›è¾“å‡ºï¼ˆç›´æ¥è¿”å› viewï¼Œé¿å… cloneï¼‰
        # æ³¨æ„ï¼šç”±äºæ˜¯å•çº¿ç¨‹é¡ºåºæ‰§è¡Œï¼Œä¸‹ä¸€ä¸ª step å‰ hidden_states å·²è¢« norm å¤„ç†å®Œ
        return self._output[:batch_size]
    
    @property
    def is_ready(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç»æ•è·äº†graph"""
        return self._ready
