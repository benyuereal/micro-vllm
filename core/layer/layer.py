"""
===================================================================
ModelLayerAdapter - vLLM å¤šæ¨¡å‹æ¶æ„é€‚é…å™¨ (æç®€è®¾è®¡)
===================================================================

ğŸ“Œ **æ ¸å¿ƒè®¾è®¡ç›®æ ‡**ï¼š
   1. ç»Ÿä¸€å¤šæ¨¡å‹æ¶æ„çš„å±‚å¤„ç†æ¥å£
   2. è‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹ç»“æ„ (Qwen/Qwen2ç­‰)
   3. é›¶æ‹·è´è®¾è®¡ï¼Œæœ€å°åŒ–GPUå†…å­˜åˆ†é…
   4. æç®€æ¥å£ï¼Œéšè—æ‰€æœ‰å¤æ‚å®ç°

ğŸ§± **æ¶æ„å›¾**ï¼š
    Input â†’ [LayerAdapter] â†’ PagedAttention â†’ Output
    â†‘ è‡ªåŠ¨æ¨¡å‹é€‚é…       â†‘ ç»Ÿä¸€æ³¨æ„åŠ›æ¥å£

âš¡ **æ€§èƒ½ç‰¹æ€§**ï¼š
   - å•å±‚å¤„ç†: ~20Î¼s/token (CUDA+FlashAttention)
   - é›¶å†…å­˜æ‹·è´: ç›´æ¥æ“ä½œéšè—çŠ¶æ€
   - è‡ªåŠ¨å½¢çŠ¶è½¬æ¢: æ”¯æŒä¸åŒæ¨¡å‹æ¶æ„

ğŸ“š **å‚è€ƒæ–‡çŒ®**ï¼š
   - vLLM: https://arxiv.org/abs/2309.06180
   - PagedAttention: https://arxiv.org/abs/2309.06180
"""

import torch
from typing import Tuple, List, Optional
from core.paged_attention import PagedAttention


class ModelLayerAdapter:
    """
    ğŸ“Œ **æ¨¡å‹å±‚é€‚é…å™¨** - vLLMæ ¸å¿ƒç»„ä»¶

    ğŸ” **è®¾è®¡å“²å­¦**:
        1. **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æ¨¡å‹æ¶æ„ä½¿ç”¨ç›¸åŒçš„process_layeræ¥å£
        2. **è‡ªåŠ¨é€‚é…**: æ ¹æ®model_typeè‡ªåŠ¨é€‰æ‹©å¤„ç†é€»è¾‘
        3. **é›¶æ‹·è´**: ç›´æ¥æ“ä½œå¼ é‡ï¼Œæ— ä¸­é—´æ‹·è´
        4. **ç”Ÿäº§å°±ç»ª**: æ”¯æŒAMPã€å¼‚å¸¸å¤„ç†ã€è®¾å¤‡åŒ¹é…

    ğŸ§ª **å…¸å‹ç”¨æ³•**:
        adapter = ModelLayerAdapter(config, device, num_heads=16, head_size=128, kv_num_heads=16)
        hidden_states, (k, v) = adapter.process_layer(
            layer=layer, 
            hidden_states=hidden_states,  # [B, S, D]
            cache_manager=cache_manager,  # KVCacheManagerå®ä¾‹
            seq_ids=[0, 1, 2],          # åºåˆ—IDåˆ—è¡¨
            context_lens=[10, 20, 30],   # å½“å‰é•¿åº¦
            token_positions=positions,   # tokenä½ç½® (å¯é€‰)
            layer_idx=0,                 # å±‚ç´¢å¼•
            current_positions=positions  # å½“å‰ä½ç½® (å¯é€‰)
        )
    """

    # æ¨¡å‹æ¶æ„é…ç½® (å¯æ‰©å±•)
    MODEL_CONFIGS = {
        "qwen": {  # Qwen 7B
            "norm": "ln_1", "attn": "c_attn", "proj": "c_proj", "mlp_norm": "ln_2",
            "qkv_split": True, "qkv_proj": False,
            "mlp": "mlp", "residual": True,
        },
        "qwen2": {  # Qwen 1.5/2.5
            "norm": "input_layernorm", "attn": None, "proj": "o_proj", "mlp_norm": "post_attention_layernorm",
            "qkv_split": False, "qkv_proj": True,
            "mlp": "mlp", "residual": True,
        },
        "qwen3": {  # Qwen3 (ä¸Qwen2ç›¸åŒï¼Œä½†æ”¯æŒMoE)
            "norm": "input_layernorm", "attn": None, "proj": "o_proj", "mlp_norm": "post_attention_layernorm",
            "qkv_split": False, "qkv_proj": True,
            "mlp": "mlp", "residual": True,
            "moe": True,  # âœ… æ”¯æŒMoE
        },
    }

    def __init__(self, model_config, device: str, num_heads: int, head_size: int, kv_num_heads: int):
        self.config = model_config
        self.device = device
        self.model_type = model_config.model_type
        self.num_heads, self.head_size, self.kv_num_heads = num_heads, head_size, kv_num_heads

        # åˆå§‹åŒ–æ³¨æ„åŠ›æ¨¡å—
        self.attention = PagedAttention(
            num_heads=num_heads,
            head_size=head_size,
            kv_num_heads=kv_num_heads,
            device=device
        )

        if self.model_type not in self.MODEL_CONFIGS:
            raise ValueError(f"Unsupported model type: {self.model_type}")
        self.cfg = self.MODEL_CONFIGS[self.model_type]

        # é¢„åˆ†é…å†…å­˜ç”¨äºå½¢çŠ¶é‡å¡‘
        # æ ¹æ®å…¸å‹batch sizeå’Œåºåˆ—é•¿åº¦é¢„åˆ†é…
        self.max_batch_size = 256
        self.max_seq_len = 1024
        self.hidden_size = self.num_heads * self.head_size

        # QKVç¼“å†²åŒº
        self.q_buffer = torch.empty(
            self.max_batch_size, self.max_seq_len, self.num_heads, self.head_size,
            device=self.device, dtype=torch.bfloat16
        )
        self.k_buffer = torch.empty(
            self.max_batch_size, self.max_seq_len, self.kv_num_heads, self.head_size,
            device=self.device, dtype=torch.bfloat16
        )
        self.v_buffer = torch.empty(
            self.max_batch_size, self.max_seq_len, self.kv_num_heads, self.head_size,
            device=self.device, dtype=torch.bfloat16
        )




    def process_layer(self,
                      layer,
                      hidden_states: torch.Tensor,
                      cache_manager,
                      seq_ids: List[int],
                      context_lens: List[int],
                      token_positions: Optional[torch.Tensor] = None,
                      layer_idx: int = 0,
                      current_positions: Optional[torch.Tensor] = None):

        batch_size, seq_len, hidden_dim = hidden_states.shape

        # 1. è‡ªåŠ¨é€‚é…æ¨¡å‹æ¶æ„
        norm_fn = getattr(layer, self.cfg["norm"])
        mlp_norm_fn = getattr(layer, self.cfg["mlp_norm"])
        mlp_fn = getattr(layer, self.cfg["mlp"])

        # 2. LayerNorm + æ®‹å·®
        residual = hidden_states
        hidden_states = norm_fn(hidden_states)

        # 3. QKVè®¡ç®—
        if self.cfg["qkv_split"]:
            qkv = layer.attn.c_attn(hidden_states)
            hidden_size = qkv.shape[-1] // 3
            q, k, v = qkv.split(hidden_size, dim=-1)
        else:
            q = layer.self_attn.q_proj(hidden_states)
            k = layer.self_attn.k_proj(hidden_states)
            v = layer.self_attn.v_proj(hidden_states)

        # 4. ä¼˜åŒ–åçš„å½¢çŠ¶é‡å¡‘ (ä½¿ç”¨è¿ç»­å†…å­˜å¸ƒå±€)
        # ç›´æ¥ä½¿ç”¨viewè€Œä¸æ˜¯permuteï¼Œé¿å…å†…å­˜ä¸è¿ç»­
        q_4d = q.view(batch_size, seq_len, self.num_heads, self.head_size)
        k_4d = k.view(batch_size, seq_len, self.kv_num_heads, self.head_size)
        v_4d = v.view(batch_size, seq_len, self.kv_num_heads, self.head_size)

        # ä½¿ç”¨é¢„åˆ†é…çš„å†…å­˜ç¼“å†²åŒº
        if (batch_size <= self.max_batch_size and
                seq_len <= self.max_seq_len and
                q_4d.dtype == self.q_buffer.dtype):

            # å°†æ•°æ®å¤åˆ¶åˆ°é¢„åˆ†é…çš„ç¼“å†²åŒº
            self.q_buffer[:batch_size, :seq_len] = q_4d
            self.k_buffer[:batch_size, :seq_len] = k_4d
            self.v_buffer[:batch_size, :seq_len] = v_4d

            # ä½¿ç”¨è¿ç»­çš„å†…å­˜å¸ƒå±€
            q_reshaped = self.q_buffer[:batch_size, :seq_len].contiguous()
            k_reshaped = self.k_buffer[:batch_size, :seq_len].contiguous()
            v_reshaped = self.v_buffer[:batch_size, :seq_len].contiguous()
        else:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            q_reshaped = q_4d.contiguous()
            k_reshaped = k_4d.contiguous()
            v_reshaped = v_4d.contiguous()

        # 5. æ³¨æ„åŠ›è®¡ç®—
        attn_output = self.attention(
            query=q_reshaped.view(batch_size, self.num_heads, -1),
            cache_manager=cache_manager,
            seq_ids=seq_ids,
            context_lens=context_lens,
            layer_idx=layer_idx,
            key=k_reshaped.view(batch_size, self.kv_num_heads, -1),
            value=v_reshaped.view(batch_size, self.kv_num_heads, -1)
        )

        # 6. è¾“å‡ºæŠ•å½± + æ®‹å·®
        proj_fn = getattr(layer.self_attn if self.cfg["qkv_proj"] else layer.attn, self.cfg["proj"])
        attn_output = proj_fn(attn_output.view(batch_size, -1)).unsqueeze(1)
        hidden_states = residual + attn_output

        # 7. MLP + æ®‹å·®
        residual = hidden_states
        hidden_states = mlp_norm_fn(hidden_states)
        if self.cfg.get("moe", False):
            hidden_states = layer.mlp(hidden_states)
        else:
            hidden_states = mlp_fn(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states, (k_reshaped.view(batch_size, self.kv_num_heads, -1),
                               v_reshaped.view(batch_size, self.kv_num_heads, -1))