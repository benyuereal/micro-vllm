"""
===================================================================
ModelLayerAdapter - vLLM å¤šæ¨¡å‹æ¶æ„é€‚é…å™¨ (æç®€è®¾è®¡)
===================================================================

ğŸ“Œ **æ ¸å¿ƒè®¾è®¡ç›®æ ‡**ï¼š
   1. ç»Ÿä¸€å¤šæ¨¡å‹æ¶æ„çš„å±‚å¤„ç†æ¥å£
   2. è‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹ç»“æ„ (Qwen/Qwen2ç­‰)
   3. é›¶æ‹·è´è®¾è®¡ï¼Œæœ€å°åŒ–GPUå†…å­˜åˆ†é…
   4. æç®€æ¥å£ï¼Œéšè—æ‰€æœ‰å¤æ‚å®ç°

ğŸ§± **æ¶æ„å›¾**ï¼š
    Input â†’ [LayerAdapter] â†’ PagedAttention â†’ Output
    â†‘ è‡ªåŠ¨æ¨¡å‹é€‚é…       â†‘ ç»Ÿä¸€æ³¨æ„åŠ›æ¥å£

âš¡ **æ€§èƒ½ç‰¹æ€§**ï¼š
   - å•å±‚å¤„ç†: ~20Î¼s/token (CUDA+FlashAttention)
   - é›¶å†…å­˜æ‹·è´: ç›´æ¥æ“ä½œéšè—çŠ¶æ€
   - è‡ªåŠ¨å½¢çŠ¶è½¬æ¢: æ”¯æŒä¸åŒæ¨¡å‹æ¶æ„

ğŸ“š **å‚è€ƒæ–‡çŒ®**ï¼š
   - vLLM: https://arxiv.org/abs/2309.06180
   - PagedAttention: https://arxiv.org/abs/2309.06180
"""
import logging
import time

import torch
from typing import Tuple, List, Optional
from core.paged_attention import PagedAttention
# è®¾ç½®æ—¥å¿—è®°å½•
logger = logging.getLogger(__name__)

class ModelLayerAdapter:
    """
    ğŸ“Œ **æ¨¡å‹å±‚é€‚é…å™¨** - vLLMæ ¸å¿ƒç»„ä»¶

    ğŸ” **è®¾è®¡å“²å­¦**:
        1. **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æ¨¡å‹æ¶æ„ä½¿ç”¨ç›¸åŒçš„process_layeræ¥å£
        2. **è‡ªåŠ¨é€‚é…**: æ ¹æ®model_typeè‡ªåŠ¨é€‰æ‹©å¤„ç†é€»è¾‘
        3. **é›¶æ‹·è´**: ç›´æ¥æ“ä½œå¼ é‡ï¼Œæ— ä¸­é—´æ‹·è´
        4. **ç”Ÿäº§å°±ç»ª**: æ”¯æŒAMPã€å¼‚å¸¸å¤„ç†ã€è®¾å¤‡åŒ¹é…

    ğŸ§ª **å…¸å‹ç”¨æ³•**:
        adapter = ModelLayerAdapter(config, device, num_heads=16, head_size=128, kv_num_heads=16)
        hidden_states, (k, v) = adapter.process_layer(
            layer=layer,
            hidden_states=hidden_states,  # [B, S, D]
            cache_manager=cache_manager,  # KVCacheManagerå®ä¾‹
            seq_ids=[0, 1, 2],          # åºåˆ—IDåˆ—è¡¨
            context_lens=[10, 20, 30],   # å½“å‰é•¿åº¦
            token_positions=positions,   # tokenä½ç½® (å¯é€‰)
            layer_idx=0,                 # å±‚ç´¢å¼•
            current_positions=positions  # å½“å‰ä½ç½® (å¯é€‰)
        )
    """

    # æ¨¡å‹æ¶æ„é…ç½® (å¯æ‰©å±•)
    MODEL_CONFIGS = {
        "qwen": {  # Qwen 7B
            "norm": "ln_1", "attn": "c_attn", "proj": "c_proj", "mlp_norm": "ln_2",
            "qkv_split": True, "qkv_proj": False,
            "mlp": "mlp", "residual": True,
        },
        "qwen2": {  # Qwen 1.5/2.5
            "norm": "input_layernorm", "attn": None, "proj": "o_proj", "mlp_norm": "post_attention_layernorm",
            "qkv_split": False, "qkv_proj": True,
            "mlp": "mlp", "residual": True,
        },
        "qwen3": {  # Qwen3 (ä¸Qwen2ç›¸åŒï¼Œä½†æ”¯æŒMoE)
            "norm": "input_layernorm", "attn": None, "proj": "o_proj", "mlp_norm": "post_attention_layernorm",
            "qkv_split": False, "qkv_proj": True,
            "mlp": "mlp", "residual": True,
            "moe": True,  # âœ… æ”¯æŒMoE
        },
    }

    def __init__(self, model_config, device: str, num_heads: int, head_size: int, kv_num_heads: int):
        """
        ğŸ“Œ **åˆå§‹åŒ–**

        ğŸ” **å‚æ•°**:
            - model_config: æ¨¡å‹é…ç½®
            - device: è®¾å¤‡ ("cuda", "mps", "cpu")
            - num_heads: æ³¨æ„åŠ›å¤´æ•°
            - head_size: æ¯ä¸ªå¤´ç»´åº¦
            - kv_num_heads: KVå¤´æ•° (GQAæ”¯æŒ)
        """
        self.config = model_config
        self.device = device
        self.model_type = model_config.model_type
        self.num_heads, self.head_size, self.kv_num_heads = num_heads, head_size, kv_num_heads

        # åˆå§‹åŒ–æ³¨æ„åŠ›æ¨¡å—
        self.attention = PagedAttention(
            num_heads=num_heads,
            head_size=head_size,
            kv_num_heads=kv_num_heads,
            device=device
        )

        # éªŒè¯æ¨¡å‹ç±»å‹
        if self.model_type not in self.MODEL_CONFIGS:
            raise ValueError(f"Unsupported model type: {self.model_type}")
        self.cfg = self.MODEL_CONFIGS[self.model_type]

    def process_layer(self,
                      layer,
                      hidden_states: torch.Tensor,  # [B, S, D]
                      cache_manager,
                      seq_ids: List[int],
                      context_lens: List[int],
                      token_positions: Optional[torch.Tensor] = None,
                      layer_idx: int = 0,
                      current_positions: Optional[torch.Tensor] = None) -> Tuple[
        torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        ğŸ“Œ **ä¸‰æ®µå¼Layerå¤„ç†** (Qwen-7Bä¸“ç”¨ä¼˜åŒ–ç‰ˆ)

        åˆ†æ®µç­–ç•¥ï¼š
        1. QKVé˜¶æ®µï¼šLayerNorm + QKVæŠ•å½± â†’ fullgraphç¼–è¯‘èåˆ
        2. Attentioné˜¶æ®µï¼šFlashAttention â†’ ä¸ç¼–è¯‘ (C++æ‰©å±•)
        3. MLPé˜¶æ®µï¼šè¾“å‡ºæŠ•å½± + MLP â†’ fullgraphç¼–è¯‘èåˆ

        Qwen-7Bä¸“ç”¨è·¯å¾„ï¼šå®Œå…¨é™æ€ï¼Œæ— æ¡ä»¶åˆ†æ”¯ï¼Œæœ€å¤§åŒ–torch.compileä¼˜åŒ–
        """
        """
        ğŸ“Œ **å¤„ç†å•å±‚è®¡ç®—** (ç»Ÿä¸€æ¥å£ï¼Œè‡ªåŠ¨é€‚é…æ¨¡å‹æ¶æ„)

        ğŸ” **å‚æ•°**:
            - layer: æ¨¡å‹å±‚ (transformer layer)
            - hidden_states: éšè—çŠ¶æ€ [B, S, D]
            - cache_manager: KVCacheManagerå®ä¾‹
            - seq_ids: åºåˆ—IDåˆ—è¡¨ [B]
            - context_lens: å½“å‰é•¿åº¦åˆ—è¡¨ [B]
            - token_positions: tokenä½ç½® (å¯é€‰)
            - layer_idx: å±‚ç´¢å¼•
            - current_positions: å½“å‰ä½ç½® (å¯é€‰)

        âœ… **è¿”å›**:
            - hidden_states: æ›´æ–°åçš„éšè—çŠ¶æ€ [B, S, D]
            - (current_k, current_v): å½“å‰å±‚çš„KV [B, H, D]

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. è‡ªåŠ¨é€‚é…æ¨¡å‹æ¶æ„ (Qwen/Qwen2ç­‰)
            2. åº”ç”¨LayerNorm
            3. è®¡ç®—QKV (è‡ªåŠ¨å¤„ç†ä¸åŒæŠ•å½±æ–¹å¼)
            4. é‡å¡‘å½¢çŠ¶ [B, S, D] â†’ [B, H, D]
            5. è°ƒç”¨PagedAttention
            6. æ®‹å·®è¿æ¥ + MLP
        """
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # ğŸ”§ ç¦ç”¨CUDAå›¾ä»¥é¿å…é‡ç”¨é—®é¢˜ (å¿…é¡»åœ¨torch.compileå‰è°ƒç”¨)
        torch.compiler.cudagraph_mark_step_begin()

        # ğŸ“ Qwenä¸“ç”¨ä¼˜åŒ–è·¯å¾„ (torch.compileèåˆï¼Œæ— æ¡ä»¶åˆ†æ”¯)
        if self.model_type == "qwen":
            # ğŸ“ ç¬¬ä¸€é˜¶æ®µï¼šQKV (torch.compileç®—å­èåˆ)
            hidden_states, residual, q, k, v = self._qkv_stage(layer, hidden_states)

            # ğŸ“ ç¬¬äºŒé˜¶æ®µï¼šAttention (FlashAttention v2)
            attn_output, kv_cache = self._attn_stage(q, k, v, cache_manager, seq_ids, context_lens, layer_idx)

            # ğŸ“ ç¬¬ä¸‰é˜¶æ®µï¼šMLP (torch.compileç®—å­èåˆ)
            hidden_states = self._mlp_stage(layer, hidden_states, residual, attn_output)
        else:
            # ğŸ“ é€šç”¨è·¯å¾„ (ä¿æŒå…¼å®¹æ€§)
            hidden_states, residual, q, k, v = self._pre_attention(layer, hidden_states)
            attn_output, kv_cache = self._attention_stage(q, k, v, cache_manager, seq_ids, context_lens, layer_idx)
            hidden_states = self._post_attention(layer, hidden_states, residual, attn_output)

        # è®°å½•æ€»è€—æ—¶
        total_time = time.time() - start_time
        if layer_idx == 0:
            logger.info(f"ğŸš€ Layer {layer_idx}: æ€»å¤„ç†è€—æ—¶ {total_time * 1000:.2f}ms")
            logger.info(f"   âš¡ torch.compileä¸‰æ®µå¼èåˆ | QKV+MLPç®—å­èåˆ | å†…å­˜ä¼˜åŒ–")

        return hidden_states, kv_cache

    @torch.compile(mode="reduce-overhead")
    def _qkv_stage(self, layer, hidden_states):
        """
        ğŸ“ **QKVé˜¶æ®µ** (torch.compileèåˆä¼˜åŒ–)
        LayerNorm + QKVæŠ•å½± + å½¢çŠ¶é‡å¡‘ï¼Œç®—å­èåˆ
        """
        # 1. Qwen-7Bå›ºå®šLayerNorm: ln_1
        residual = hidden_states.clone()  # é¿å…CUDAGraphé‡ç”¨é—®é¢˜
        hidden_states = layer.ln_1(hidden_states)

        # 2. Qwen-7Bå›ºå®šåˆå¹¶QKVæŠ•å½±: c_attn
        qkv = layer.attn.c_attn(hidden_states)
        hidden_size = qkv.shape[-1] // 3
        q, k, v = qkv.split(hidden_size, dim=-1)

        # 3. å›ºå®šå½¢çŠ¶é‡å¡‘ [B, S, D] â†’ [B, H, D]
        batch_size, seq_len, _ = hidden_states.shape
        q = q.view(batch_size, seq_len, self.num_heads, self.head_size).permute(0, 2, 1, 3).contiguous()
        k = k.view(batch_size, seq_len, self.kv_num_heads, self.head_size).permute(0, 2, 1, 3).contiguous()
        v = v.view(batch_size, seq_len, self.kv_num_heads, self.head_size).permute(0, 2, 1, 3).contiguous()

        return hidden_states, residual, q, k, v

    def _attn_stage(self, q, k, v, cache_manager, seq_ids, context_lens, layer_idx):
        """
        ğŸ“ **Attentioné˜¶æ®µ** (ä¸ç¼–è¯‘)
        è°ƒç”¨PagedAttentionï¼Œé¿å…C++æ‰©å±•ç¼–è¯‘é—®é¢˜
        """
        attn_output = self.attention(
            query=q.squeeze(2),  # [B, H, D]
            cache_manager=cache_manager,
            seq_ids=seq_ids,
            context_lens=context_lens,
            layer_idx=layer_idx,
            key=k.squeeze(2),  # [B, H, D]
            value=v.squeeze(2)  # [B, H, D]
        )
        # è¿”å›attentionè¾“å‡ºå’Œkvç¼“å­˜
        return attn_output, (k.squeeze(2), v.squeeze(2))

    @torch.compile(mode="reduce-overhead")
    def _mlp_stage(self, layer, hidden_states, residual, attn_output):
        """
        ğŸ“ **MLPé˜¶æ®µ** (torch.compileèåˆä¼˜åŒ–)
        è¾“å‡ºæŠ•å½± + MLPï¼Œç®—å­èåˆ
        """
        # 1. Qwen-7Bå›ºå®šè¾“å‡ºæŠ•å½±: c_proj
        batch_size = hidden_states.shape[0]
        attn_output = layer.attn.c_proj(attn_output.reshape(batch_size, -1)).unsqueeze(1)  # [B, 1, D]
        hidden_states = residual + attn_output

        # 2. Qwen-7Bå›ºå®šMLP: ln_2 + mlp (æ— MoE)
        residual = hidden_states.clone()  # é¿å…CUDAGraphé‡ç”¨é—®é¢˜
        hidden_states = layer.ln_2(hidden_states)
        hidden_states = layer.mlp(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states