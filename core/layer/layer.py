"""
===================================================================
ModelLayerAdapter - vLLM å¤šæ¨¡å‹æ¶æ„é€‚é…å™¨ (æç®€è®¾è®¡)
===================================================================

ğŸ“Œ **æ ¸å¿ƒè®¾è®¡ç›®æ ‡**ï¼š
   1. ç»Ÿä¸€å¤šæ¨¡å‹æ¶æ„çš„å±‚å¤„ç†æ¥å£
   2. è‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹ç»“æ„ (Qwen/Qwen2ç­‰)
   3. é›¶æ‹·è´è®¾è®¡ï¼Œæœ€å°åŒ–GPUå†…å­˜åˆ†é…
   4. æç®€æ¥å£ï¼Œéšè—æ‰€æœ‰å¤æ‚å®ç°

ğŸ§± **æ¶æ„å›¾**ï¼š
    Input â†’ [LayerAdapter] â†’ PagedAttention â†’ Output
    â†‘ è‡ªåŠ¨æ¨¡å‹é€‚é…       â†‘ ç»Ÿä¸€æ³¨æ„åŠ›æ¥å£

âš¡ **æ€§èƒ½ç‰¹æ€§**ï¼š
   - å•å±‚å¤„ç†: ~20Î¼s/token (CUDA+FlashAttention)
   - é›¶å†…å­˜æ‹·è´: ç›´æ¥æ“ä½œéšè—çŠ¶æ€
   - è‡ªåŠ¨å½¢çŠ¶è½¬æ¢: æ”¯æŒä¸åŒæ¨¡å‹æ¶æ„

ğŸ“š **å‚è€ƒæ–‡çŒ®**ï¼š
   - vLLM: https://arxiv.org/abs/2309.06180
   - PagedAttention: https://arxiv.org/abs/2309.06180
"""
import time

import torch
from typing import Tuple, List, Optional

from core import KVCacheManager
from core.paged_attention import PagedAttention


class ModelLayerAdapter:
    """
    ğŸ“Œ **æ¨¡å‹å±‚é€‚é…å™¨** - vLLMæ ¸å¿ƒç»„ä»¶

    ğŸ” **è®¾è®¡å“²å­¦**:
        1. **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æ¨¡å‹æ¶æ„ä½¿ç”¨ç›¸åŒçš„process_layeræ¥å£
        2. **è‡ªåŠ¨é€‚é…**: æ ¹æ®model_typeè‡ªåŠ¨é€‰æ‹©å¤„ç†é€»è¾‘
        3. **é›¶æ‹·è´**: ç›´æ¥æ“ä½œå¼ é‡ï¼Œæ— ä¸­é—´æ‹·è´
        4. **ç”Ÿäº§å°±ç»ª**: æ”¯æŒAMPã€å¼‚å¸¸å¤„ç†ã€è®¾å¤‡åŒ¹é…

    ğŸ§ª **å…¸å‹ç”¨æ³•**:
        adapter = ModelLayerAdapter(config, device, num_heads=16, head_size=128, kv_num_heads=16)
        hidden_states, (k, v) = adapter.process_layer(
            layer=layer, 
            hidden_states=hidden_states,  # [B, S, D]
            cache_manager=cache_manager,  # KVCacheManagerå®ä¾‹
            seq_ids=[0, 1, 2],          # åºåˆ—IDåˆ—è¡¨
            context_lens=[10, 20, 30],   # å½“å‰é•¿åº¦
            token_positions=positions,   # tokenä½ç½® (å¯é€‰)
            layer_idx=0,                 # å±‚ç´¢å¼•
            current_positions=positions  # å½“å‰ä½ç½® (å¯é€‰)
        )
    """
    """
        ğŸ“Œ A100 ç‰¹åŒ–ä¼˜åŒ–æ¨¡å‹å±‚é€‚é…å™¨

        ğŸ” è®¾è®¡ç†å¿µ:
            1. å†…å­˜é›¶æ‹·è´: é¢„åˆ†é…æ‰€æœ‰ç¼“å†²åŒº
            2. è®¡ç®—æè‡´ä¼˜åŒ–: åˆ©ç”¨ A100 Tensor Core
            3. å¼‚æ­¥æ‰§è¡Œ: è®¡ç®—ä¸å†…å­˜æ“ä½œé‡å 
            4. æ™ºèƒ½æ‰¹å¤„ç†: åŠ¨æ€è°ƒæ•´è®¡ç®—å‚æ•°
        """

    # æ¨¡å‹é…ç½®æ¨¡æ¿
    MODEL_CONFIGS = {
        "qwen": {
            "norm": "ln_1", "attn": "c_attn", "proj": "c_proj", "mlp_norm": "ln_2",
            "qkv_split": True, "qkv_proj": False, "mlp": "mlp", "residual": True
        },
        "qwen2": {
            "norm": "input_layernorm", "attn": None, "proj": "o_proj",
            "mlp_norm": "post_attention_layernorm", "qkv_split": False,
            "qkv_proj": True, "mlp": "mlp", "residual": True
        },
        "qwen3": {
            "norm": "input_layernorm", "attn": None, "proj": "o_proj",
            "mlp_norm": "post_attention_layernorm", "qkv_split": False,
            "qkv_proj": True, "mlp": "mlp", "residual": True, "moe": True
        },
    }

    def __init__(self, model_config, device: str, num_heads: int,
                 head_size: int, kv_num_heads: int):
        """
        åˆå§‹åŒ– A100 ä¼˜åŒ–é€‚é…å™¨

        Args:
            model_config: æ¨¡å‹é…ç½®å¯¹è±¡
            device: è®¡ç®—è®¾å¤‡ ('cuda', 'cpu')
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            head_size: æ¯ä¸ªå¤´çš„ç»´åº¦
            kv_num_heads: KV æ³¨æ„åŠ›å¤´æ•° (GQA æ”¯æŒ)
        """
        self.config = model_config
        self.device = torch.device(device)
        self.model_type = model_config.model_type

        # æ³¨æ„åŠ›å‚æ•°
        self.num_heads = num_heads
        self.head_size = head_size
        self.kv_num_heads = kv_num_heads

        # A100 ç‰¹åŒ–é…ç½®
        self._setup()

        # æ¨¡å‹é…ç½®
        self.cfg = self.MODEL_CONFIGS.get(self.model_type, self.MODEL_CONFIGS["qwen2"])

        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'total_time': 0, 'layer_count': 0, 'batch_sizes': []
        }

    def _setup(self):
        """è®¾ç½® A100 ç‰¹åŒ–ä¼˜åŒ–å‚æ•°"""
        # å†…å­˜é¢„åˆ†é… (é’ˆå¯¹ A100 40GB ä¼˜åŒ–)
        self.buffer_size = 512  # é¢„åˆ†é…æ‰¹æ¬¡å¤§å°
        self.__init_buffers()

        # PyTorch 2.0 ç¼–è¯‘ä¼˜åŒ–
        self._compile_func()

        # A100 ç‰¹åŒ–æ³¨æ„åŠ›æ¨¡å—
        self.attention = PagedAttention(
            num_heads=self.num_heads,
            head_size=self.head_size,
            kv_num_heads=self.kv_num_heads,
            device=self.device
        )

        # å¼‚æ­¥æ‰§è¡Œæµ
        self.compute_stream = torch.cuda.Stream()
        self.memory_stream = torch.cuda.Stream()

    def __init_buffers(self):
        """é¢„åˆ†é…å†…å­˜ç¼“å†²åŒº"""
        # QKV ç¼“å†²åŒº (BF16 æ ¼å¼åˆ©ç”¨ Tensor Core)
        self.q_buffer = torch.empty(
            (self.buffer_size, self.num_heads, self.head_size),
            device=self.device, dtype=torch.bfloat16
        )
        self.k_buffer = torch.empty(
            (self.buffer_size, self.kv_num_heads, self.head_size),
            device=self.device, dtype=torch.bfloat16
        )
        self.v_buffer = torch.empty(
            (self.buffer_size, self.kv_num_heads, self.head_size),
            device=self.device, dtype=torch.bfloat16
        )

        # ä¸­é—´ç»“æœç¼“å†²åŒº
        self.norm_buffer = torch.empty(
            (self.buffer_size, self.head_size * self.num_heads),
            device=self.device, dtype=torch.bfloat16
        )

        # ä½ç½®ç¼–ç ç¼“å†²åŒº
        self.pos_buffer = torch.empty(
            (self.buffer_size,), dtype=torch.int32, device=self.device
        )

    def _compile_func(self):
        """ç¼–è¯‘å…³é”®å‡½æ•°ä¸ºé«˜æ•ˆå†…æ ¸"""
        try:
            # ä½¿ç”¨æœ€å¤§ä¼˜åŒ–çº§åˆ«
            self._fast_norm = torch.compile(
                self._layer_norm,
                mode="max-autotune",
                fullgraph=True
            )
            self._fast_qkv = torch.compile(
                self._compute_qkv,
                mode="max-autotune"
            )
            self._fast_reshape = torch.compile(
                self._reshape,
                mode="max-autotune"
            )
        except Exception as e:
            print(f"âš ï¸ ç¼–è¯‘è­¦å‘Š: {e}, ä½¿ç”¨åŸç”Ÿå‡½æ•°")
            self._fast_norm = self._layer_norm
            self._fast_qkv = self._compute_qkv
            self._fast_reshape = self._reshape

    @torch.inference_mode()
    def process_layer(self, layer, hidden_states: torch.Tensor,
                      cache_manager: KVCacheManager, seq_ids: List[int],
                      context_lens: List[int], layer_idx: int = 0,
                      **kwargs) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        ğŸ“Œ ä¼˜åŒ–ç‰ˆå•å±‚å¤„ç† (A100 ç‰¹åŒ–)

        Args:
            layer: Transformer å±‚æ¨¡å—
            hidden_states: è¾“å…¥éšè—çŠ¶æ€ [B, S, D]
            cache_manager: KV ç¼“å­˜ç®¡ç†å™¨
            seq_ids: åºåˆ—IDåˆ—è¡¨
            context_lens: ä¸Šä¸‹æ–‡é•¿åº¦åˆ—è¡¨
            layer_idx: å±‚ç´¢å¼•

        Returns:
            æ›´æ–°åçš„éšè—çŠ¶æ€å’Œå½“å‰å±‚çš„ KV ç¼“å­˜
        """
        batch_size = hidden_states.size(0)
        self.performance_stats['batch_sizes'].append(batch_size)

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.perf_counter_ns()

        # 1. LayerNorm åº”ç”¨ (å¼‚æ­¥æ‰§è¡Œ)
        with torch.cuda.stream(self.compute_stream):
            residual = hidden_states
            norm_output = self._fast_norm(layer, hidden_states)

        # 2. QKV æŠ•å½±è®¡ç®—
        q, k, v = self._fast_qkv(layer, norm_output, batch_size)

        # 3. å½¢çŠ¶é‡å¡‘
        q, k, v = self._fast_reshape(q, k, v, batch_size)

        # 4. æ³¨æ„åŠ›è®¡ç®— (ä¸»è¦ä¼˜åŒ–ç‚¹)
        attn_output = self.attention(
            query=q, cache_manager=cache_manager,
            seq_ids=seq_ids, context_lens=context_lens,
            layer_idx=layer_idx, key=k, value=v
        )

        # 5. è¾“å‡ºæŠ•å½±å’Œæ®‹å·®è¿æ¥
        output_proj = self._apply_output(layer, attn_output, batch_size)
        hidden_states = residual + output_proj

        # 6. MLP å¤„ç†
        hidden_states = self._apply_mlp(layer, hidden_states)

        # æ€§èƒ½ç»Ÿè®¡
        self._stats(start_time, layer_idx, batch_size)

        return hidden_states, (k, v)

    def _layer_norm(self, layer, hidden_states):
        """ä¼˜åŒ–çš„ LayerNorm åº”ç”¨"""
        norm_fn = getattr(layer, self.cfg["norm"])
        return norm_fn(hidden_states)

    def _compute_qkv(self, layer, hidden_states, batch_size):
        """ä¼˜åŒ–çš„ QKV æŠ•å½±è®¡ç®—"""
        if self.cfg["qkv_split"]:
            # åˆå¹¶çš„ QKV æŠ•å½±
            qkv = layer.attn.c_attn(hidden_states)
            hidden_size = qkv.shape[-1] // 3
            return qkv.split(hidden_size, dim=-1)
        else:
            # åˆ†ç¦»çš„ QKV æŠ•å½± (ä½¿ç”¨é¢„åˆ†é…ç¼“å†²åŒº)
            if batch_size <= self.buffer_size:
                q = layer.self_attn.q_proj(hidden_states, output_tensor=self.q_buffer[:batch_size])
                k = layer.self_attn.k_proj(hidden_states, output_tensor=self.k_buffer[:batch_size])
                v = layer.self_attn.v_proj(hidden_states, output_tensor=self.v_buffer[:batch_size])
                return q, k, v
            else:
                # åŠ¨æ€åˆ†é…
                return (
                    layer.self_attn.q_proj(hidden_states),
                    layer.self_attn.k_proj(hidden_states),
                    layer.self_attn.v_proj(hidden_states)
                )

    def _reshape(self, q, k, v, batch_size):
        """ä¼˜åŒ–çš„å¼ é‡å½¢çŠ¶é‡å¡‘"""
        # ä½¿ç”¨ view + permute (æ¯” reshape æ›´é«˜æ•ˆ)
        q = q.view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)
        k = k.view(batch_size, -1, self.kv_num_heads, self.head_size).permute(0, 2, 1, 3)
        v = v.view(batch_size, -1, self.kv_num_heads, self.head_size).permute(0, 2, 1, 3)

        return q.squeeze(2), k.squeeze(2), v.squeeze(2)

    def _apply_output(self, layer, attn_output, batch_size):
        """è¾“å‡ºæŠ•å½±åº”ç”¨"""
        proj_fn = getattr(layer.self_attn if self.cfg["qkv_proj"] else layer.attn, self.cfg["proj"])
        return proj_fn(attn_output.reshape(batch_size, -1)).unsqueeze(1)

    def _apply_mlp(self, layer, hidden_states):
        """MLP å¤„ç†"""
        residual = hidden_states
        mlp_norm_fn = getattr(layer, self.cfg["mlp_norm"])
        mlp_fn = getattr(layer, self.cfg["mlp"])

        # åº”ç”¨ LayerNorm
        normalized = mlp_norm_fn(hidden_states)

        # MLP æˆ– MoE
        if self.cfg.get("moe", False) and hasattr(layer, 'mlp') and hasattr(layer.mlp, 'experts'):
            mlp_output = layer.mlp(normalized)
        else:
            mlp_output = mlp_fn(normalized)

        return residual + mlp_output

    def _stats(self, start_time, layer_idx, batch_size):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        layer_time = (time.perf_counter_ns() - start_time) / 1e6  # ms

        self.performance_stats['total_time'] += layer_time
        self.performance_stats['layer_count'] += 1

        # æ¯10å±‚è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
        if layer_idx % 10 == 0:
            avg_time = self.performance_stats['total_time'] / self.performance_stats['layer_count']
            avg_batch = sum(self.performance_stats['batch_sizes']) / len(self.performance_stats['batch_sizes'])

            print(f"ğŸš€ Layer {layer_idx}: {layer_time:.2f}ms | "
                  f"Avg: {avg_time:.2f}ms | Batch: {batch_size} | "
                  f"Mean Batch: {avg_batch:.1f}")

    def summary(self):
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if self.performance_stats['layer_count'] > 0:
            avg_time = self.performance_stats['total_time'] / self.performance_stats['layer_count']
            avg_batch = sum(self.performance_stats['batch_sizes']) / len(self.performance_stats['batch_sizes'])

            return {
                'total_layers': self.performance_stats['layer_count'],
                'avg_layer_time_ms': avg_time,
                'avg_batch_size': avg_batch,
                'total_time_ms': self.performance_stats['total_time']
            }
        return None
