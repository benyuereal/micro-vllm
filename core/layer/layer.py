import torch
from typing import Tuple, List, Optional
from core.paged_attention import PagedAttention
import time
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLayerAdapter:
    """
    ğŸ“Œ **æ¨¡å‹å±‚é€‚é…å™¨** - vLLMæ ¸å¿ƒç»„ä»¶

    ğŸ” **è®¾è®¡å“²å­¦**:
        1. **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æ¨¡å‹æ¶æ„ä½¿ç”¨ç›¸åŒçš„process_layeræ¥å£
        2. **è‡ªåŠ¨é€‚é…**: æ ¹æ®model_typeè‡ªåŠ¨é€‰æ‹©å¤„ç†é€»è¾‘
        3. **é›¶æ‹·è´**: ç›´æ¥æ“ä½œå¼ é‡ï¼Œæ— ä¸­é—´æ‹·è´
        4. **ç”Ÿäº§å°±ç»ª**: æ”¯æŒAMPã€å¼‚å¸¸å¤„ç†ã€è®¾å¤‡åŒ¹é…

    ğŸ§ª **å…¸å‹ç”¨æ³•**:
        adapter = ModelLayerAdapter(config, device, num_heads=16, head_size=128, kv_num_heads=16)
        hidden_states, (k, v) = adapter.process_layer(
            layer=layer,
            hidden_states=hidden_states,  # [B, S, D]
            cache_manager=cache_manager,  # KVCacheManagerå®ä¾‹
            seq_ids=[0, 1, 2],          # åºåˆ—IDåˆ—è¡¨
            context_lens=[10, 20, 30],   # å½“å‰é•¿åº¦
            token_positions=positions,   # tokenä½ç½® (å¯é€‰)
            layer_idx=0,                 # å±‚ç´¢å¼•
            current_positions=positions  # å½“å‰ä½ç½® (å¯é€‰)
        )
    """

    # æ¨¡å‹æ¶æ„é…ç½® (å¯æ‰©å±•)
    MODEL_CONFIGS = {
        "qwen": {  # Qwen 7B
            "norm": "ln_1", "attn": "c_attn", "proj": "c_proj", "mlp_norm": "ln_2",
            "qkv_split": True, "qkv_proj": False,
            "mlp": "mlp", "residual": True,
        },
        "qwen2": {  # Qwen 1.5/2.5
            "norm": "input_layernorm", "attn": None, "proj": "o_proj", "mlp_norm": "post_attention_layernorm",
            "qkv_split": False, "qkv_proj": True,
            "mlp": "mlp", "residual": True,
        },
        "qwen3": {  # Qwen3 (ä¸Qwen2ç›¸åŒï¼Œä½†æ”¯æŒMoE)
            "norm": "input_layernorm", "attn": None, "proj": "o_proj", "mlp_norm": "post_attention_layernorm",
            "qkv_split": False, "qkv_proj": True,
            "mlp": "mlp", "residual": True,
            "moe": True,  # âœ… æ”¯æŒMoE
        },
    }

    def __init__(self, model_config, device: str, num_heads: int, head_size: int, kv_num_heads: int):
        """
        ğŸ“Œ **åˆå§‹åŒ–**

        ğŸ” **å‚æ•°**:
            - model_config: æ¨¡å‹é…ç½®
            - device: è®¾å¤‡ ("cuda", "mps", "cpu")
            - num_heads: æ³¨æ„åŠ›å¤´æ•°
            - head_size: æ¯ä¸ªå¤´ç»´åº¦
            - kv_num_heads: KVå¤´æ•° (GQAæ”¯æŒ)
        """
        self.config = model_config
        self.device = device
        self.model_type = model_config.model_type
        self.num_heads, self.head_size, self.kv_num_heads = num_heads, head_size, kv_num_heads

        # æ€§èƒ½ç»Ÿè®¡
        self.layer_times = []
        self.total_calls = 0
        self.enable_profiling = True  # æ˜¯å¦å¯ç”¨æ€§èƒ½åˆ†æ

        # åˆå§‹åŒ–æ³¨æ„åŠ›æ¨¡å—
        self.attention = PagedAttention(
            num_heads=num_heads,
            head_size=head_size,
            kv_num_heads=kv_num_heads,
            device=device
        )

        # éªŒè¯æ¨¡å‹ç±»å‹
        if self.model_type not in self.MODEL_CONFIGS:
            raise ValueError(f"Unsupported model type: {self.model_type}")
        self.cfg = self.MODEL_CONFIGS[self.model_type]

        logger.info(f"ModelLayerAdapter initialized for {self.model_type} on {device}")

    def _log_timing(self, stage_name: str, start_time: float, layer_idx: int, batch_size: int, seq_len: int):
        """è®°å½•å¾®ç§’çº§åˆ«çš„æ—¶é—´æ¶ˆè€—"""
        if not self.enable_profiling:
            return

        elapsed_us = (time.time() - start_time) * 1e6  # è½¬æ¢ä¸ºå¾®ç§’
        logger.debug(f"Layer {layer_idx} | {stage_name}: {elapsed_us:.2f}Î¼s "
                    f"(batch={batch_size}, seq={seq_len})")
        return elapsed_us

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.layer_times:
            return "No performance data available"

        import numpy as np
        times = np.array(self.layer_times)

        stats = {
            'total_calls': self.total_calls,
            'avg_total_time_us': np.mean(times[:, 0]),
            'avg_norm_time_us': np.mean(times[:, 1]),
            'avg_qkv_time_us': np.mean(times[:, 2]),
            'avg_reshape_time_us': np.mean(times[:, 3]),
            'avg_attention_time_us': np.mean(times[:, 4]),
            'avg_proj_time_us': np.mean(times[:, 5]),
            'avg_mlp_time_us': np.mean(times[:, 6]),
            'p95_total_time_us': np.percentile(times[:, 0], 95)
        }

        return stats

    def process_layer(self,
                      layer,
                      hidden_states: torch.Tensor,  # [B, S, D]
                      cache_manager,
                      seq_ids: List[int],
                      context_lens: List[int],
                      token_positions: Optional[torch.Tensor] = None,
                      layer_idx: int = 0,
                      current_positions: Optional[torch.Tensor] = None) -> Tuple[
        torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        ğŸ“Œ **å¤„ç†å•å±‚è®¡ç®—** (ç»Ÿä¸€æ¥å£ï¼Œè‡ªåŠ¨é€‚é…æ¨¡å‹æ¶æ„)

        ğŸ” **å‚æ•°**:
            - layer: æ¨¡å‹å±‚ (transformer layer)
            - hidden_states: éšè—çŠ¶æ€ [B, S, D]
            - cache_manager: KVCacheManagerå®ä¾‹
            - seq_ids: åºåˆ—IDåˆ—è¡¨ [B]
            - context_lens: å½“å‰é•¿åº¦åˆ—è¡¨ [B]
            - token_positions: tokenä½ç½® (å¯é€‰)
            - layer_idx: å±‚ç´¢å¼•
            - current_positions: å½“å‰ä½ç½® (å¯é€‰)

        âœ… **è¿”å›**:
            - hidden_states: æ›´æ–°åçš„éšè—çŠ¶æ€ [B, S, D]
            - (current_k, current_v): å½“å‰å±‚çš„KV [B, H, D]

        ğŸ§  **å†…éƒ¨é€»è¾‘**:
            1. è‡ªåŠ¨é€‚é…æ¨¡å‹æ¶æ„ (Qwen/Qwen2ç­‰)
            2. åº”ç”¨LayerNorm
            3. è®¡ç®—QKV (è‡ªåŠ¨å¤„ç†ä¸åŒæŠ•å½±æ–¹å¼)
            4. é‡å¡‘å½¢çŠ¶ [B, S, D] â†’ [B, H, D]
            5. è°ƒç”¨PagedAttention
            6. æ®‹å·®è¿æ¥ + MLP
        """
        total_start = time.time()
        batch_size, seq_len, _ = hidden_states.shape
        stage_times = [0.0] * 7  # å­˜å‚¨å„é˜¶æ®µè€—æ—¶

        # è®°å½•æ‰¹æ¬¡å’Œåºåˆ—ä¿¡æ¯
        if self.total_calls % 100 == 0:  # æ¯100æ¬¡è®°å½•ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
            logger.info(f"Processing layer {layer_idx} | batch={batch_size}, seq={seq_len}")

        # 1. è‡ªåŠ¨é€‚é…æ¨¡å‹æ¶æ„
        norm_fn = getattr(layer, self.cfg["norm"])
        mlp_norm_fn = getattr(layer, self.cfg["mlp_norm"])
        mlp_fn = getattr(layer, self.cfg["mlp"])

        # 2. LayerNorm + æ®‹å·®
        norm_start = time.time()
        residual = hidden_states
        hidden_states = norm_fn(hidden_states)
        stage_times[1] = self._log_timing("LayerNorm", norm_start, layer_idx, batch_size, seq_len)

        # 3. QKVè®¡ç®— (è‡ªåŠ¨å¤„ç†ä¸åŒæŠ•å½±æ–¹å¼)
        qkv_start = time.time()
        if self.cfg["qkv_split"]:
            # Qwen 7B: åˆå¹¶çš„c_attnæŠ•å½±
            qkv = layer.attn.c_attn(hidden_states)
            hidden_size = qkv.shape[-1] // 3
            q, k, v = qkv.split(hidden_size, dim=-1)
        else:
            # Qwen 1.5: åˆ†å¼€çš„q_proj/k_proj/v_proj
            q = layer.self_attn.q_proj(hidden_states)
            k = layer.self_attn.k_proj(hidden_states)
            v = layer.self_attn.v_proj(hidden_states)
        stage_times[2] = self._log_timing("QKV_Projection", qkv_start, layer_idx, batch_size, seq_len)

        # 4. é‡å¡‘å½¢çŠ¶ [B, S, D] â†’ [B, H, D]
        reshape_start = time.time()
        q = q.view(batch_size, seq_len, self.num_heads, self.head_size).permute(0, 2, 1, 3)  # [B, H, S, D]
        k = k.view(batch_size, seq_len, self.kv_num_heads, self.head_size).permute(0, 2, 1, 3)
        v = v.view(batch_size, seq_len, self.kv_num_heads, self.head_size).permute(0, 2, 1, 3)
        stage_times[3] = self._log_timing("Reshape", reshape_start, layer_idx, batch_size, seq_len)

        # 5. æ³¨æ„åŠ›è®¡ç®— (é›¶æ‹·è´)
        attention_start = time.time()
        attn_output = self.attention(
            query=q.squeeze(2),  # [B, H, D]
            cache_manager=cache_manager,
            seq_ids=seq_ids,
            context_lens=context_lens,
            layer_idx=layer_idx,
            key=k.squeeze(2),  # [B, H, D]
            value=v.squeeze(2)  # [B, H, D]
        )
        stage_times[4] = self._log_timing("Attention", attention_start, layer_idx, batch_size, seq_len)

        # 6. è¾“å‡ºæŠ•å½± + æ®‹å·®
        proj_start = time.time()
        proj_fn = getattr(layer.self_attn if self.cfg["qkv_proj"] else layer.attn, self.cfg["proj"])
        attn_output = proj_fn(attn_output.reshape(batch_size, -1)).unsqueeze(1)  # [B, 1, D]
        hidden_states = residual + attn_output
        stage_times[5] = self._log_timing("Projection+Residual", proj_start, layer_idx, batch_size, seq_len)

        # 7. MLP + æ®‹å·® (æ”¯æŒMoE)
        mlp_start = time.time()
        residual = hidden_states
        hidden_states = mlp_norm_fn(hidden_states)
        if self.cfg.get("moe", False):
            # âœ… Qwen3 MoE: ä½¿ç”¨ mlp æ¨¡å— (åŒ…å« experts å’Œ gate)
            if hasattr(layer, 'mlp') and hasattr(layer.mlp, 'experts'):
                hidden_states = layer.mlp(hidden_states)  # ç›´æ¥è°ƒç”¨mlpæ¨¡å—
        else:
            # Qwen2: æ™®é€šMLP
            hidden_states = mlp_fn(hidden_states)
        hidden_states = residual + hidden_states
        stage_times[6] = self._log_timing("MLP+Residual", mlp_start, layer_idx, batch_size, seq_len)

        # è®°å½•æ€»è€—æ—¶
        total_time = self._log_timing("TOTAL_LAYER", total_start, layer_idx, batch_size, seq_len)
        stage_times[0] = total_time

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        if self.enable_profiling:
            self.layer_times.append(stage_times)
            self.total_calls += 1

            # æ¯100å±‚è¾“å‡ºä¸€æ¬¡æ€§èƒ½æ‘˜è¦
            if self.total_calls % 100 == 0:
                stats = self.get_performance_stats()
                logger.info(f"Performance Summary after {self.total_calls} layers:")
                logger.info(f"  Avg Total: {stats['avg_total_time_us']:.2f}Î¼s")
                logger.info(f"  P95 Total: {stats['p95_total_time_us']:.2f}Î¼s")
                logger.info(f"  Norm: {stats['avg_norm_time_us']:.2f}Î¼s")
                logger.info(f"  QKV: {stats['avg_qkv_time_us']:.2f}Î¼s")
                logger.info(f"  Reshape: {stats['avg_reshape_time_us']:.2f}Î¼s")
                logger.info(f"  Attention: {stats['avg_attention_time_us']:.2f}Î¼s")
                logger.info(f"  Projection: {stats['avg_proj_time_us']:.2f}Î¼s")
                logger.info(f"  MLP: {stats['avg_mlp_time_us']:.2f}Î¼s")

        return hidden_states, (k.squeeze(2), v.squeeze(2))  # [B, H, D]