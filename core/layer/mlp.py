"""
===================================================================
MLP Forward - Qwen MLP ç¼–è¯‘ä¼˜åŒ–å®ç°
===================================================================

ğŸ“Œ **æ ¸å¿ƒåŠŸèƒ½**ï¼š
   - Attention è¾“å‡ºæŠ•å½± + MLP çš„ç¼–è¯‘èåˆ
   - ä¸“ä¸º Qwen æ¨¡å‹ä¼˜åŒ–
   - é’ˆå¯¹ CUDA Graphs ä¼˜åŒ– (é˜²æ­¢ tensor overwriting)

âš¡ **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - torch.compile fullgraph æ¨¡å¼
   - ç®—å­èåˆå‡å°‘å†…å­˜è®¿é—®
   - é™æ€ shape ä¼˜åŒ–
"""
import torch
import torch.nn.functional as F
from kernel.rmsnorm import rms_norm


def compiled(hidden_dim: int = 4096):
    """
    ğŸ“Œ **ç¼–è¯‘ MLP Forward å‡½æ•°**
    
    ğŸ” **å‚æ•°**:
        - hidden_dim: éšè—å±‚ç»´åº¦ (Qwen: 4096)
    
    âœ… **è¿”å›**:
        - ç¼–è¯‘åçš„ MLP forward å‡½æ•°
    """
    
    def mlp_forward(layer, hidden, attn_res, attn_out):
        """
        MLP å®ç° (ä¼šè¢« torch.compile ç¼–è¯‘)
        """
        batch_size = hidden.shape[0]
        
        # Attention è¾“å‡ºæŠ•å½± + æ®‹å·®è¿æ¥
        hidden = attn_res + torch.matmul(attn_out.view(batch_size, -1), layer.attn.c_proj.weight.t().contiguous()).unsqueeze(1)
        
        # LayerNorm + MLP
        normed = rms_norm(hidden, layer.ln_2.weight, layer.ln_2.eps)
        x = normed.view(-1, hidden_dim)
        
        # MLP æŠ•å½± (Gate + Up èåˆ)
        gate_up = torch.matmul(x, layer.mlp._gu)
        up, gate = gate_up.chunk(2, dim=-1)
        
        # SwiGLU æ¿€æ´»
        output = torch.matmul(F.silu(gate) * up, layer.mlp._d)
        
        # å…³é”®ï¼šClone output é˜²æ­¢ CUDA graphs buffer overwriting
        result = (hidden + output.view(hidden.shape)).clone()
        return result
    
    # ç¼–è¯‘ MLP å‡½æ•°
    return torch.compile(
        mlp_forward,
        mode="reduce-overhead",
        fullgraph=True,
        dynamic=False,
    )


class MLPForward:
    """
    ğŸ“Œ **MLP Forward å°è£…ç±»**
    """
    
    def __init__(self, hidden_dim: int = 4096):
        self.hidden_dim = hidden_dim
        self._forward = compiled(hidden_dim)
    
    def __call__(self, layer, hidden, attn_res, attn_out):
        return self._forward(layer, hidden, attn_res, attn_out)
