# api_server.py
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import threading
import time
import json
from core.engine import InferenceEngine
import asyncio
from queue import Queue
import config.config as Config
# å…¨å±€å¼•æ“å®ä¾‹
engine = None
app = FastAPI(title="vLLM API Server", version="0.1.0")


# è¯·æ±‚æ¨¡å‹
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: Optional[int] = 128
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9
    stream: Optional[bool] = False


class BatchGenerateRequest(BaseModel):
    prompts: List[str]
    max_tokens: Optional[int] = 128
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9


# å“åº”æ¨¡å‹
class GenerateResponse(BaseModel):
    text: str
    tokens: int
    time_ms: float


class BatchGenerateResponse(BaseModel):
    results: List[GenerateResponse]


class HealthResponse(BaseModel):
    status: str
    model: str
    device: str
    running_sequences: int
    waiting_sequences: int


# åˆå§‹åŒ–å¼•æ“
@app.on_event("startup")
async def startup_event():
    global engine
    model_path = Config.ModelConfig.MODEL_PATH  # fixme  ä¿®æ”¹ä¸ºæ‚¨çš„æ¨¡å‹è·¯å¾„
    print(f"Loading model from {model_path}...")
    try:
        engine = InferenceEngine(model_path)
        print(f"Model loaded successfully on device: {engine.device}")
    except Exception as e:
        print(f"Failed to load model: {str(e)}")
        raise e


# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health", response_model=HealthResponse)
async def health_check():
    if engine is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    return HealthResponse(
        status="healthy",
        model="Qwen-7B",
        device=str(engine.device),
        running_sequences=len(engine.scheduler.running_sequences),
        waiting_sequences=len(engine.scheduler.waiting_queue)
    )


# éæµå¼ç”Ÿæˆç«¯ç‚¹
@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    if engine is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    start_time = time.time()

    try:
        # ä½¿ç”¨å¼•æ“çš„generateæ–¹æ³•
        results = engine.generate(
            [request.prompt],
            max_tokens=request.max_tokens
        )

        # æå–ç»“æœ
        text = ""
        for seq, result_text in results.items():
            text = result_text
            break  # åªå–ç¬¬ä¸€ä¸ªç»“æœ

        end_time = time.time()
        time_ms = (end_time - start_time) * 1000

        return GenerateResponse(
            text=text,
            tokens=len(text),
            time_ms=time_ms
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")


# æ‰¹é‡ç”Ÿæˆç«¯ç‚¹
@app.post("/batch_generate", response_model=BatchGenerateResponse)
async def batch_generate_text(request: BatchGenerateRequest):
    if engine is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    start_time = time.time()

    try:
        # ä½¿ç”¨å¼•æ“çš„generateæ–¹æ³•
        results = engine.generate(
            request.prompts,
            max_tokens=request.max_tokens
        )

        # æ„å»ºå“åº”
        response_results = []
        for seq, text in results.items():
            response_results.append(GenerateResponse(
                text=text,
                tokens=len(text),
                time_ms=0  # æ‰¹é‡ç”Ÿæˆä¸å•ç‹¬è®¡ç®—æ—¶é—´
            ))

        return BatchGenerateResponse(results=response_results)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Batch generation failed: {str(e)}")


# åœ¨ generate_stream å‡½æ•°ä¸­æ·»åŠ ç»Ÿè®¡
@app.post("/generate_stream")
async def generate_stream(request: GenerateRequest):
    if engine is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    if not request.stream:
        response = await generate_text(request)
        return response

    # æ·»åŠ ç»Ÿè®¡å˜é‡
    start_time = time.time()
    token_count = 0

    async def event_generator():
        nonlocal token_count
        token_queue = Queue()
        full_text = ""

        seq_id = engine.add_request(
            request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p
        )

        def callback(token, text):
            nonlocal token_count
            token_count += 1
            token_queue.put((token, text))

        engine.register_stream_callback(seq_id, callback)

        try:
            step_count = 0
            total_step_time = 0
            first_batch_type = None
            first_batch_size = None
            
            while True:
                step_start = time.time()
                
                engine.step()
                step_time = time.time() - step_start
                
                step_count += 1
                total_step_time += step_time
                
                # è®°å½•é¦–æ‰¹æ¬¡çš„ç±»å‹å’Œå¤§å°
                if step_count == 1:
                    first_batch_type = "prefill" if engine.scheduler.waiting_queue else "decode"
                    first_batch_size = len(engine.scheduler.running_sequences)

                # æ¯ 50 æ­¥æ‰“å°ç»Ÿè®¡
                if step_count % 50 == 0:
                    avg_step = total_step_time / step_count * 1000
                    print(f"ğŸ“Š Steps {step_count-49}~{step_count}: avg={avg_step:.1f}ms/step, batch={len(engine.scheduler.running_sequences)}")

                # æ¶ˆè´¹æ‰€æœ‰å·²ç”Ÿæˆ token
                while not token_queue.empty():
                    token, text = token_queue.get()
                    full_text += text

                    data = {
                        "token": token,
                        "text": text,
                        "full_text": full_text,
                        "finished": token == engine.eos_token_id
                    }
                    yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

                # æ£€æŸ¥åºåˆ—æ˜¯å¦ç»“æŸ
                running_seqs = [seq for seq in engine.scheduler.running_sequences if seq.seq_id == seq_id]
                if not running_seqs:
                    avg_step_time = total_step_time / step_count * 1000
                    print(f"[DONE] å…± {step_count} steps, é¦–æ‰¹æ¬¡: type={first_batch_type}, size={first_batch_size}, å¹³å‡æ¯step: {avg_step_time:.1f}ms")
                    end_time = time.time()
                    gen_time = end_time - start_time
                    tokens_per_sec = token_count / gen_time if gen_time > 0 else 0

                    print(f"\nStream generated {token_count} tokens in {gen_time:.2f} seconds")
                    print(f"Throughput: {tokens_per_sec:.2f} tokens/sec")
                    break

                # âœ… å…³é”®ï¼šè®©å‡ºæ§åˆ¶æƒï¼Œä½†ä¸ç­‰å¾…
                await asyncio.sleep(0)  # â† åŸä¸º 0.01ï¼Œç°åœ¨æ”¹ä¸º 0

        finally:
            engine.unregister_stream_callback(seq_id)

    return StreamingResponse(event_generator(), media_type="text/event-stream")

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)