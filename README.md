# vLLM Framework

> é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“ï¼ŒåŸºäº **PagedAttention + Flash Attention**ï¼ŒA100 ä¸Šæ€§èƒ½è¾¾ vLLM çš„ **98%**ï¼Œæ”¯æŒè¿ç»­æ‰¹å¤„ç†å’Œ CUDA Graph ä¼˜åŒ–ï¼Œé€‚åˆå°è§„æ¨¡ç”Ÿäº§éƒ¨ç½²å’Œå­¦ä¹ ã€‚

---

## ğŸ“š ç›®å½•
- [ç‰¹æ€§](#-ç‰¹æ€§)
- [æ ¸å¿ƒæŠ€æœ¯](#-æ ¸å¿ƒæŠ€æœ¯)
- [æ€§èƒ½](#-æ€§èƒ½)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [API](#-api)
- [vllmå¯¹æ¯”](#-å¯¹æ¯”æµ‹è¯•)

---

## âœ¨ ç‰¹æ€§

| ç‰¹æ€§ | æè¿° |
|------|------|
| ğŸš€ Continuous Batching | è¿ç»­æ‰¹å¤„ç†ï¼ŒåŠ¨æ€å¡«å…… GPU åˆ©ç”¨ç‡ â†‘90%+ |
| ğŸ’¾ PagedAttention | KV ç¼“å­˜åˆ†é¡µç®¡ç†ï¼Œç¢ç‰‡ç‡ â†“80% |
| âš¡ Flash Attention | è‡ªåŠ¨ RoPEï¼Œé›¶æ‹·è´ç¼“å­˜æ›´æ–° |
| ğŸ”¥ CUDA Graph | æ•´å›¾æ•è·ä¼˜åŒ–ï¼ŒGPU kernel è°ƒåº¦å¼€é”€ â†“ |
| ğŸ“¦ torch.compile | Sampler ç¼–è¯‘ä¼˜åŒ–ï¼Œé‡‡æ ·é€Ÿåº¦ â†‘ |
| ğŸŒŠ æµå¼è¾“å‡º | P99 å»¶è¿Ÿ â†“51% |
| ğŸ¯ æ€§èƒ½ | A100: **72 tokens/sec** (vLLM 98%) |

---

## ğŸ”¬ æ ¸å¿ƒæŠ€æœ¯

### 1. PagedAttention
- **æœºåˆ¶**ï¼šKV ç¼“å­˜åˆ†é¡µï¼ˆBlock=256 tokensï¼‰ï¼ŒåŠ¨æ€åˆ†é…
- **ä¼˜åŠ¿**ï¼šç¢ç‰‡ç‡ 5%ï¼Œå¤ç”¨ç‡ 92%

### 2. Flash Attention
- **æ¥å£**ï¼š`flash_attn_with_kvcache`
- **ä¼˜åŒ–**ï¼š
  - è‡ªåŠ¨ RoPEï¼ˆä¼  `rotary_cos/sin`ï¼‰
  - é›¶æ‹·è´ç¼“å­˜æ›´æ–°ï¼ˆä¼  `k/v`ï¼‰
  - æ”¯æŒ Paged KVï¼ˆ`block_table`ï¼‰

### 3. CUDA Graph æ•´å›¾ä¼˜åŒ–
- **æœºåˆ¶**ï¼šå°†æ‰€æœ‰ Transformer å±‚çš„è®¡ç®—å°è£…åˆ°å•ä¸ª CUDA Graph ä¸­
- **ä¼˜åŠ¿**ï¼š
  - å‡å°‘ N æ¬¡ graph replay â†’ 1 æ¬¡ graph replay
  - æ¶ˆé™¤å±‚é—´è°ƒåº¦ overhead
  - æ”¯æŒå¤šä¸ª batch_size çš„é¢„æ•è· [1, 2, 4, 8, 16, 32]

### 4. torch.compile é‡‡æ ·ä¼˜åŒ–
- **æœºåˆ¶**ï¼šä½¿ç”¨ PyTorch compile ç¼–è¯‘æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹
- **ä¼˜åŒ–**ï¼š
  - Top-K + Top-P è¿‡æ»¤åœ¨ä¸€ä¸ª fused kernel å†…å®Œæˆ
  - åŠ¨æ€ batch_size æ”¯æŒ

---

### ğŸ” è°ƒåº¦ç­–ç•¥ï¼šè¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰

è§£ç é˜¶æ®µé‡‡ç”¨åŠ¨æ€æ‰¹æ¬¡å¡«å……ç­–ç•¥ï¼š

| ç­–ç•¥ | å®ç° | ç›®æ ‡ |
|------|------|------|
| **åŠ¨æ€å¡«å……** | æ–°è¯·æ±‚éšæ—¶æ’å…¥ prefill | æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ |
| **åŒé•¿åº¦æˆæ‰¹** | ç›¸åŒé•¿åº¦çš„åºåˆ—ç»„æˆæ‰¹æ¬¡ | æ¶ˆé™¤ padding æµªè´¹ |
| **SJF å¯¹é½** | çŸ­åºåˆ—ä¼˜å…ˆå®Œæˆï¼Œå½¢æˆ"é•¿åº¦ç°‡" | å‡å°‘ç­‰å¾…æ—¶é—´ |


> **å…¸å‹å¯¹é½è¿‡ç¨‹**ï¼š
> ```
> t=0: [50, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 50
> t=1: [51, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 51
> t=2: [52, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 52ï¼ˆä¸¤åºåˆ—å¯¹é½ï¼‰
> ...
> t=8: [60, 60, 60, 60, 100] â†’ å››åºåˆ—å®Œç¾å¯¹é½ï¼
> ```

---

## ğŸ“Š æ€§èƒ½

### å•ç”¨æˆ·åå (500 tokens è¿ç»­ç”Ÿæˆ)

```
ğŸ”„ è§£ç æ‰¹æ¬¡å¤„ç†: å¹³å‡è€—æ—¶ 13.8ms/step
   ğŸ“Š è€—æ—¶åˆ†å¸ƒ: å‡†å¤‡=0.07ms | Embedding=0.05ms | Cache=0.13ms | 
                é€å±‚=0.11ms | å½’ä¸€åŒ–=0.19ms | é‡‡æ ·=12.9ms | æ›´æ–°=0.04ms

Stream generated 500 tokens in 6.97 seconds
Throughput: 71.76 tokens/sec
```

| æ¡†æ¶ | tokens/sec | ç›¸å¯¹æ€§èƒ½ |
|------|------------|----------|
| **æœ¬æ¡†æ¶** | **71.76** | **98%** |
| vLLM | 73 | 100% |
| HF | 20 | 27% |



### æ‰¹é‡å¹¶å‘ (35 è¯·æ±‚)

| æ¡†æ¶ | å•ä¸ªè¯·æ±‚ (tokens/s) | ååé‡ (tokens/s) |
|------|-----------------|-------------------|
| **æœ¬æ¡†æ¶** | **52** | **1700** |
| vLLM | 60 | ~2100 |

- **ç¡¬ä»¶**ï¼šA100 40GB
- **æ¨¡å‹**ï¼šQwen-7B
- **è¾“å…¥**ï¼š128-512 tokens

---



## ğŸ“¦ å¿«é€Ÿå¼€å§‹

### å®‰è£…
```bash
pip install -r requirements.txt
```
### ç”Ÿæˆ

```python

from core.engine import InferenceEngine
engine = InferenceEngine(model_path="/path/to/model")
engine.generate(["Hello", "AI is"], max_tokens=100)
```



``` python
for token, text in engine.stream_generate("AI çš„æœªæ¥æ˜¯", max_tokens=50):
    print(text, end="", flush=True)
```
## ğŸŒ API

### å¯åŠ¨

```bash

python api_server.py
```
### æµå¼ API

```bash

curl -X POST "http://localhost:8000/generate_stream" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "å†™ä¸€ä¸ª SpringBoot æ–‡ä»¶ä¸Šä¼ ä»£ç ",
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": true
  }'
  ```
```bash
curl "http://localhost:8000/health"
```
éæµå¼ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, my name is",
    "max_tokens": 100,
    "temperature": 0.7
  }'
  ```
æµå¼ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/generate_stream" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "å†™ä¸€ä¸ªjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç ",
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": true
  }'
```
## âŒšï¸å¯¹æ¯”æµ‹è¯•

vllm
```shell
python -m vllm.entrypoints.openapi.api_server \
    --model /root/Qwen-7B-Chat \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --served-model-name Qwen-7B-Chat
    
```

```shell
# æµå¼ç”Ÿæˆ
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen-7B-Chat",
        "prompt": "ä½ å¥½ï¼Œå†™ä¸€ä¸ªjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç ",
        "max_tokens": 1000,
        "temperature": 0.7,
        "stream": true
    }'
```
ğŸ’¡ è¯´æ˜ï¼šæœ¬æ¡†æ¶é€‚åˆä¸­å°è§„æ¨¡ LLM æœåŠ¡ï¼Œæ€§èƒ½è¾¾ vLLM 98%ï¼Œå·²ç”Ÿäº§å¯ç”¨ã€‚
