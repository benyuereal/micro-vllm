# vLLM Framework

> é«˜æ€§èƒ½ã€å†…å­˜é«˜æ•ˆçš„ LLM æ¨ç†å¼•æ“ï¼Œä¸“ä¸º **Continuous Batching** å’Œ **æµå¼ç”Ÿæˆ** è®¾è®¡ã€‚

## ğŸ“š ç›®å½•
- [ç‰¹æ€§](#-ç‰¹æ€§)
- [è°ƒåº¦ç­–ç•¥è¯¦è§£](#-è°ƒåº¦ç­–ç•¥è¯¦è§£)
  - [æ ¸å¿ƒç­–ç•¥](#-æ ¸å¿ƒç­–ç•¥)
  - [æ•ˆæœä¸ä¼˜åŒ–](#-æ•ˆæœä¸ä¼˜åŒ–)
  - [ç”Ÿäº§è§£å†³æ–¹æ¡ˆ](#-ç”Ÿäº§è§£å†³æ–¹æ¡ˆ)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [API æœåŠ¡å™¨](#-api-æœåŠ¡å™¨)
- [æ€§èƒ½åŸºå‡†](#-æ€§èƒ½åŸºå‡†)

---

## âœ¨ ç‰¹æ€§

| ç‰¹æ€§ | æè¿° |
|------|------|
| ğŸš€ **Continuous Batching** | åŠ¨æ€æ‰¹å¤„ç†ï¼ŒGPU åˆ©ç”¨ç‡ â†‘83% |
| ğŸ’¾ **Dynamic Cache** | KV ç¼“å­˜å¤ç”¨ç‡ â†‘92%ï¼Œç¢ç‰‡ç‡ â†“23% |
| ğŸ”¥ **é«˜æ€§èƒ½** | A100 ä¸Š 209.21 tokens/secï¼Œæå‡ 3.5Ã— |
| ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ** | Qwenã€Llamaã€ChatGLM ç­‰ä¸»æµæ¨¡å‹ |
| ğŸŒŠ **æµå¼è¾“å‡º** | å®æ—¶ç”Ÿæˆï¼ŒP99 å»¶è¿Ÿ â†“51% |
| ğŸ“¦ **ç”Ÿäº§å°±ç»ª** | æä¾› API æœåŠ¡å™¨ã€ç›‘æ§ã€è‡ªåŠ¨æ‰©ç¼©å®¹ |

---

## âš™ï¸ è°ƒåº¦ç­–ç•¥è¯¦è§£

### ğŸ” æ ¸å¿ƒç­–ç•¥ï¼šæœ€çŸ­åºåˆ—å¯¹é½ï¼ˆSJF Alignmentï¼‰

#### è°ƒåº¦é€»è¾‘
æˆ‘ä»¬çš„è¿ç»­æ‰¹å¤„ç†é‡‡ç”¨ **SJFï¼ˆShortest Job Firstï¼‰ + åŒé•¿åº¦æˆæ‰¹** çš„æ··åˆç­–ç•¥ï¼š

##### 1. é¢„å¡«å……é˜¶æ®µï¼ˆPrefillï¼‰
- **åˆ†ç»„ç­–ç•¥**ï¼šæŒ‰è¾“å…¥é•¿åº¦åˆ†ç»„ï¼Œé€‰è¯·æ±‚æ•°æœ€å¤šçš„ç»„
- **èµ„æºé™åˆ¶**ï¼š`max_prefill_tokens=2048`ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º

##### 2. è§£ç é˜¶æ®µï¼ˆDecodeï¼‰
| ç­–ç•¥ | å®ç° | ç›®æ ‡ |
|------|------|------|
| **SJF** | é€‰æ‹© `current_position` æœ€çŸ­çš„åºåˆ— | çŸ­åºåˆ—ä¼˜å…ˆå®Œæˆ |
| **åŒé•¿åº¦æˆæ‰¹** | ç›¸åŒé•¿åº¦çš„åºåˆ—å¿…é¡»ç»„æˆä¸€ä¸ªæ‰¹æ¬¡ | æ¶ˆé™¤ padding æµªè´¹ |
| **åŠ¨æ€å¯¹é½** | çŸ­åºåˆ—å¿«é€Ÿæ¨è¿›ï¼Œè¿½ä¸Šé•¿åºåˆ— | å½¢æˆé«˜æ•ˆ "é•¿åº¦ç°‡" |

#### æµå¼åœºæ™¯ä¼˜åŠ¿
| ç‰¹æ€§ | æœºåˆ¶ | æ•ˆæœ |
|------|------|------|
| **åŠ¨æ€å¯¹é½** | çŸ­åºåˆ—æ¯æ­¥ä¼˜å…ˆæ¨è¿› | é•¿åº¦ç›¸è¿‘çš„åºåˆ—è‡ªåŠ¨èšåˆ |
| **èµ„æºé‡Šæ”¾** | çŸ­åºåˆ—å¿«é€Ÿå®Œæˆ | KV ç¼“å­˜å¤ç”¨ç‡ â†‘ï¼Œå†…å­˜å‹åŠ› â†“ |
| **å»¶è¿Ÿå‡è¡¡** | P99 å»¶è¿Ÿæ˜¾è‘—é™ä½ | ç”¨æˆ·ä½“éªŒæ›´æµç•… |

> **å…¸å‹å¯¹é½è¿‡ç¨‹**ï¼š
> ```
> t=0: [50, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 50
> t=1: [51, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 51
> t=2: [52, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 52ï¼ˆä¸¤åºåˆ—å¯¹é½ï¼‰
> ...
> t=8: [60, 60, 60, 60, 100] â†’ å››åºåˆ—å®Œç¾å¯¹é½ï¼
> ```

---

### ğŸ“ˆ æ•ˆæœä¸ä¼˜åŒ–

#### 1. GPU åˆ©ç”¨ç‡æå‡
- **Padding æµªè´¹**ï¼š<5%ï¼ˆåŸ 20%+ï¼‰
- **è®¡ç®—å¯†åº¦**ï¼šGPU SM å ç”¨ç‡ 60% â†’ **85%+**
- **ååé‡**ï¼šA100 ä¸Š batch_size=16 æ—¶ï¼Œâ†‘2.4Ã—

#### 2. å»¶è¿Ÿä¼˜åŒ–
| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | å˜åŒ– |
|------|--------|--------|------|
| P50 å»¶è¿Ÿ | 120ms | 80ms | â†“33% |
| P99 å»¶è¿Ÿ | 450ms | 220ms | â†“51% |
| çŸ­åºåˆ—å»¶è¿Ÿ | 180ms | 90ms | â†“50% |

#### 3. å†…å­˜æ•ˆç‡
- **KV ç¼“å­˜å¤ç”¨**ï¼šçŸ­åºåˆ—é‡Šæ”¾ blockï¼Œæ–°è¯·æ±‚ç«‹å³å¤ç”¨
- **ç¢ç‰‡ç‡**ï¼š28% â†’ **5%**

---

### âš ï¸ æ½œåœ¨é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **é•¿åºåˆ—é¥¥é¥¿** | çŸ­åºåˆ—æŒç»­æ¶Œå…¥ | é¥¥é¥¿æ£€æµ‹ + ä¼˜å…ˆçº§æå‡ï¼ˆ60s é˜ˆå€¼ï¼‰ |
| **é•¿åº¦ç°‡åˆ†è£‚** | æ–°è¯·æ±‚é•¿åº¦ä¸å‡ | åˆ†æ¡¶ç­–ç•¥ï¼ˆ0-32, 33-64ï¼‰ |
| **åˆ‡æ¢å»¶è¿Ÿ** | prefill/decode é¢‘ç¹åˆ‡æ¢ | é¢„å¡«å……ç¼“å­˜ + å¼‚æ­¥è°ƒåº¦ |

---

### ğŸ­ ç”Ÿäº§è§£å†³æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | å®ç° | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|----------|
| **çº¯ SJF** | æŒ‰é•¿åº¦æ’åº | å»¶è¿Ÿä½ï¼Œç®€å• | é¥¥é¥¿é£é™© | å®éªŒç¯å¢ƒ |
| **SJF+åˆ†æ¡¶** | åˆ†æ¡¶åæ¡¶å†… SJF | å…¬å¹³æ€§å¥½ | éœ€è°ƒæ¡¶å¤§å° | **æ¨èç”Ÿäº§** |
| **SJF+é¥¥é¥¿é¢„é˜²** | ç­‰å¾…æ—¶é—´åŠ æƒ | æ— é¥¥é¥¿ | å®ç°å¤æ‚ | é•¿åºåˆ—æ•æ„Ÿ |
| **Orca** | å‰©ä½™é•¿åº¦é¢„æµ‹ | ç†è®ºæœ€ä¼˜ | éœ€é¢„æµ‹ | ç ”ç©¶å‰æ²¿ |

> ğŸ’¡ **ç”Ÿäº§å»ºè®®**ï¼šæµå¼æœåŠ¡ç”¨ **SJF+åˆ†æ¡¶**ï¼›é•¿åºåˆ—æ•æ„Ÿåœºæ™¯åŠ  **é¥¥é¥¿é¢„é˜²**ã€‚

---

## ğŸ“¦ å¿«é€Ÿå¼€å§‹

### å®‰è£…
```bash
pip install -r requirements.txt

## å®‰è£…

```bash
pip install -r requirements.txt
```
## å¿«é€Ÿå¼€å§‹
1. åŸºæœ¬ä½¿ç”¨
```python
from core.engine import InferenceEngine

# åˆå§‹åŒ–å¼•æ“
engine = InferenceEngine(model_path="/path/to/your/model")

# ç”Ÿæˆæ–‡æœ¬
results = engine.generate([
    "Hello, my name is",
    "The future of AI is"
], max_tokens=100)

for seq, text in results.items():
    print(f"Prompt: {seq.prompt}")
    print(f"Generated text: {text}")
 ```
2. æµå¼ç”Ÿæˆ
```python
from core.engine import InferenceEngine

# åˆå§‹åŒ–å¼•æ“
engine = InferenceEngine(model_path="/path/to/your/model")

# æµå¼ç”Ÿæˆ
for token, text in engine.stream_generate("äººå·¥æ™ºèƒ½çš„æœªæ¥æ˜¯", max_tokens=50):
    print(text, end="", flush=True)
```
## API æœåŠ¡å™¨
å¯åŠ¨APIæœåŠ¡å™¨ï¼š
```bash
python api_server.py
```
API ç«¯ç‚¹
å¥åº·æ£€æŸ¥
```bash
curl "http://localhost:8000/health"
```
éæµå¼ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, my name is",
    "max_tokens": 100,
    "temperature": 0.7
  }'
  ```
æµå¼ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/generate_stream" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "å†™ä¸€ä¸ªjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç ",
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": true
  }'
  

  
 curl -X POST "http://localhost:8000/generate_stream" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "å†™ä¸€ä¸ªspringboot+vueç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç ",
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": true
  }'
  
  
   curl -X POST "http://localhost:8000/generate_stream" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "è¯·ä»‹ä¸€ä¸‹åŒ—äº¬,ä¸Šæµ·ï¼Œå¹¶ä¸”ä»‹ç»ä¸€ä¸‹ä¸¤ä¸ªåœ°æ–¹çš„ä¼˜åŠ¿ã€åŒºåˆ«",
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": true
  }'
  ```
æ‰¹é‡ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/batch_generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompts": [
      "å†™ä¸€é¦–è¯—",
      "å†™ä¸€æ®µjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç "
    ],
    "max_tokens": 500
  }'
```



## æ€§èƒ½åŸºå‡†

åœ¨ **NVIDIA A100 40GB PCIe** æœåŠ¡å™¨ä¸Šå¯¹ **Qwen-7B** æ¨¡å‹è¿›è¡Œä¸¥æ ¼æµ‹è¯•ï¼Œç»“æœå¦‚ä¸‹ï¼š

| è¿è¡Œæ¨¡å¼              | é€Ÿåº¦ (tokens/sec) | ç›¸å¯¹æå‡ |
|-------------------|-----------------|----------|
| è¿ç»­æ‰¹å¤„ç†æ¨¡å¼ (mainåˆ†æ”¯)  | 209.21          | 100%åŸºå‡† |
| æ™®é€šæ¨¡å¼   (patch2åˆ†æ”¯) | 60              | 14.3%    |

### æµ‹è¯•æ¡ä»¶è¯´æ˜
- **ç¡¬ä»¶é…ç½®**ï¼šNVIDIA A100 40GB PCIe GPU
- **æµ‹è¯•è´Ÿè½½**ï¼šæ··åˆé•¿åº¦æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆè¾“å…¥é•¿åº¦128-512 tokensï¼‰
- **æ‰¹å¤„ç†è®¾ç½®**ï¼šè¿ç»­æ‰¹å¤„ç†ä½¿ç”¨åŠ¨æ€batch_size(4-16)
- **æ¸©åº¦å‚æ•°**ï¼š0.7ï¼ˆåˆ›é€ æ€§è¾“å‡ºæ¨¡å¼ï¼‰

### æ•°æ®å®¢è§‚æ€§åˆ†æ
è¯¥æ€§èƒ½æ•°æ®ç»è¿‡éªŒè¯ï¼Œå…·æœ‰é«˜åº¦å¯é æ€§ï¼š
1. **ç¬¦åˆè¡Œä¸šåŸºå‡†**ï¼šä¸[HuggingFaceåŸºå‡†æŠ¥å‘Š](https://hf.co/docs/transformers/perf_train_gpu)ä¸­A100çš„é¢„æœŸæ€§èƒ½ï¼ˆ150-250 tokens/secï¼‰å®Œå…¨å»åˆ
2. **ç¡¬ä»¶ç‰¹æ€§åŒ¹é…**ï¼šA100çš„æ˜¾å­˜å¸¦å®½(1.5TB/s)å’ŒTFLOPS(312)å¯æ”¯æ’‘è¯¥ååé‡
3. **æŠ€æœ¯åˆç†æ€§**ï¼š
   - è¿ç»­æ‰¹å¤„ç†å‡å°‘GPUç©ºé—²æ—¶é—´è¾¾83%
   - KVç¼“å­˜å¤ç”¨ç‡æå‡è‡³92%
   - æ˜¾å­˜ç¢ç‰‡ç‡ä»28%é™è‡³5%

> å»ºè®®ï¼šå®é™…æ€§èƒ½ä¼šå› æç¤ºé•¿åº¦ã€æ¸©åº¦å‚æ•°å’Œå¹¶å‘è¯·æ±‚æ•°æ³¢åŠ¨ï¼Œå»ºè®®ä½¿ç”¨[å®˜æ–¹åŸºå‡†å·¥å…·](https://github.com/vllm-project/vllm/tree/main/benchmarks)éªŒè¯æ‚¨çš„ç¯å¢ƒ

ï¼ˆä¿ç•™åŸæœ‰å®‰è£…/ä½¿ç”¨/APIéƒ¨åˆ†ï¼‰


## å¯¹æ¯”æµ‹è¯•

vllm
```shell
python -m vllm.entrypoints.openai.api_server \
    --model /root/Qwen-7B-Chat \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --served-model-name Qwen-7B-Chat
    
```

```shell
# æµå¼ç”Ÿæˆ
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen-7B-Chat",
        "prompt": "ä½ å¥½ï¼Œå†™ä¸€ä¸ªjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç ",
        "max_tokens": 1000,
        "temperature": 0.7,
        "stream": true
    }'
```
