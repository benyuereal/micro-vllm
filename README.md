# vLLM Framework

ä¸€ä¸ªé«˜æ€§èƒ½ã€å†…å­˜é«˜æ•ˆçš„LLMæ¨ç†å¼•æ“ï¼Œæ”¯æŒContinuous Batchingå’ŒDynamic CacheæŠ€æœ¯ã€‚

## ç‰¹æ€§

- ğŸš€ **Continuous Batching**: åŠ¨æ€æ‰¹å¤„ç†ï¼Œæé«˜GPUåˆ©ç”¨ç‡
- ğŸ’¾ **Dynamic Cache**: åŠ¨æ€KVç¼“å­˜ç®¡ç†ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
- ğŸ”¥ **é«˜æ€§èƒ½**: ä¼˜åŒ–çš„æ¨ç†å¼•æ“ï¼Œå®ç°æè‡´æ€§èƒ½
- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒQwenç­‰ä¸»æµæ¨¡å‹
- ğŸ“¦ **ç”Ÿäº§å°±ç»ª**: æä¾›APIæœåŠ¡å™¨å’Œå®Œæ•´çš„ç®¡ç†å·¥å…·
- ğŸŒŠ **æµå¼è¾“å‡º**: æ”¯æŒå®æ—¶æµå¼æ–‡æœ¬ç”Ÿæˆ

## å®‰è£…

```bash
pip install -r requirements.txt
```
## å¿«é€Ÿå¼€å§‹
1. åŸºæœ¬ä½¿ç”¨
```python
from core.engine import InferenceEngine

# åˆå§‹åŒ–å¼•æ“
engine = InferenceEngine(model_path="/path/to/your/model")

# ç”Ÿæˆæ–‡æœ¬
results = engine.generate([
    "Hello, my name is",
    "The future of AI is"
], max_tokens=100)

for seq, text in results.items():
    print(f"Prompt: {seq.prompt}")
    print(f"Generated text: {text}")
 ```
2. æµå¼ç”Ÿæˆ
```python
from core.engine import InferenceEngine

# åˆå§‹åŒ–å¼•æ“
engine = InferenceEngine(model_path="/path/to/your/model")

# æµå¼ç”Ÿæˆ
for token, text in engine.stream_generate("äººå·¥æ™ºèƒ½çš„æœªæ¥æ˜¯", max_tokens=50):
    print(text, end="", flush=True)
```
## API æœåŠ¡å™¨
å¯åŠ¨APIæœåŠ¡å™¨ï¼š
```bash
python api_server.py
```
API ç«¯ç‚¹
å¥åº·æ£€æŸ¥
```bash
curl "http://localhost:8000/health"
```
éæµå¼ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, my name is",
    "max_tokens": 100,
    "temperature": 0.7
  }'
  ```
æµå¼ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/generate_stream" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "å†™ä¸€ä¸ªjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç ",
    "max_tokens": 1000,
    "temperature": 0.7,
    "stream": true
  }'
  ```
æ‰¹é‡ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/batch_generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompts": [
      "å†™ä¸€é¦–è¯—",
      "å†™ä¸€æ®µjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç "
    ],
    "max_tokens": 500
  }'
```
