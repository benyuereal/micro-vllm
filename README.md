# vLLM Framework

> é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“ï¼ŒåŸºäº **PagedAttention + Flash Attention**ï¼ŒA100 ä¸Šæ€§èƒ½è¾¾ vLLM çš„ **70%**,é€‚åˆå°è§„æ¨¡ç”Ÿäº§éƒ¨ç½²å’Œå­¦ä¹ ã€‚

---

## ğŸ“š ç›®å½•
- [ç‰¹æ€§](#-ç‰¹æ€§)
- [æ ¸å¿ƒæŠ€æœ¯](#-æ ¸å¿ƒæŠ€æœ¯)
- [æ€§èƒ½](#-æ€§èƒ½)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [API](#-api)
- [vllmå¯¹æ¯”](#-å¯¹æ¯”æµ‹è¯•)

---

## âœ¨ ç‰¹æ€§

| ç‰¹æ€§ | æè¿° |
|------|------|
| ğŸš€ Continuous Batching | GPU åˆ©ç”¨ç‡ â†‘83% |
| ğŸ’¾ PagedAttention | KV ç¼“å­˜ç¢ç‰‡ç‡ â†“80% |
| âš¡ Flash Attention | è‡ªåŠ¨ RoPEï¼Œé€Ÿåº¦ â†‘2Ã— |
| ğŸ”¥ æ€§èƒ½ | A100: **51 tokens/sec** (vLLM 70%) |
| ğŸŒŠ æµå¼è¾“å‡º | P99 å»¶è¿Ÿ â†“51% |

---

## ğŸ”¬ æ ¸å¿ƒæŠ€æœ¯

### 1. PagedAttention
- **æœºåˆ¶**ï¼šKV ç¼“å­˜åˆ†é¡µï¼ˆBlock=256 tokensï¼‰ï¼ŒåŠ¨æ€åˆ†é…
- **ä¼˜åŠ¿**ï¼šç¢ç‰‡ç‡ 5%ï¼Œå¤ç”¨ç‡ 92%

### 2. Flash Attention
- **æ¥å£**ï¼š`flash_attn_with_kvcache`
- **ä¼˜åŒ–**ï¼š
  - è‡ªåŠ¨ RoPEï¼ˆä¼  `rotary_cos/sin`ï¼‰
  - é›¶æ‹·è´ç¼“å­˜æ›´æ–°ï¼ˆä¼  `k/v`ï¼‰
  - æ”¯æŒ Paged KVï¼ˆ`block_table`ï¼‰

---
### ğŸ” è°ƒåº¦ç­–ç•¥ï¼šæœ€çŸ­åºåˆ—å¯¹é½ï¼ˆSJF Alignmentï¼‰

è§£ç 

| ç­–ç•¥ | å®ç° | ç›®æ ‡ |
|------|------|------|
| **SJF** | é€‰æ‹© `current_position` æœ€çŸ­çš„åºåˆ— | çŸ­åºåˆ—ä¼˜å…ˆå®Œæˆ |
| **åŒé•¿åº¦æˆæ‰¹** | ç›¸åŒé•¿åº¦çš„åºåˆ—å¿…é¡»ç»„æˆä¸€ä¸ªæ‰¹æ¬¡ | æ¶ˆé™¤ padding æµªè´¹ |
| **åŠ¨æ€å¯¹é½** | çŸ­åºåˆ—å¿«é€Ÿæ¨è¿›ï¼Œè¿½ä¸Šé•¿åºåˆ— | å½¢æˆé«˜æ•ˆ "é•¿åº¦ç°‡" |


> **å…¸å‹å¯¹é½è¿‡ç¨‹**ï¼š
> ```
> t=0: [50, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 50
> t=1: [51, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 51
> t=2: [52, 52, 55, 60, 100] â†’ é€‰é•¿åº¦ 52ï¼ˆä¸¤åºåˆ—å¯¹é½ï¼‰
> ...
> t=8: [60, 60, 60, 60, 100] â†’ å››åºåˆ—å®Œç¾å¯¹é½ï¼
> ```

---

## ğŸ“Š æ€§èƒ½

| æ¡†æ¶ | tokens/sec | ç›¸å¯¹æ€§èƒ½ |
|------|------------|----------|
| **æœ¬æ¡†æ¶** | **51** | **70%** |
| vLLM | 70 | 100% |
| HF | 20 | 28.6% |

- **ç¡¬ä»¶**ï¼šA100 40GB
- **æ¨¡å‹**ï¼šQwen-7B
- **è¾“å…¥**ï¼š128-512 tokens

---

## ğŸ“¦ å¿«é€Ÿå¼€å§‹

### å®‰è£…
```bash
pip install -r requirements.txt
```
### ç”Ÿæˆ

```python

from core.engine import InferenceEngine
engine = InferenceEngine(model_path="/path/to/model")
engine.generate(["Hello", "AI is"], max_tokens=100)
```



``` python
for token, text in engine.stream_generate("AI çš„æœªæ¥æ˜¯", max_tokens=50):
    print(text, end="", flush=True)
```
## ğŸŒ API

### å¯åŠ¨

```bash

python api_server.py
```
### æµå¼ API

```bash

curl -X POST "http://localhost:8000/generate_stream" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "å†™ä¸€ä¸ª SpringBoot æ–‡ä»¶ä¸Šä¼ ä»£ç ",
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": true
  }'
  ```
```bash
curl "http://localhost:8000/health"
```
éæµå¼ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, my name is",
    "max_tokens": 100,
    "temperature": 0.7
  }'
  ```
æµå¼ç”Ÿæˆ
```bash
curl -X POST "http://localhost:8000/generate_stream" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "å†™ä¸€ä¸ªjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç ",
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": true
  }'
```
## âŒšï¸å¯¹æ¯”æµ‹è¯•

vllm
```shell
python -m vllm.entrypoints.openai.api_server \
    --model /root/Qwen-7B-Chat \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --served-model-name Qwen-7B-Chat
    
```

```shell
# æµå¼ç”Ÿæˆ
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen-7B-Chat",
        "prompt": "ä½ å¥½ï¼Œå†™ä¸€ä¸ªjavaç‰ˆæœ¬çš„æ–‡ä»¶ä¸Šä¼ ä»£ç ",
        "max_tokens": 1000,
        "temperature": 0.7,
        "stream": true
    }'
```
ğŸ’¡ è¯´æ˜ï¼šæœ¬æ¡†æ¶é€‚åˆä¸­å°è§„æ¨¡ LLM æœåŠ¡ï¼Œæ€§èƒ½è¾¾ vLLM 70%ï¼Œå·²ç”Ÿäº§å¯ç”¨ã€‚