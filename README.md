# vLLM Framework

ä¸€ä¸ªé«˜æ€§èƒ½ã€å†…å­˜é«˜æ•ˆçš„LLMæ¨ç†å¼•æ“ï¼Œæ”¯æŒContinuous Batchingå’ŒPage AttentionæŠ€æœ¯ã€‚

## ç‰¹æ€§

- ğŸš€ **Continuous Batching**: åŠ¨æ€æ‰¹å¤„ç†ï¼Œæé«˜GPUåˆ©ç”¨ç‡
- ğŸ’¾ **Page Attention**: åˆ†é¡µæ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
- ğŸ”¥ **é«˜æ€§èƒ½**: ä¼˜åŒ–çš„CUDAå†…æ ¸ï¼Œå®ç°æè‡´æ€§èƒ½
- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒQwenã€LLaMAã€ChatGLMç­‰ä¸»æµæ¨¡å‹
- ğŸ“¦ **ç”Ÿäº§å°±ç»ª**: æä¾›APIæœåŠ¡å™¨å’Œå®Œæ•´çš„ç®¡ç†å·¥å…·

## å®‰è£…
```bash

pip install -r requirements.txt

python setup.py install
```
## å¿«é€Ÿå¼€å§‹
```python

from vllm import LLMEngine, SamplingParams

åˆå§‹åŒ–å¼•æ“
engine = LLMEngine(model="Qwen/Qwen-7B")

è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(

temperature=0.8,

top_p=0.9,

max_tokens=100,

)

ç”Ÿæˆæ–‡æœ¬
outputs = engine.generate(

prompts=["Hello, my name is", "The future of AI is"],

sampling_params=sampling_params,

)

for output in outputs:

print(f"Prompt: {output.prompt}")

print(f"Generated text: {output.generated_text}")
```
## APIæœåŠ¡å™¨

å¯åŠ¨APIæœåŠ¡å™¨ï¼š
```bash

python -m vllm.server
```
ç„¶åä½¿ç”¨curlæˆ–å®¢æˆ·ç«¯å‘é€è¯·æ±‚ï¼š
```bash

curl -X POST "http://localhost:8000/generate" \

-H "Content-Type: application/json" \

-d '{

"prompt": "Hello, my name is",

"sampling_params": {

"temperature": 0.8,

"max_tokens": 100

}

}'
```
## æ€§èƒ½åŸºå‡†

åœ¨NVIDIA A100ä¸Šæµ‹è¯•ï¼š

| æ¨¡å‹ | ååé‡ (tokens/sec) | å»¶è¿Ÿ (ms/token) |
|------|---------------------|-----------------|
| Qwen-7B | 1250 | 45 |
| LLaMA-13B | 890 | 62 |

## æ–‡æ¡£

è¯¦ç»†æ–‡æ¡£è¯·å‚è€ƒ [æ–‡æ¡£é“¾æ¥]ã€‚

## è®¸å¯è¯

Apache License 2.0